I0625 14:48:02.374258 29461 caffe.cpp:185] Using GPUs 1
I0625 14:48:02.389979 29461 caffe.cpp:190] GPU 1: GeForce GTX TITAN X
I0625 14:48:02.747627 29461 solver.cpp:48] Initializing solver from parameters: 
test_iter: 128
test_interval: 100
base_lr: 0.01
display: 20
max_iter: 4000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 4000
snapshot: 500
snapshot_prefix: "data/models/segnet"
solver_mode: GPU
device_id: 1
net: "models/segnet/train_val.prototxt"
test_initialization: true
iter_size: 1
I0625 14:48:02.747743 29461 solver.cpp:91] Creating training net from net file: models/segnet/train_val.prototxt
I0625 14:48:02.749171 29461 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0625 14:48:02.749574 29461 net.cpp:49] Initializing net from parameters: 
name: "segnet"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "DenseImageData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_value: 100
    crop_h: 224
    crop_w: 288
  }
  dense_image_data_param {
    source: "data/train_seg.txt"
    batch_size: 32
    shuffle: true
    new_height: 224
    new_width: 310
    is_color: false
    root_folder: "data/raw/train/"
  }
}
layer {
  name: "conv1_1"
  type: "Convolution"
  bottom: "data"
  top: "conv1_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_1"
  type: "BatchNorm"
  bottom: "conv1_1"
  top: "conv1_1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale1_1"
  type: "Scale"
  bottom: "conv1_1"
  top: "conv1_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1_1"
  type: "ReLU"
  bottom: "conv1_1"
  top: "conv1_1"
}
layer {
  name: "conv1_2"
  type: "Convolution"
  bottom: "conv1_1"
  top: "conv1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_2"
  type: "BatchNorm"
  bottom: "conv1_2"
  top: "conv1_2"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale1_2"
  type: "Scale"
  bottom: "conv1_2"
  top: "conv1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1_2"
  type: "ReLU"
  bottom: "conv1_2"
  top: "conv1_2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1_2"
  top: "pool1"
  top: "pool1_mask"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2"
  top: "pool2_mask"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_1"
  type: "BatchNorm"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale3_1"
  type: "Scale"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv3_2"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv3_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_2"
  type: "BatchNorm"
  bottom: "conv3_2"
  top: "conv3_2"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale3_2"
  type: "Scale"
  bottom: "conv3_2"
  top: "conv3_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_2"
  type: "ReLU"
  bottom: "conv3_2"
  top: "conv3_2"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3_2"
  top: "pool3"
  top: "pool3_mask"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4"
  top: "pool4_mask"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv5_1"
  type: "Convolution"
  bottom: "pool4"
  top: "conv5_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn5_1"
  type: "BatchNorm"
  bottom: "conv5_1"
  top: "conv5_1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale5_1"
  type: "Scale"
  bottom: "conv5_1"
  top: "conv5_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5_1"
  type: "ReLU"
  bottom: "conv5_1"
  top: "conv5_1"
}
layer {
  name: "conv5_2"
  type: "Convolution"
  bottom: "conv5_1"
  top: "conv5_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn5_2"
  type: "BatchNorm"
  bottom: "conv5_2"
  top: "conv5_2"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale5_2"
  type: "Scale"
  bottom: "conv5_2"
  top: "conv5_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5_2"
  type: "ReLU"
  bottom: "conv5_2"
  top: "conv5_2"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5_2"
  top: "pool5"
  top: "pool5_mask"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "upsample5"
  type: "Upsample"
  bottom: "pool5"
  bottom: "pool5_mask"
  top: "pool5_D"
  upsample_param {
    scale: 2
  }
}
layer {
  name: "conv5_2_D"
  type: "Convolution"
  bottom: "pool5_D"
  top: "conv5_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn5_2_D"
  type: "BatchNorm"
  bottom: "conv5_2_D"
  top: "conv5_2_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale5_2_D"
  type: "Scale"
  bottom: "conv5_2_D"
  top: "conv5_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5_2_D"
  type: "ReLU"
  bottom: "conv5_2_D"
  top: "conv5_2_D"
}
layer {
  name: "conv5_1_D"
  type: "Convolution"
  bottom: "conv5_2_D"
  top: "conv5_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn5_1_D"
  type: "BatchNorm"
  bottom: "conv5_1_D"
  top: "conv5_1_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale5_1_D"
  type: "Scale"
  bottom: "conv5_1_D"
  top: "conv5_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5_1_D"
  type: "ReLU"
  bottom: "conv5_1_D"
  top: "conv5_1_D"
}
layer {
  name: "upsample4"
  type: "Upsample"
  bottom: "conv5_1_D"
  bottom: "pool4_mask"
  top: "pool4_D"
  upsample_param {
    scale: 2
  }
}
layer {
  name: "conv4_2_D"
  type: "Convolution"
  bottom: "pool4_D"
  top: "conv4_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2_D"
  type: "BatchNorm"
  bottom: "conv4_2_D"
  top: "conv4_2_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale4_2_D"
  type: "Scale"
  bottom: "conv4_2_D"
  top: "conv4_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4_2_D"
  type: "ReLU"
  bottom: "conv4_2_D"
  top: "conv4_2_D"
}
layer {
  name: "conv4_1_D"
  type: "Convolution"
  bottom: "conv4_2_D"
  top: "conv4_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1_D"
  type: "BatchNorm"
  bottom: "conv4_1_D"
  top: "conv4_1_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale4_1_D"
  type: "Scale"
  bottom: "conv4_1_D"
  top: "conv4_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4_1_D"
  type: "ReLU"
  bottom: "conv4_1_D"
  top: "conv4_1_D"
}
layer {
  name: "upsample3"
  type: "Upsample"
  bottom: "conv4_1_D"
  bottom: "pool3_mask"
  top: "pool3_D"
  upsample_param {
    scale: 2
  }
}
layer {
  name: "conv3_2_D"
  type: "Convolution"
  bottom: "pool3_D"
  top: "conv3_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3_2_D"
  type: "BatchNorm"
  bottom: "conv3_2_D"
  top: "conv3_2_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale3_2_D"
  type: "Scale"
  bottom: "conv3_2_D"
  top: "conv3_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_2_D"
  type: "ReLU"
  bottom: "conv3_2_D"
  top: "conv3_2_D"
}
layer {
  name: "conv3_1_D"
  type: "Convolution"
  bottom: "conv3_2_D"
  top: "conv3_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_1_D"
  type: "BatchNorm"
  bottom: "conv3_1_D"
  top: "conv3_1_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale3_1_D"
  type: "Scale"
  bottom: "conv3_1_D"
  top: "conv3_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_1_D"
  type: "ReLU"
  bottom: "conv3_1_D"
  top: "conv3_1_D"
}
layer {
  name: "upsample2"
  type: "Upsample"
  bottom: "conv3_1_D"
  bottom: "pool2_mask"
  top: "pool2_D"
  upsample_param {
    scale: 2
  }
}
layer {
  name: "conv2_2_D"
  type: "Convolution"
  bottom: "pool2_D"
  top: "conv2_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2_D"
  type: "BatchNorm"
  bottom: "conv2_2_D"
  top: "conv2_2_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale2_2_D"
  type: "Scale"
  bottom: "conv2_2_D"
  top: "conv2_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2_2_D"
  type: "ReLU"
  bottom: "conv2_2_D"
  top: "conv2_2_D"
}
layer {
  name: "conv2_1_D"
  type: "Convolution"
  bottom: "conv2_2_D"
  top: "conv2_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_1_D"
  type: "BatchNorm"
  bottom: "conv2_1_D"
  top: "conv2_1_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale2_1_D"
  type: "Scale"
  bottom: "conv2_1_D"
  top: "conv2_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2_1_D"
  type: "ReLU"
  bottom: "conv2_1_D"
  top: "conv2_1_D"
}
layer {
  name: "upsample1"
  type: "Upsample"
  bottom: "conv2_1_D"
  bottom: "pool1_mask"
  top: "pool1_D"
  upsample_param {
    scale: 2
  }
}
layer {
  name: "conv1_2_D"
  type: "Convolution"
  bottom: "pool1_D"
  top: "conv1_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_2_D"
  type: "BatchNorm"
  bottom: "conv1_2_D"
  top: "conv1_2_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale1_2_D"
  type: "Scale"
  bottom: "conv1_2_D"
  top: "conv1_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1_2_D"
  type: "ReLU"
  bottom: "conv1_2_D"
  top: "conv1_2_D"
}
layer {
  name: "conv1_1_D"
  type: "Convolution"
  bottom: "conv1_2_D"
  top: "conv1_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 2
    bias_term: true
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "conv1_1_D"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "conv1_1_D"
  bottom: "label"
  top: "accuracy"
}
I0625 14:48:02.749850 29461 layer_factory.hpp:77] Creating layer data
I0625 14:48:02.749882 29461 net.cpp:91] Creating Layer data
I0625 14:48:02.749887 29461 net.cpp:399] data -> data
I0625 14:48:02.749907 29461 net.cpp:399] data -> label
I0625 14:48:02.750221 29461 dense_image_data_layer.cpp:38] Opening file data/train_seg.txt
I0625 14:48:02.751107 29461 dense_image_data_layer.cpp:48] Shuffling data
I0625 14:48:02.751318 29461 dense_image_data_layer.cpp:53] A total of 2024 examples.
I0625 14:48:03.032990 29461 dense_image_data_layer.cpp:92] output data size: 1,1,224,288
I0625 14:48:03.035815 29461 net.cpp:141] Setting up data
I0625 14:48:03.035847 29461 net.cpp:148] Top shape: 1 1 224 288 (64512)
I0625 14:48:03.035856 29461 net.cpp:148] Top shape: 1 1 224 288 (64512)
I0625 14:48:03.035861 29461 net.cpp:156] Memory required for data: 516096
I0625 14:48:03.035871 29461 layer_factory.hpp:77] Creating layer label_data_1_split
I0625 14:48:03.035897 29461 net.cpp:91] Creating Layer label_data_1_split
I0625 14:48:03.035904 29461 net.cpp:425] label_data_1_split <- label
I0625 14:48:03.035918 29461 net.cpp:399] label_data_1_split -> label_data_1_split_0
I0625 14:48:03.035933 29461 net.cpp:399] label_data_1_split -> label_data_1_split_1
I0625 14:48:03.036056 29461 net.cpp:141] Setting up label_data_1_split
I0625 14:48:03.036070 29461 net.cpp:148] Top shape: 1 1 224 288 (64512)
I0625 14:48:03.036077 29461 net.cpp:148] Top shape: 1 1 224 288 (64512)
I0625 14:48:03.036080 29461 net.cpp:156] Memory required for data: 1032192
I0625 14:48:03.036085 29461 layer_factory.hpp:77] Creating layer conv1_1
I0625 14:48:03.036108 29461 net.cpp:91] Creating Layer conv1_1
I0625 14:48:03.036115 29461 net.cpp:425] conv1_1 <- data
I0625 14:48:03.036123 29461 net.cpp:399] conv1_1 -> conv1_1
I0625 14:48:03.280279 29461 net.cpp:141] Setting up conv1_1
I0625 14:48:03.280308 29461 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 14:48:03.280311 29461 net.cpp:156] Memory required for data: 9289728
I0625 14:48:03.280323 29461 layer_factory.hpp:77] Creating layer bn1_1
I0625 14:48:03.280339 29461 net.cpp:91] Creating Layer bn1_1
I0625 14:48:03.280344 29461 net.cpp:425] bn1_1 <- conv1_1
I0625 14:48:03.280349 29461 net.cpp:386] bn1_1 -> conv1_1 (in-place)
I0625 14:48:03.280558 29461 net.cpp:141] Setting up bn1_1
I0625 14:48:03.280567 29461 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 14:48:03.280570 29461 net.cpp:156] Memory required for data: 17547264
I0625 14:48:03.280578 29461 layer_factory.hpp:77] Creating layer scale1_1
I0625 14:48:03.280591 29461 net.cpp:91] Creating Layer scale1_1
I0625 14:48:03.280593 29461 net.cpp:425] scale1_1 <- conv1_1
I0625 14:48:03.280597 29461 net.cpp:386] scale1_1 -> conv1_1 (in-place)
I0625 14:48:03.280638 29461 layer_factory.hpp:77] Creating layer scale1_1
I0625 14:48:03.280844 29461 net.cpp:141] Setting up scale1_1
I0625 14:48:03.280854 29461 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 14:48:03.280858 29461 net.cpp:156] Memory required for data: 25804800
I0625 14:48:03.280864 29461 layer_factory.hpp:77] Creating layer relu1_1
I0625 14:48:03.280874 29461 net.cpp:91] Creating Layer relu1_1
I0625 14:48:03.280876 29461 net.cpp:425] relu1_1 <- conv1_1
I0625 14:48:03.280880 29461 net.cpp:386] relu1_1 -> conv1_1 (in-place)
I0625 14:48:03.281158 29461 net.cpp:141] Setting up relu1_1
I0625 14:48:03.281170 29461 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 14:48:03.281172 29461 net.cpp:156] Memory required for data: 34062336
I0625 14:48:03.281175 29461 layer_factory.hpp:77] Creating layer conv1_2
I0625 14:48:03.281185 29461 net.cpp:91] Creating Layer conv1_2
I0625 14:48:03.281188 29461 net.cpp:425] conv1_2 <- conv1_1
I0625 14:48:03.281193 29461 net.cpp:399] conv1_2 -> conv1_2
I0625 14:48:03.282753 29461 net.cpp:141] Setting up conv1_2
I0625 14:48:03.282766 29461 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 14:48:03.282769 29461 net.cpp:156] Memory required for data: 42319872
I0625 14:48:03.282773 29461 layer_factory.hpp:77] Creating layer bn1_2
I0625 14:48:03.282779 29461 net.cpp:91] Creating Layer bn1_2
I0625 14:48:03.282781 29461 net.cpp:425] bn1_2 <- conv1_2
I0625 14:48:03.282785 29461 net.cpp:386] bn1_2 -> conv1_2 (in-place)
I0625 14:48:03.282980 29461 net.cpp:141] Setting up bn1_2
I0625 14:48:03.282987 29461 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 14:48:03.282990 29461 net.cpp:156] Memory required for data: 50577408
I0625 14:48:03.282999 29461 layer_factory.hpp:77] Creating layer scale1_2
I0625 14:48:03.283006 29461 net.cpp:91] Creating Layer scale1_2
I0625 14:48:03.283010 29461 net.cpp:425] scale1_2 <- conv1_2
I0625 14:48:03.283030 29461 net.cpp:386] scale1_2 -> conv1_2 (in-place)
I0625 14:48:03.283062 29461 layer_factory.hpp:77] Creating layer scale1_2
I0625 14:48:03.283840 29461 net.cpp:141] Setting up scale1_2
I0625 14:48:03.283852 29461 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 14:48:03.283854 29461 net.cpp:156] Memory required for data: 58834944
I0625 14:48:03.283859 29461 layer_factory.hpp:77] Creating layer relu1_2
I0625 14:48:03.283865 29461 net.cpp:91] Creating Layer relu1_2
I0625 14:48:03.283867 29461 net.cpp:425] relu1_2 <- conv1_2
I0625 14:48:03.283871 29461 net.cpp:386] relu1_2 -> conv1_2 (in-place)
I0625 14:48:03.284008 29461 net.cpp:141] Setting up relu1_2
I0625 14:48:03.284018 29461 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 14:48:03.284020 29461 net.cpp:156] Memory required for data: 67092480
I0625 14:48:03.284023 29461 layer_factory.hpp:77] Creating layer pool1
I0625 14:48:03.284026 29461 layer_factory.cpp:90] cuDNN does not support multiple tops. Using Caffe's own pooling layer.
I0625 14:48:03.284031 29461 net.cpp:91] Creating Layer pool1
I0625 14:48:03.284034 29461 net.cpp:425] pool1 <- conv1_2
I0625 14:48:03.284037 29461 net.cpp:399] pool1 -> pool1
I0625 14:48:03.284045 29461 net.cpp:399] pool1 -> pool1_mask
I0625 14:48:03.284090 29461 net.cpp:141] Setting up pool1
I0625 14:48:03.284096 29461 net.cpp:148] Top shape: 1 32 112 144 (516096)
I0625 14:48:03.284099 29461 net.cpp:148] Top shape: 1 32 112 144 (516096)
I0625 14:48:03.284101 29461 net.cpp:156] Memory required for data: 71221248
I0625 14:48:03.284103 29461 layer_factory.hpp:77] Creating layer conv2_1
I0625 14:48:03.284111 29461 net.cpp:91] Creating Layer conv2_1
I0625 14:48:03.284113 29461 net.cpp:425] conv2_1 <- pool1
I0625 14:48:03.284116 29461 net.cpp:399] conv2_1 -> conv2_1
I0625 14:48:03.285084 29461 net.cpp:141] Setting up conv2_1
I0625 14:48:03.285096 29461 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 14:48:03.285099 29461 net.cpp:156] Memory required for data: 75350016
I0625 14:48:03.285104 29461 layer_factory.hpp:77] Creating layer bn2_1
I0625 14:48:03.285109 29461 net.cpp:91] Creating Layer bn2_1
I0625 14:48:03.285112 29461 net.cpp:425] bn2_1 <- conv2_1
I0625 14:48:03.285116 29461 net.cpp:386] bn2_1 -> conv2_1 (in-place)
I0625 14:48:03.285275 29461 net.cpp:141] Setting up bn2_1
I0625 14:48:03.285282 29461 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 14:48:03.285284 29461 net.cpp:156] Memory required for data: 79478784
I0625 14:48:03.285290 29461 layer_factory.hpp:77] Creating layer scale2_1
I0625 14:48:03.285295 29461 net.cpp:91] Creating Layer scale2_1
I0625 14:48:03.285298 29461 net.cpp:425] scale2_1 <- conv2_1
I0625 14:48:03.285302 29461 net.cpp:386] scale2_1 -> conv2_1 (in-place)
I0625 14:48:03.285332 29461 layer_factory.hpp:77] Creating layer scale2_1
I0625 14:48:03.285434 29461 net.cpp:141] Setting up scale2_1
I0625 14:48:03.285440 29461 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 14:48:03.285442 29461 net.cpp:156] Memory required for data: 83607552
I0625 14:48:03.285449 29461 layer_factory.hpp:77] Creating layer relu2_1
I0625 14:48:03.285454 29461 net.cpp:91] Creating Layer relu2_1
I0625 14:48:03.285456 29461 net.cpp:425] relu2_1 <- conv2_1
I0625 14:48:03.285460 29461 net.cpp:386] relu2_1 -> conv2_1 (in-place)
I0625 14:48:03.285717 29461 net.cpp:141] Setting up relu2_1
I0625 14:48:03.285727 29461 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 14:48:03.285730 29461 net.cpp:156] Memory required for data: 87736320
I0625 14:48:03.285732 29461 layer_factory.hpp:77] Creating layer conv2_2
I0625 14:48:03.285740 29461 net.cpp:91] Creating Layer conv2_2
I0625 14:48:03.285743 29461 net.cpp:425] conv2_2 <- conv2_1
I0625 14:48:03.285748 29461 net.cpp:399] conv2_2 -> conv2_2
I0625 14:48:03.286742 29461 net.cpp:141] Setting up conv2_2
I0625 14:48:03.286754 29461 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 14:48:03.286757 29461 net.cpp:156] Memory required for data: 91865088
I0625 14:48:03.286761 29461 layer_factory.hpp:77] Creating layer bn2_2
I0625 14:48:03.286769 29461 net.cpp:91] Creating Layer bn2_2
I0625 14:48:03.286782 29461 net.cpp:425] bn2_2 <- conv2_2
I0625 14:48:03.286787 29461 net.cpp:386] bn2_2 -> conv2_2 (in-place)
I0625 14:48:03.286943 29461 net.cpp:141] Setting up bn2_2
I0625 14:48:03.286950 29461 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 14:48:03.286953 29461 net.cpp:156] Memory required for data: 95993856
I0625 14:48:03.286959 29461 layer_factory.hpp:77] Creating layer scale2_2
I0625 14:48:03.286964 29461 net.cpp:91] Creating Layer scale2_2
I0625 14:48:03.286967 29461 net.cpp:425] scale2_2 <- conv2_2
I0625 14:48:03.286972 29461 net.cpp:386] scale2_2 -> conv2_2 (in-place)
I0625 14:48:03.287001 29461 layer_factory.hpp:77] Creating layer scale2_2
I0625 14:48:03.287098 29461 net.cpp:141] Setting up scale2_2
I0625 14:48:03.287106 29461 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 14:48:03.287107 29461 net.cpp:156] Memory required for data: 100122624
I0625 14:48:03.287111 29461 layer_factory.hpp:77] Creating layer relu2_2
I0625 14:48:03.287117 29461 net.cpp:91] Creating Layer relu2_2
I0625 14:48:03.287119 29461 net.cpp:425] relu2_2 <- conv2_2
I0625 14:48:03.287122 29461 net.cpp:386] relu2_2 -> conv2_2 (in-place)
I0625 14:48:03.287398 29461 net.cpp:141] Setting up relu2_2
I0625 14:48:03.287410 29461 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 14:48:03.287412 29461 net.cpp:156] Memory required for data: 104251392
I0625 14:48:03.287415 29461 layer_factory.hpp:77] Creating layer pool2
I0625 14:48:03.287418 29461 layer_factory.cpp:90] cuDNN does not support multiple tops. Using Caffe's own pooling layer.
I0625 14:48:03.287422 29461 net.cpp:91] Creating Layer pool2
I0625 14:48:03.287425 29461 net.cpp:425] pool2 <- conv2_2
I0625 14:48:03.287430 29461 net.cpp:399] pool2 -> pool2
I0625 14:48:03.287434 29461 net.cpp:399] pool2 -> pool2_mask
I0625 14:48:03.287468 29461 net.cpp:141] Setting up pool2
I0625 14:48:03.287474 29461 net.cpp:148] Top shape: 1 64 56 72 (258048)
I0625 14:48:03.287477 29461 net.cpp:148] Top shape: 1 64 56 72 (258048)
I0625 14:48:03.287479 29461 net.cpp:156] Memory required for data: 106315776
I0625 14:48:03.287482 29461 layer_factory.hpp:77] Creating layer conv3_1
I0625 14:48:03.287488 29461 net.cpp:91] Creating Layer conv3_1
I0625 14:48:03.287492 29461 net.cpp:425] conv3_1 <- pool2
I0625 14:48:03.287495 29461 net.cpp:399] conv3_1 -> conv3_1
I0625 14:48:03.289378 29461 net.cpp:141] Setting up conv3_1
I0625 14:48:03.289391 29461 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 14:48:03.289393 29461 net.cpp:156] Memory required for data: 108380160
I0625 14:48:03.289398 29461 layer_factory.hpp:77] Creating layer bn3_1
I0625 14:48:03.289404 29461 net.cpp:91] Creating Layer bn3_1
I0625 14:48:03.289407 29461 net.cpp:425] bn3_1 <- conv3_1
I0625 14:48:03.289412 29461 net.cpp:386] bn3_1 -> conv3_1 (in-place)
I0625 14:48:03.290144 29461 net.cpp:141] Setting up bn3_1
I0625 14:48:03.290155 29461 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 14:48:03.290158 29461 net.cpp:156] Memory required for data: 110444544
I0625 14:48:03.290164 29461 layer_factory.hpp:77] Creating layer scale3_1
I0625 14:48:03.290170 29461 net.cpp:91] Creating Layer scale3_1
I0625 14:48:03.290172 29461 net.cpp:425] scale3_1 <- conv3_1
I0625 14:48:03.290176 29461 net.cpp:386] scale3_1 -> conv3_1 (in-place)
I0625 14:48:03.290210 29461 layer_factory.hpp:77] Creating layer scale3_1
I0625 14:48:03.290298 29461 net.cpp:141] Setting up scale3_1
I0625 14:48:03.290305 29461 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 14:48:03.290307 29461 net.cpp:156] Memory required for data: 112508928
I0625 14:48:03.290312 29461 layer_factory.hpp:77] Creating layer relu3_1
I0625 14:48:03.290315 29461 net.cpp:91] Creating Layer relu3_1
I0625 14:48:03.290318 29461 net.cpp:425] relu3_1 <- conv3_1
I0625 14:48:03.290323 29461 net.cpp:386] relu3_1 -> conv3_1 (in-place)
I0625 14:48:03.290459 29461 net.cpp:141] Setting up relu3_1
I0625 14:48:03.290467 29461 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 14:48:03.290470 29461 net.cpp:156] Memory required for data: 114573312
I0625 14:48:03.290472 29461 layer_factory.hpp:77] Creating layer conv3_2
I0625 14:48:03.290490 29461 net.cpp:91] Creating Layer conv3_2
I0625 14:48:03.290493 29461 net.cpp:425] conv3_2 <- conv3_1
I0625 14:48:03.290498 29461 net.cpp:399] conv3_2 -> conv3_2
I0625 14:48:03.292296 29461 net.cpp:141] Setting up conv3_2
I0625 14:48:03.292310 29461 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 14:48:03.292314 29461 net.cpp:156] Memory required for data: 116637696
I0625 14:48:03.292318 29461 layer_factory.hpp:77] Creating layer bn3_2
I0625 14:48:03.292325 29461 net.cpp:91] Creating Layer bn3_2
I0625 14:48:03.292327 29461 net.cpp:425] bn3_2 <- conv3_2
I0625 14:48:03.292331 29461 net.cpp:386] bn3_2 -> conv3_2 (in-place)
I0625 14:48:03.292481 29461 net.cpp:141] Setting up bn3_2
I0625 14:48:03.292490 29461 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 14:48:03.292491 29461 net.cpp:156] Memory required for data: 118702080
I0625 14:48:03.292500 29461 layer_factory.hpp:77] Creating layer scale3_2
I0625 14:48:03.292506 29461 net.cpp:91] Creating Layer scale3_2
I0625 14:48:03.292510 29461 net.cpp:425] scale3_2 <- conv3_2
I0625 14:48:03.292515 29461 net.cpp:386] scale3_2 -> conv3_2 (in-place)
I0625 14:48:03.292546 29461 layer_factory.hpp:77] Creating layer scale3_2
I0625 14:48:03.292632 29461 net.cpp:141] Setting up scale3_2
I0625 14:48:03.292639 29461 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 14:48:03.292641 29461 net.cpp:156] Memory required for data: 120766464
I0625 14:48:03.292645 29461 layer_factory.hpp:77] Creating layer relu3_2
I0625 14:48:03.292650 29461 net.cpp:91] Creating Layer relu3_2
I0625 14:48:03.292652 29461 net.cpp:425] relu3_2 <- conv3_2
I0625 14:48:03.292655 29461 net.cpp:386] relu3_2 -> conv3_2 (in-place)
I0625 14:48:03.292922 29461 net.cpp:141] Setting up relu3_2
I0625 14:48:03.292932 29461 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 14:48:03.292934 29461 net.cpp:156] Memory required for data: 122830848
I0625 14:48:03.292938 29461 layer_factory.hpp:77] Creating layer pool3
I0625 14:48:03.292942 29461 layer_factory.cpp:90] cuDNN does not support multiple tops. Using Caffe's own pooling layer.
I0625 14:48:03.292946 29461 net.cpp:91] Creating Layer pool3
I0625 14:48:03.292949 29461 net.cpp:425] pool3 <- conv3_2
I0625 14:48:03.292953 29461 net.cpp:399] pool3 -> pool3
I0625 14:48:03.292958 29461 net.cpp:399] pool3 -> pool3_mask
I0625 14:48:03.292994 29461 net.cpp:141] Setting up pool3
I0625 14:48:03.292999 29461 net.cpp:148] Top shape: 1 128 28 36 (129024)
I0625 14:48:03.293002 29461 net.cpp:148] Top shape: 1 128 28 36 (129024)
I0625 14:48:03.293004 29461 net.cpp:156] Memory required for data: 123863040
I0625 14:48:03.293006 29461 layer_factory.hpp:77] Creating layer conv4_1
I0625 14:48:03.293015 29461 net.cpp:91] Creating Layer conv4_1
I0625 14:48:03.293017 29461 net.cpp:425] conv4_1 <- pool3
I0625 14:48:03.293021 29461 net.cpp:399] conv4_1 -> conv4_1
I0625 14:48:03.296151 29461 net.cpp:141] Setting up conv4_1
I0625 14:48:03.296165 29461 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 14:48:03.296169 29461 net.cpp:156] Memory required for data: 124895232
I0625 14:48:03.296172 29461 layer_factory.hpp:77] Creating layer bn4_1
I0625 14:48:03.296178 29461 net.cpp:91] Creating Layer bn4_1
I0625 14:48:03.296181 29461 net.cpp:425] bn4_1 <- conv4_1
I0625 14:48:03.296185 29461 net.cpp:386] bn4_1 -> conv4_1 (in-place)
I0625 14:48:03.296345 29461 net.cpp:141] Setting up bn4_1
I0625 14:48:03.296353 29461 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 14:48:03.296355 29461 net.cpp:156] Memory required for data: 125927424
I0625 14:48:03.296361 29461 layer_factory.hpp:77] Creating layer scale4_1
I0625 14:48:03.296366 29461 net.cpp:91] Creating Layer scale4_1
I0625 14:48:03.296370 29461 net.cpp:425] scale4_1 <- conv4_1
I0625 14:48:03.296373 29461 net.cpp:386] scale4_1 -> conv4_1 (in-place)
I0625 14:48:03.296406 29461 layer_factory.hpp:77] Creating layer scale4_1
I0625 14:48:03.296499 29461 net.cpp:141] Setting up scale4_1
I0625 14:48:03.296505 29461 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 14:48:03.296509 29461 net.cpp:156] Memory required for data: 126959616
I0625 14:48:03.296524 29461 layer_factory.hpp:77] Creating layer relu4_1
I0625 14:48:03.296531 29461 net.cpp:91] Creating Layer relu4_1
I0625 14:48:03.296535 29461 net.cpp:425] relu4_1 <- conv4_1
I0625 14:48:03.296538 29461 net.cpp:386] relu4_1 -> conv4_1 (in-place)
I0625 14:48:03.296803 29461 net.cpp:141] Setting up relu4_1
I0625 14:48:03.296814 29461 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 14:48:03.296816 29461 net.cpp:156] Memory required for data: 127991808
I0625 14:48:03.296819 29461 layer_factory.hpp:77] Creating layer conv4_2
I0625 14:48:03.296828 29461 net.cpp:91] Creating Layer conv4_2
I0625 14:48:03.296830 29461 net.cpp:425] conv4_2 <- conv4_1
I0625 14:48:03.296834 29461 net.cpp:399] conv4_2 -> conv4_2
I0625 14:48:03.303108 29461 net.cpp:141] Setting up conv4_2
I0625 14:48:03.303122 29461 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 14:48:03.303125 29461 net.cpp:156] Memory required for data: 129024000
I0625 14:48:03.303129 29461 layer_factory.hpp:77] Creating layer bn4_2
I0625 14:48:03.303136 29461 net.cpp:91] Creating Layer bn4_2
I0625 14:48:03.303139 29461 net.cpp:425] bn4_2 <- conv4_2
I0625 14:48:03.303143 29461 net.cpp:386] bn4_2 -> conv4_2 (in-place)
I0625 14:48:03.303306 29461 net.cpp:141] Setting up bn4_2
I0625 14:48:03.303314 29461 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 14:48:03.303316 29461 net.cpp:156] Memory required for data: 130056192
I0625 14:48:03.303323 29461 layer_factory.hpp:77] Creating layer scale4_2
I0625 14:48:03.303328 29461 net.cpp:91] Creating Layer scale4_2
I0625 14:48:03.303331 29461 net.cpp:425] scale4_2 <- conv4_2
I0625 14:48:03.303335 29461 net.cpp:386] scale4_2 -> conv4_2 (in-place)
I0625 14:48:03.303367 29461 layer_factory.hpp:77] Creating layer scale4_2
I0625 14:48:03.303458 29461 net.cpp:141] Setting up scale4_2
I0625 14:48:03.303469 29461 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 14:48:03.303472 29461 net.cpp:156] Memory required for data: 131088384
I0625 14:48:03.303477 29461 layer_factory.hpp:77] Creating layer relu4_2
I0625 14:48:03.303480 29461 net.cpp:91] Creating Layer relu4_2
I0625 14:48:03.303483 29461 net.cpp:425] relu4_2 <- conv4_2
I0625 14:48:03.303488 29461 net.cpp:386] relu4_2 -> conv4_2 (in-place)
I0625 14:48:03.303623 29461 net.cpp:141] Setting up relu4_2
I0625 14:48:03.303632 29461 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 14:48:03.303633 29461 net.cpp:156] Memory required for data: 132120576
I0625 14:48:03.303637 29461 layer_factory.hpp:77] Creating layer pool4
I0625 14:48:03.303639 29461 layer_factory.cpp:90] cuDNN does not support multiple tops. Using Caffe's own pooling layer.
I0625 14:48:03.303644 29461 net.cpp:91] Creating Layer pool4
I0625 14:48:03.303647 29461 net.cpp:425] pool4 <- conv4_2
I0625 14:48:03.303652 29461 net.cpp:399] pool4 -> pool4
I0625 14:48:03.303656 29461 net.cpp:399] pool4 -> pool4_mask
I0625 14:48:03.303692 29461 net.cpp:141] Setting up pool4
I0625 14:48:03.303699 29461 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 14:48:03.303701 29461 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 14:48:03.303704 29461 net.cpp:156] Memory required for data: 132636672
I0625 14:48:03.303706 29461 layer_factory.hpp:77] Creating layer conv5_1
I0625 14:48:03.303714 29461 net.cpp:91] Creating Layer conv5_1
I0625 14:48:03.303716 29461 net.cpp:425] conv5_1 <- pool4
I0625 14:48:03.303720 29461 net.cpp:399] conv5_1 -> conv5_1
I0625 14:48:03.316891 29461 net.cpp:141] Setting up conv5_1
I0625 14:48:03.316916 29461 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 14:48:03.316920 29461 net.cpp:156] Memory required for data: 132894720
I0625 14:48:03.316927 29461 layer_factory.hpp:77] Creating layer bn5_1
I0625 14:48:03.316938 29461 net.cpp:91] Creating Layer bn5_1
I0625 14:48:03.316943 29461 net.cpp:425] bn5_1 <- conv5_1
I0625 14:48:03.316948 29461 net.cpp:386] bn5_1 -> conv5_1 (in-place)
I0625 14:48:03.317123 29461 net.cpp:141] Setting up bn5_1
I0625 14:48:03.317131 29461 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 14:48:03.317133 29461 net.cpp:156] Memory required for data: 133152768
I0625 14:48:03.317154 29461 layer_factory.hpp:77] Creating layer scale5_1
I0625 14:48:03.317163 29461 net.cpp:91] Creating Layer scale5_1
I0625 14:48:03.317167 29461 net.cpp:425] scale5_1 <- conv5_1
I0625 14:48:03.317170 29461 net.cpp:386] scale5_1 -> conv5_1 (in-place)
I0625 14:48:03.317209 29461 layer_factory.hpp:77] Creating layer scale5_1
I0625 14:48:03.317298 29461 net.cpp:141] Setting up scale5_1
I0625 14:48:03.317306 29461 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 14:48:03.317307 29461 net.cpp:156] Memory required for data: 133410816
I0625 14:48:03.317312 29461 layer_factory.hpp:77] Creating layer relu5_1
I0625 14:48:03.317317 29461 net.cpp:91] Creating Layer relu5_1
I0625 14:48:03.317319 29461 net.cpp:425] relu5_1 <- conv5_1
I0625 14:48:03.317323 29461 net.cpp:386] relu5_1 -> conv5_1 (in-place)
I0625 14:48:03.317591 29461 net.cpp:141] Setting up relu5_1
I0625 14:48:03.317602 29461 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 14:48:03.317605 29461 net.cpp:156] Memory required for data: 133668864
I0625 14:48:03.317608 29461 layer_factory.hpp:77] Creating layer conv5_2
I0625 14:48:03.317617 29461 net.cpp:91] Creating Layer conv5_2
I0625 14:48:03.317620 29461 net.cpp:425] conv5_2 <- conv5_1
I0625 14:48:03.317625 29461 net.cpp:399] conv5_2 -> conv5_2
I0625 14:48:03.322960 29461 net.cpp:141] Setting up conv5_2
I0625 14:48:03.322973 29461 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 14:48:03.322976 29461 net.cpp:156] Memory required for data: 133926912
I0625 14:48:03.322981 29461 layer_factory.hpp:77] Creating layer bn5_2
I0625 14:48:03.322988 29461 net.cpp:91] Creating Layer bn5_2
I0625 14:48:03.322990 29461 net.cpp:425] bn5_2 <- conv5_2
I0625 14:48:03.322994 29461 net.cpp:386] bn5_2 -> conv5_2 (in-place)
I0625 14:48:03.323146 29461 net.cpp:141] Setting up bn5_2
I0625 14:48:03.323158 29461 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 14:48:03.323160 29461 net.cpp:156] Memory required for data: 134184960
I0625 14:48:03.323165 29461 layer_factory.hpp:77] Creating layer scale5_2
I0625 14:48:03.323171 29461 net.cpp:91] Creating Layer scale5_2
I0625 14:48:03.323174 29461 net.cpp:425] scale5_2 <- conv5_2
I0625 14:48:03.323179 29461 net.cpp:386] scale5_2 -> conv5_2 (in-place)
I0625 14:48:03.323212 29461 layer_factory.hpp:77] Creating layer scale5_2
I0625 14:48:03.323298 29461 net.cpp:141] Setting up scale5_2
I0625 14:48:03.323304 29461 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 14:48:03.323307 29461 net.cpp:156] Memory required for data: 134443008
I0625 14:48:03.323312 29461 layer_factory.hpp:77] Creating layer relu5_2
I0625 14:48:03.323315 29461 net.cpp:91] Creating Layer relu5_2
I0625 14:48:03.323318 29461 net.cpp:425] relu5_2 <- conv5_2
I0625 14:48:03.323321 29461 net.cpp:386] relu5_2 -> conv5_2 (in-place)
I0625 14:48:03.323586 29461 net.cpp:141] Setting up relu5_2
I0625 14:48:03.323597 29461 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 14:48:03.323601 29461 net.cpp:156] Memory required for data: 134701056
I0625 14:48:03.323603 29461 layer_factory.hpp:77] Creating layer pool5
I0625 14:48:03.323606 29461 layer_factory.cpp:90] cuDNN does not support multiple tops. Using Caffe's own pooling layer.
I0625 14:48:03.323611 29461 net.cpp:91] Creating Layer pool5
I0625 14:48:03.323614 29461 net.cpp:425] pool5 <- conv5_2
I0625 14:48:03.323619 29461 net.cpp:399] pool5 -> pool5
I0625 14:48:03.323624 29461 net.cpp:399] pool5 -> pool5_mask
I0625 14:48:03.323662 29461 net.cpp:141] Setting up pool5
I0625 14:48:03.323668 29461 net.cpp:148] Top shape: 1 256 7 9 (16128)
I0625 14:48:03.323671 29461 net.cpp:148] Top shape: 1 256 7 9 (16128)
I0625 14:48:03.323673 29461 net.cpp:156] Memory required for data: 134830080
I0625 14:48:03.323675 29461 layer_factory.hpp:77] Creating layer upsample5
I0625 14:48:03.323683 29461 net.cpp:91] Creating Layer upsample5
I0625 14:48:03.323684 29461 net.cpp:425] upsample5 <- pool5
I0625 14:48:03.323688 29461 net.cpp:425] upsample5 <- pool5_mask
I0625 14:48:03.323691 29461 net.cpp:399] upsample5 -> pool5_D
I0625 14:48:03.323699 29461 upsample_layer.cpp:31] Params 'pad_out_{}_' are deprecated. Please declare upsample height and width useing the upsample_h, upsample_w parameters.
I0625 14:48:03.323731 29461 net.cpp:141] Setting up upsample5
I0625 14:48:03.323739 29461 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 14:48:03.323740 29461 net.cpp:156] Memory required for data: 135088128
I0625 14:48:03.323742 29461 layer_factory.hpp:77] Creating layer conv5_2_D
I0625 14:48:03.323750 29461 net.cpp:91] Creating Layer conv5_2_D
I0625 14:48:03.323752 29461 net.cpp:425] conv5_2_D <- pool5_D
I0625 14:48:03.323756 29461 net.cpp:399] conv5_2_D -> conv5_2_D
I0625 14:48:03.329126 29461 net.cpp:141] Setting up conv5_2_D
I0625 14:48:03.329139 29461 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 14:48:03.329143 29461 net.cpp:156] Memory required for data: 135346176
I0625 14:48:03.329146 29461 layer_factory.hpp:77] Creating layer bn5_2_D
I0625 14:48:03.329152 29461 net.cpp:91] Creating Layer bn5_2_D
I0625 14:48:03.329155 29461 net.cpp:425] bn5_2_D <- conv5_2_D
I0625 14:48:03.329160 29461 net.cpp:386] bn5_2_D -> conv5_2_D (in-place)
I0625 14:48:03.329313 29461 net.cpp:141] Setting up bn5_2_D
I0625 14:48:03.329320 29461 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 14:48:03.329322 29461 net.cpp:156] Memory required for data: 135604224
I0625 14:48:03.329329 29461 layer_factory.hpp:77] Creating layer scale5_2_D
I0625 14:48:03.329334 29461 net.cpp:91] Creating Layer scale5_2_D
I0625 14:48:03.329336 29461 net.cpp:425] scale5_2_D <- conv5_2_D
I0625 14:48:03.329339 29461 net.cpp:386] scale5_2_D -> conv5_2_D (in-place)
I0625 14:48:03.329371 29461 layer_factory.hpp:77] Creating layer scale5_2_D
I0625 14:48:03.329458 29461 net.cpp:141] Setting up scale5_2_D
I0625 14:48:03.329464 29461 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 14:48:03.329468 29461 net.cpp:156] Memory required for data: 135862272
I0625 14:48:03.329479 29461 layer_factory.hpp:77] Creating layer relu5_2_D
I0625 14:48:03.329484 29461 net.cpp:91] Creating Layer relu5_2_D
I0625 14:48:03.329486 29461 net.cpp:425] relu5_2_D <- conv5_2_D
I0625 14:48:03.329490 29461 net.cpp:386] relu5_2_D -> conv5_2_D (in-place)
I0625 14:48:03.329627 29461 net.cpp:141] Setting up relu5_2_D
I0625 14:48:03.329637 29461 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 14:48:03.329638 29461 net.cpp:156] Memory required for data: 136120320
I0625 14:48:03.329641 29461 layer_factory.hpp:77] Creating layer conv5_1_D
I0625 14:48:03.329648 29461 net.cpp:91] Creating Layer conv5_1_D
I0625 14:48:03.329651 29461 net.cpp:425] conv5_1_D <- conv5_2_D
I0625 14:48:03.329656 29461 net.cpp:399] conv5_1_D -> conv5_1_D
I0625 14:48:03.334967 29461 net.cpp:141] Setting up conv5_1_D
I0625 14:48:03.334980 29461 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 14:48:03.334982 29461 net.cpp:156] Memory required for data: 136378368
I0625 14:48:03.334987 29461 layer_factory.hpp:77] Creating layer bn5_1_D
I0625 14:48:03.334993 29461 net.cpp:91] Creating Layer bn5_1_D
I0625 14:48:03.334996 29461 net.cpp:425] bn5_1_D <- conv5_1_D
I0625 14:48:03.335000 29461 net.cpp:386] bn5_1_D -> conv5_1_D (in-place)
I0625 14:48:03.335161 29461 net.cpp:141] Setting up bn5_1_D
I0625 14:48:03.335170 29461 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 14:48:03.335171 29461 net.cpp:156] Memory required for data: 136636416
I0625 14:48:03.335177 29461 layer_factory.hpp:77] Creating layer scale5_1_D
I0625 14:48:03.335183 29461 net.cpp:91] Creating Layer scale5_1_D
I0625 14:48:03.335186 29461 net.cpp:425] scale5_1_D <- conv5_1_D
I0625 14:48:03.335189 29461 net.cpp:386] scale5_1_D -> conv5_1_D (in-place)
I0625 14:48:03.335224 29461 layer_factory.hpp:77] Creating layer scale5_1_D
I0625 14:48:03.335333 29461 net.cpp:141] Setting up scale5_1_D
I0625 14:48:03.335343 29461 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 14:48:03.335347 29461 net.cpp:156] Memory required for data: 136894464
I0625 14:48:03.335355 29461 layer_factory.hpp:77] Creating layer relu5_1_D
I0625 14:48:03.335362 29461 net.cpp:91] Creating Layer relu5_1_D
I0625 14:48:03.335367 29461 net.cpp:425] relu5_1_D <- conv5_1_D
I0625 14:48:03.335388 29461 net.cpp:386] relu5_1_D -> conv5_1_D (in-place)
I0625 14:48:03.335667 29461 net.cpp:141] Setting up relu5_1_D
I0625 14:48:03.335680 29461 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 14:48:03.335685 29461 net.cpp:156] Memory required for data: 137152512
I0625 14:48:03.335690 29461 layer_factory.hpp:77] Creating layer upsample4
I0625 14:48:03.335697 29461 net.cpp:91] Creating Layer upsample4
I0625 14:48:03.335703 29461 net.cpp:425] upsample4 <- conv5_1_D
I0625 14:48:03.335710 29461 net.cpp:425] upsample4 <- pool4_mask
I0625 14:48:03.335716 29461 net.cpp:399] upsample4 -> pool4_D
I0625 14:48:03.335726 29461 upsample_layer.cpp:31] Params 'pad_out_{}_' are deprecated. Please declare upsample height and width useing the upsample_h, upsample_w parameters.
I0625 14:48:03.335757 29461 net.cpp:141] Setting up upsample4
I0625 14:48:03.335765 29461 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 14:48:03.335769 29461 net.cpp:156] Memory required for data: 138184704
I0625 14:48:03.335773 29461 layer_factory.hpp:77] Creating layer conv4_2_D
I0625 14:48:03.335783 29461 net.cpp:91] Creating Layer conv4_2_D
I0625 14:48:03.335788 29461 net.cpp:425] conv4_2_D <- pool4_D
I0625 14:48:03.335795 29461 net.cpp:399] conv4_2_D -> conv4_2_D
I0625 14:48:03.341034 29461 net.cpp:141] Setting up conv4_2_D
I0625 14:48:03.341050 29461 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 14:48:03.341055 29461 net.cpp:156] Memory required for data: 139216896
I0625 14:48:03.341063 29461 layer_factory.hpp:77] Creating layer bn4_2_D
I0625 14:48:03.341073 29461 net.cpp:91] Creating Layer bn4_2_D
I0625 14:48:03.341078 29461 net.cpp:425] bn4_2_D <- conv4_2_D
I0625 14:48:03.341084 29461 net.cpp:386] bn4_2_D -> conv4_2_D (in-place)
I0625 14:48:03.341264 29461 net.cpp:141] Setting up bn4_2_D
I0625 14:48:03.341274 29461 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 14:48:03.341279 29461 net.cpp:156] Memory required for data: 140249088
I0625 14:48:03.341289 29461 layer_factory.hpp:77] Creating layer scale4_2_D
I0625 14:48:03.341298 29461 net.cpp:91] Creating Layer scale4_2_D
I0625 14:48:03.341303 29461 net.cpp:425] scale4_2_D <- conv4_2_D
I0625 14:48:03.341310 29461 net.cpp:386] scale4_2_D -> conv4_2_D (in-place)
I0625 14:48:03.341356 29461 layer_factory.hpp:77] Creating layer scale4_2_D
I0625 14:48:03.341506 29461 net.cpp:141] Setting up scale4_2_D
I0625 14:48:03.341522 29461 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 14:48:03.341526 29461 net.cpp:156] Memory required for data: 141281280
I0625 14:48:03.341533 29461 layer_factory.hpp:77] Creating layer relu4_2_D
I0625 14:48:03.341542 29461 net.cpp:91] Creating Layer relu4_2_D
I0625 14:48:03.341545 29461 net.cpp:425] relu4_2_D <- conv4_2_D
I0625 14:48:03.341552 29461 net.cpp:386] relu4_2_D -> conv4_2_D (in-place)
I0625 14:48:03.341874 29461 net.cpp:141] Setting up relu4_2_D
I0625 14:48:03.341886 29461 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 14:48:03.341889 29461 net.cpp:156] Memory required for data: 142313472
I0625 14:48:03.341892 29461 layer_factory.hpp:77] Creating layer conv4_1_D
I0625 14:48:03.341900 29461 net.cpp:91] Creating Layer conv4_1_D
I0625 14:48:03.341903 29461 net.cpp:425] conv4_1_D <- conv4_2_D
I0625 14:48:03.341908 29461 net.cpp:399] conv4_1_D -> conv4_1_D
I0625 14:48:03.345142 29461 net.cpp:141] Setting up conv4_1_D
I0625 14:48:03.345156 29461 net.cpp:148] Top shape: 1 128 28 36 (129024)
I0625 14:48:03.345160 29461 net.cpp:156] Memory required for data: 142829568
I0625 14:48:03.345165 29461 layer_factory.hpp:77] Creating layer bn4_1_D
I0625 14:48:03.345170 29461 net.cpp:91] Creating Layer bn4_1_D
I0625 14:48:03.345175 29461 net.cpp:425] bn4_1_D <- conv4_1_D
I0625 14:48:03.345178 29461 net.cpp:386] bn4_1_D -> conv4_1_D (in-place)
I0625 14:48:03.345345 29461 net.cpp:141] Setting up bn4_1_D
I0625 14:48:03.345358 29461 net.cpp:148] Top shape: 1 128 28 36 (129024)
I0625 14:48:03.345361 29461 net.cpp:156] Memory required for data: 143345664
I0625 14:48:03.345366 29461 layer_factory.hpp:77] Creating layer scale4_1_D
I0625 14:48:03.345372 29461 net.cpp:91] Creating Layer scale4_1_D
I0625 14:48:03.345387 29461 net.cpp:425] scale4_1_D <- conv4_1_D
I0625 14:48:03.345392 29461 net.cpp:386] scale4_1_D -> conv4_1_D (in-place)
I0625 14:48:03.345428 29461 layer_factory.hpp:77] Creating layer scale4_1_D
I0625 14:48:03.345526 29461 net.cpp:141] Setting up scale4_1_D
I0625 14:48:03.345532 29461 net.cpp:148] Top shape: 1 128 28 36 (129024)
I0625 14:48:03.345535 29461 net.cpp:156] Memory required for data: 143861760
I0625 14:48:03.345541 29461 layer_factory.hpp:77] Creating layer relu4_1_D
I0625 14:48:03.345551 29461 net.cpp:91] Creating Layer relu4_1_D
I0625 14:48:03.345556 29461 net.cpp:425] relu4_1_D <- conv4_1_D
I0625 14:48:03.345562 29461 net.cpp:386] relu4_1_D -> conv4_1_D (in-place)
I0625 14:48:03.345718 29461 net.cpp:141] Setting up relu4_1_D
I0625 14:48:03.345729 29461 net.cpp:148] Top shape: 1 128 28 36 (129024)
I0625 14:48:03.345733 29461 net.cpp:156] Memory required for data: 144377856
I0625 14:48:03.345739 29461 layer_factory.hpp:77] Creating layer upsample3
I0625 14:48:03.345748 29461 net.cpp:91] Creating Layer upsample3
I0625 14:48:03.345753 29461 net.cpp:425] upsample3 <- conv4_1_D
I0625 14:48:03.345759 29461 net.cpp:425] upsample3 <- pool3_mask
I0625 14:48:03.345767 29461 net.cpp:399] upsample3 -> pool3_D
I0625 14:48:03.345775 29461 upsample_layer.cpp:31] Params 'pad_out_{}_' are deprecated. Please declare upsample height and width useing the upsample_h, upsample_w parameters.
I0625 14:48:03.345808 29461 net.cpp:141] Setting up upsample3
I0625 14:48:03.345816 29461 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 14:48:03.345820 29461 net.cpp:156] Memory required for data: 146442240
I0625 14:48:03.345824 29461 layer_factory.hpp:77] Creating layer conv3_2_D
I0625 14:48:03.345836 29461 net.cpp:91] Creating Layer conv3_2_D
I0625 14:48:03.345840 29461 net.cpp:425] conv3_2_D <- pool3_D
I0625 14:48:03.345849 29461 net.cpp:399] conv3_2_D -> conv3_2_D
I0625 14:48:03.348280 29461 net.cpp:141] Setting up conv3_2_D
I0625 14:48:03.348295 29461 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 14:48:03.348299 29461 net.cpp:156] Memory required for data: 148506624
I0625 14:48:03.348309 29461 layer_factory.hpp:77] Creating layer bn3_2_D
I0625 14:48:03.348318 29461 net.cpp:91] Creating Layer bn3_2_D
I0625 14:48:03.348323 29461 net.cpp:425] bn3_2_D <- conv3_2_D
I0625 14:48:03.348330 29461 net.cpp:386] bn3_2_D -> conv3_2_D (in-place)
I0625 14:48:03.348508 29461 net.cpp:141] Setting up bn3_2_D
I0625 14:48:03.348518 29461 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 14:48:03.348522 29461 net.cpp:156] Memory required for data: 150571008
I0625 14:48:03.348531 29461 layer_factory.hpp:77] Creating layer scale3_2_D
I0625 14:48:03.348541 29461 net.cpp:91] Creating Layer scale3_2_D
I0625 14:48:03.348546 29461 net.cpp:425] scale3_2_D <- conv3_2_D
I0625 14:48:03.348552 29461 net.cpp:386] scale3_2_D -> conv3_2_D (in-place)
I0625 14:48:03.348599 29461 layer_factory.hpp:77] Creating layer scale3_2_D
I0625 14:48:03.348712 29461 net.cpp:141] Setting up scale3_2_D
I0625 14:48:03.348722 29461 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 14:48:03.348726 29461 net.cpp:156] Memory required for data: 152635392
I0625 14:48:03.348734 29461 layer_factory.hpp:77] Creating layer relu3_2_D
I0625 14:48:03.348742 29461 net.cpp:91] Creating Layer relu3_2_D
I0625 14:48:03.348745 29461 net.cpp:425] relu3_2_D <- conv3_2_D
I0625 14:48:03.348752 29461 net.cpp:386] relu3_2_D -> conv3_2_D (in-place)
I0625 14:48:03.349035 29461 net.cpp:141] Setting up relu3_2_D
I0625 14:48:03.349046 29461 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 14:48:03.349050 29461 net.cpp:156] Memory required for data: 154699776
I0625 14:48:03.349056 29461 layer_factory.hpp:77] Creating layer conv3_1_D
I0625 14:48:03.349066 29461 net.cpp:91] Creating Layer conv3_1_D
I0625 14:48:03.349072 29461 net.cpp:425] conv3_1_D <- conv3_2_D
I0625 14:48:03.349081 29461 net.cpp:399] conv3_1_D -> conv3_1_D
I0625 14:48:03.350420 29461 net.cpp:141] Setting up conv3_1_D
I0625 14:48:03.350435 29461 net.cpp:148] Top shape: 1 64 56 72 (258048)
I0625 14:48:03.350456 29461 net.cpp:156] Memory required for data: 155731968
I0625 14:48:03.350466 29461 layer_factory.hpp:77] Creating layer bn3_1_D
I0625 14:48:03.350476 29461 net.cpp:91] Creating Layer bn3_1_D
I0625 14:48:03.350481 29461 net.cpp:425] bn3_1_D <- conv3_1_D
I0625 14:48:03.350487 29461 net.cpp:386] bn3_1_D -> conv3_1_D (in-place)
I0625 14:48:03.350670 29461 net.cpp:141] Setting up bn3_1_D
I0625 14:48:03.350680 29461 net.cpp:148] Top shape: 1 64 56 72 (258048)
I0625 14:48:03.350684 29461 net.cpp:156] Memory required for data: 156764160
I0625 14:48:03.350694 29461 layer_factory.hpp:77] Creating layer scale3_1_D
I0625 14:48:03.350703 29461 net.cpp:91] Creating Layer scale3_1_D
I0625 14:48:03.350708 29461 net.cpp:425] scale3_1_D <- conv3_1_D
I0625 14:48:03.350715 29461 net.cpp:386] scale3_1_D -> conv3_1_D (in-place)
I0625 14:48:03.350764 29461 layer_factory.hpp:77] Creating layer scale3_1_D
I0625 14:48:03.350881 29461 net.cpp:141] Setting up scale3_1_D
I0625 14:48:03.350890 29461 net.cpp:148] Top shape: 1 64 56 72 (258048)
I0625 14:48:03.350894 29461 net.cpp:156] Memory required for data: 157796352
I0625 14:48:03.350903 29461 layer_factory.hpp:77] Creating layer relu3_1_D
I0625 14:48:03.350909 29461 net.cpp:91] Creating Layer relu3_1_D
I0625 14:48:03.350914 29461 net.cpp:425] relu3_1_D <- conv3_1_D
I0625 14:48:03.350920 29461 net.cpp:386] relu3_1_D -> conv3_1_D (in-place)
I0625 14:48:03.351202 29461 net.cpp:141] Setting up relu3_1_D
I0625 14:48:03.351215 29461 net.cpp:148] Top shape: 1 64 56 72 (258048)
I0625 14:48:03.351219 29461 net.cpp:156] Memory required for data: 158828544
I0625 14:48:03.351224 29461 layer_factory.hpp:77] Creating layer upsample2
I0625 14:48:03.351233 29461 net.cpp:91] Creating Layer upsample2
I0625 14:48:03.351238 29461 net.cpp:425] upsample2 <- conv3_1_D
I0625 14:48:03.351243 29461 net.cpp:425] upsample2 <- pool2_mask
I0625 14:48:03.351249 29461 net.cpp:399] upsample2 -> pool2_D
I0625 14:48:03.351258 29461 upsample_layer.cpp:31] Params 'pad_out_{}_' are deprecated. Please declare upsample height and width useing the upsample_h, upsample_w parameters.
I0625 14:48:03.351292 29461 net.cpp:141] Setting up upsample2
I0625 14:48:03.351300 29461 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 14:48:03.351305 29461 net.cpp:156] Memory required for data: 162957312
I0625 14:48:03.351308 29461 layer_factory.hpp:77] Creating layer conv2_2_D
I0625 14:48:03.351320 29461 net.cpp:91] Creating Layer conv2_2_D
I0625 14:48:03.351325 29461 net.cpp:425] conv2_2_D <- pool2_D
I0625 14:48:03.351332 29461 net.cpp:399] conv2_2_D -> conv2_2_D
I0625 14:48:03.352501 29461 net.cpp:141] Setting up conv2_2_D
I0625 14:48:03.352514 29461 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 14:48:03.352519 29461 net.cpp:156] Memory required for data: 167086080
I0625 14:48:03.352525 29461 layer_factory.hpp:77] Creating layer bn2_2_D
I0625 14:48:03.352535 29461 net.cpp:91] Creating Layer bn2_2_D
I0625 14:48:03.352540 29461 net.cpp:425] bn2_2_D <- conv2_2_D
I0625 14:48:03.352547 29461 net.cpp:386] bn2_2_D -> conv2_2_D (in-place)
I0625 14:48:03.352741 29461 net.cpp:141] Setting up bn2_2_D
I0625 14:48:03.352749 29461 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 14:48:03.352753 29461 net.cpp:156] Memory required for data: 171214848
I0625 14:48:03.352762 29461 layer_factory.hpp:77] Creating layer scale2_2_D
I0625 14:48:03.352772 29461 net.cpp:91] Creating Layer scale2_2_D
I0625 14:48:03.352777 29461 net.cpp:425] scale2_2_D <- conv2_2_D
I0625 14:48:03.352783 29461 net.cpp:386] scale2_2_D -> conv2_2_D (in-place)
I0625 14:48:03.352830 29461 layer_factory.hpp:77] Creating layer scale2_2_D
I0625 14:48:03.352964 29461 net.cpp:141] Setting up scale2_2_D
I0625 14:48:03.352974 29461 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 14:48:03.352979 29461 net.cpp:156] Memory required for data: 175343616
I0625 14:48:03.352987 29461 layer_factory.hpp:77] Creating layer relu2_2_D
I0625 14:48:03.352993 29461 net.cpp:91] Creating Layer relu2_2_D
I0625 14:48:03.352998 29461 net.cpp:425] relu2_2_D <- conv2_2_D
I0625 14:48:03.353004 29461 net.cpp:386] relu2_2_D -> conv2_2_D (in-place)
I0625 14:48:03.353164 29461 net.cpp:141] Setting up relu2_2_D
I0625 14:48:03.353175 29461 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 14:48:03.353179 29461 net.cpp:156] Memory required for data: 179472384
I0625 14:48:03.353184 29461 layer_factory.hpp:77] Creating layer conv2_1_D
I0625 14:48:03.353195 29461 net.cpp:91] Creating Layer conv2_1_D
I0625 14:48:03.353200 29461 net.cpp:425] conv2_1_D <- conv2_2_D
I0625 14:48:03.353207 29461 net.cpp:399] conv2_1_D -> conv2_1_D
I0625 14:48:03.354166 29461 net.cpp:141] Setting up conv2_1_D
I0625 14:48:03.354181 29461 net.cpp:148] Top shape: 1 32 112 144 (516096)
I0625 14:48:03.354184 29461 net.cpp:156] Memory required for data: 181536768
I0625 14:48:03.354192 29461 layer_factory.hpp:77] Creating layer bn2_1_D
I0625 14:48:03.354200 29461 net.cpp:91] Creating Layer bn2_1_D
I0625 14:48:03.354205 29461 net.cpp:425] bn2_1_D <- conv2_1_D
I0625 14:48:03.354212 29461 net.cpp:386] bn2_1_D -> conv2_1_D (in-place)
I0625 14:48:03.354408 29461 net.cpp:141] Setting up bn2_1_D
I0625 14:48:03.354418 29461 net.cpp:148] Top shape: 1 32 112 144 (516096)
I0625 14:48:03.354421 29461 net.cpp:156] Memory required for data: 183601152
I0625 14:48:03.354430 29461 layer_factory.hpp:77] Creating layer scale2_1_D
I0625 14:48:03.354439 29461 net.cpp:91] Creating Layer scale2_1_D
I0625 14:48:03.354444 29461 net.cpp:425] scale2_1_D <- conv2_1_D
I0625 14:48:03.354451 29461 net.cpp:386] scale2_1_D -> conv2_1_D (in-place)
I0625 14:48:03.354498 29461 layer_factory.hpp:77] Creating layer scale2_1_D
I0625 14:48:03.354634 29461 net.cpp:141] Setting up scale2_1_D
I0625 14:48:03.354643 29461 net.cpp:148] Top shape: 1 32 112 144 (516096)
I0625 14:48:03.354647 29461 net.cpp:156] Memory required for data: 185665536
I0625 14:48:03.354655 29461 layer_factory.hpp:77] Creating layer relu2_1_D
I0625 14:48:03.354662 29461 net.cpp:91] Creating Layer relu2_1_D
I0625 14:48:03.354667 29461 net.cpp:425] relu2_1_D <- conv2_1_D
I0625 14:48:03.354673 29461 net.cpp:386] relu2_1_D -> conv2_1_D (in-place)
I0625 14:48:03.354951 29461 net.cpp:141] Setting up relu2_1_D
I0625 14:48:03.354964 29461 net.cpp:148] Top shape: 1 32 112 144 (516096)
I0625 14:48:03.354969 29461 net.cpp:156] Memory required for data: 187729920
I0625 14:48:03.354972 29461 layer_factory.hpp:77] Creating layer upsample1
I0625 14:48:03.354981 29461 net.cpp:91] Creating Layer upsample1
I0625 14:48:03.354986 29461 net.cpp:425] upsample1 <- conv2_1_D
I0625 14:48:03.354992 29461 net.cpp:425] upsample1 <- pool1_mask
I0625 14:48:03.355000 29461 net.cpp:399] upsample1 -> pool1_D
I0625 14:48:03.355007 29461 upsample_layer.cpp:31] Params 'pad_out_{}_' are deprecated. Please declare upsample height and width useing the upsample_h, upsample_w parameters.
I0625 14:48:03.355041 29461 net.cpp:141] Setting up upsample1
I0625 14:48:03.355048 29461 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 14:48:03.355052 29461 net.cpp:156] Memory required for data: 195987456
I0625 14:48:03.355057 29461 layer_factory.hpp:77] Creating layer conv1_2_D
I0625 14:48:03.355067 29461 net.cpp:91] Creating Layer conv1_2_D
I0625 14:48:03.355072 29461 net.cpp:425] conv1_2_D <- pool1_D
I0625 14:48:03.355079 29461 net.cpp:399] conv1_2_D -> conv1_2_D
I0625 14:48:03.355998 29461 net.cpp:141] Setting up conv1_2_D
I0625 14:48:03.356011 29461 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 14:48:03.356015 29461 net.cpp:156] Memory required for data: 204244992
I0625 14:48:03.356022 29461 layer_factory.hpp:77] Creating layer bn1_2_D
I0625 14:48:03.356032 29461 net.cpp:91] Creating Layer bn1_2_D
I0625 14:48:03.356037 29461 net.cpp:425] bn1_2_D <- conv1_2_D
I0625 14:48:03.356043 29461 net.cpp:386] bn1_2_D -> conv1_2_D (in-place)
I0625 14:48:03.356288 29461 net.cpp:141] Setting up bn1_2_D
I0625 14:48:03.356298 29461 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 14:48:03.356302 29461 net.cpp:156] Memory required for data: 212502528
I0625 14:48:03.356313 29461 layer_factory.hpp:77] Creating layer scale1_2_D
I0625 14:48:03.356323 29461 net.cpp:91] Creating Layer scale1_2_D
I0625 14:48:03.356338 29461 net.cpp:425] scale1_2_D <- conv1_2_D
I0625 14:48:03.356346 29461 net.cpp:386] scale1_2_D -> conv1_2_D (in-place)
I0625 14:48:03.356395 29461 layer_factory.hpp:77] Creating layer scale1_2_D
I0625 14:48:03.357182 29461 net.cpp:141] Setting up scale1_2_D
I0625 14:48:03.357194 29461 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 14:48:03.357199 29461 net.cpp:156] Memory required for data: 220760064
I0625 14:48:03.357208 29461 layer_factory.hpp:77] Creating layer relu1_2_D
I0625 14:48:03.357215 29461 net.cpp:91] Creating Layer relu1_2_D
I0625 14:48:03.357220 29461 net.cpp:425] relu1_2_D <- conv1_2_D
I0625 14:48:03.357228 29461 net.cpp:386] relu1_2_D -> conv1_2_D (in-place)
I0625 14:48:03.357511 29461 net.cpp:141] Setting up relu1_2_D
I0625 14:48:03.357523 29461 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 14:48:03.357528 29461 net.cpp:156] Memory required for data: 229017600
I0625 14:48:03.357532 29461 layer_factory.hpp:77] Creating layer conv1_1_D
I0625 14:48:03.357545 29461 net.cpp:91] Creating Layer conv1_1_D
I0625 14:48:03.357550 29461 net.cpp:425] conv1_1_D <- conv1_2_D
I0625 14:48:03.357558 29461 net.cpp:399] conv1_1_D -> conv1_1_D
I0625 14:48:03.358623 29461 net.cpp:141] Setting up conv1_1_D
I0625 14:48:03.358638 29461 net.cpp:148] Top shape: 1 2 224 288 (129024)
I0625 14:48:03.358641 29461 net.cpp:156] Memory required for data: 229533696
I0625 14:48:03.358649 29461 layer_factory.hpp:77] Creating layer conv1_1_D_conv1_1_D_0_split
I0625 14:48:03.358659 29461 net.cpp:91] Creating Layer conv1_1_D_conv1_1_D_0_split
I0625 14:48:03.358664 29461 net.cpp:425] conv1_1_D_conv1_1_D_0_split <- conv1_1_D
I0625 14:48:03.358670 29461 net.cpp:399] conv1_1_D_conv1_1_D_0_split -> conv1_1_D_conv1_1_D_0_split_0
I0625 14:48:03.358680 29461 net.cpp:399] conv1_1_D_conv1_1_D_0_split -> conv1_1_D_conv1_1_D_0_split_1
I0625 14:48:03.358729 29461 net.cpp:141] Setting up conv1_1_D_conv1_1_D_0_split
I0625 14:48:03.358739 29461 net.cpp:148] Top shape: 1 2 224 288 (129024)
I0625 14:48:03.358744 29461 net.cpp:148] Top shape: 1 2 224 288 (129024)
I0625 14:48:03.358748 29461 net.cpp:156] Memory required for data: 230565888
I0625 14:48:03.358752 29461 layer_factory.hpp:77] Creating layer loss
I0625 14:48:03.358760 29461 net.cpp:91] Creating Layer loss
I0625 14:48:03.358765 29461 net.cpp:425] loss <- conv1_1_D_conv1_1_D_0_split_0
I0625 14:48:03.358772 29461 net.cpp:425] loss <- label_data_1_split_0
I0625 14:48:03.358778 29461 net.cpp:399] loss -> loss
I0625 14:48:03.358789 29461 layer_factory.hpp:77] Creating layer loss
I0625 14:48:03.359689 29461 net.cpp:141] Setting up loss
I0625 14:48:03.359704 29461 net.cpp:148] Top shape: (1)
I0625 14:48:03.359709 29461 net.cpp:151]     with loss weight 1
I0625 14:48:03.359730 29461 net.cpp:156] Memory required for data: 230565892
I0625 14:48:03.359736 29461 layer_factory.hpp:77] Creating layer accuracy
I0625 14:48:03.359745 29461 net.cpp:91] Creating Layer accuracy
I0625 14:48:03.359750 29461 net.cpp:425] accuracy <- conv1_1_D_conv1_1_D_0_split_1
I0625 14:48:03.359756 29461 net.cpp:425] accuracy <- label_data_1_split_1
I0625 14:48:03.359762 29461 net.cpp:399] accuracy -> accuracy
I0625 14:48:03.359773 29461 net.cpp:141] Setting up accuracy
I0625 14:48:03.359781 29461 net.cpp:148] Top shape: (1)
I0625 14:48:03.359786 29461 net.cpp:156] Memory required for data: 230565896
I0625 14:48:03.359789 29461 net.cpp:219] accuracy does not need backward computation.
I0625 14:48:03.359794 29461 net.cpp:217] loss needs backward computation.
I0625 14:48:03.359799 29461 net.cpp:217] conv1_1_D_conv1_1_D_0_split needs backward computation.
I0625 14:48:03.359804 29461 net.cpp:217] conv1_1_D needs backward computation.
I0625 14:48:03.359808 29461 net.cpp:217] relu1_2_D needs backward computation.
I0625 14:48:03.359812 29461 net.cpp:217] scale1_2_D needs backward computation.
I0625 14:48:03.359817 29461 net.cpp:217] bn1_2_D needs backward computation.
I0625 14:48:03.359820 29461 net.cpp:217] conv1_2_D needs backward computation.
I0625 14:48:03.359824 29461 net.cpp:217] upsample1 needs backward computation.
I0625 14:48:03.359839 29461 net.cpp:217] relu2_1_D needs backward computation.
I0625 14:48:03.359844 29461 net.cpp:217] scale2_1_D needs backward computation.
I0625 14:48:03.359848 29461 net.cpp:217] bn2_1_D needs backward computation.
I0625 14:48:03.359853 29461 net.cpp:217] conv2_1_D needs backward computation.
I0625 14:48:03.359856 29461 net.cpp:217] relu2_2_D needs backward computation.
I0625 14:48:03.359859 29461 net.cpp:217] scale2_2_D needs backward computation.
I0625 14:48:03.359863 29461 net.cpp:217] bn2_2_D needs backward computation.
I0625 14:48:03.359868 29461 net.cpp:217] conv2_2_D needs backward computation.
I0625 14:48:03.359871 29461 net.cpp:217] upsample2 needs backward computation.
I0625 14:48:03.359875 29461 net.cpp:217] relu3_1_D needs backward computation.
I0625 14:48:03.359879 29461 net.cpp:217] scale3_1_D needs backward computation.
I0625 14:48:03.359882 29461 net.cpp:217] bn3_1_D needs backward computation.
I0625 14:48:03.359886 29461 net.cpp:217] conv3_1_D needs backward computation.
I0625 14:48:03.359890 29461 net.cpp:217] relu3_2_D needs backward computation.
I0625 14:48:03.359894 29461 net.cpp:217] scale3_2_D needs backward computation.
I0625 14:48:03.359899 29461 net.cpp:217] bn3_2_D needs backward computation.
I0625 14:48:03.359901 29461 net.cpp:217] conv3_2_D needs backward computation.
I0625 14:48:03.359906 29461 net.cpp:217] upsample3 needs backward computation.
I0625 14:48:03.359910 29461 net.cpp:217] relu4_1_D needs backward computation.
I0625 14:48:03.359915 29461 net.cpp:217] scale4_1_D needs backward computation.
I0625 14:48:03.359920 29461 net.cpp:217] bn4_1_D needs backward computation.
I0625 14:48:03.359923 29461 net.cpp:217] conv4_1_D needs backward computation.
I0625 14:48:03.359926 29461 net.cpp:217] relu4_2_D needs backward computation.
I0625 14:48:03.359930 29461 net.cpp:217] scale4_2_D needs backward computation.
I0625 14:48:03.359935 29461 net.cpp:217] bn4_2_D needs backward computation.
I0625 14:48:03.359938 29461 net.cpp:217] conv4_2_D needs backward computation.
I0625 14:48:03.359942 29461 net.cpp:217] upsample4 needs backward computation.
I0625 14:48:03.359947 29461 net.cpp:217] relu5_1_D needs backward computation.
I0625 14:48:03.359951 29461 net.cpp:217] scale5_1_D needs backward computation.
I0625 14:48:03.359956 29461 net.cpp:217] bn5_1_D needs backward computation.
I0625 14:48:03.359958 29461 net.cpp:217] conv5_1_D needs backward computation.
I0625 14:48:03.359963 29461 net.cpp:217] relu5_2_D needs backward computation.
I0625 14:48:03.359967 29461 net.cpp:217] scale5_2_D needs backward computation.
I0625 14:48:03.359972 29461 net.cpp:217] bn5_2_D needs backward computation.
I0625 14:48:03.359975 29461 net.cpp:217] conv5_2_D needs backward computation.
I0625 14:48:03.359979 29461 net.cpp:217] upsample5 needs backward computation.
I0625 14:48:03.359984 29461 net.cpp:217] pool5 needs backward computation.
I0625 14:48:03.359988 29461 net.cpp:217] relu5_2 needs backward computation.
I0625 14:48:03.359992 29461 net.cpp:217] scale5_2 needs backward computation.
I0625 14:48:03.359997 29461 net.cpp:217] bn5_2 needs backward computation.
I0625 14:48:03.360000 29461 net.cpp:217] conv5_2 needs backward computation.
I0625 14:48:03.360004 29461 net.cpp:217] relu5_1 needs backward computation.
I0625 14:48:03.360008 29461 net.cpp:217] scale5_1 needs backward computation.
I0625 14:48:03.360013 29461 net.cpp:217] bn5_1 needs backward computation.
I0625 14:48:03.360016 29461 net.cpp:217] conv5_1 needs backward computation.
I0625 14:48:03.360020 29461 net.cpp:217] pool4 needs backward computation.
I0625 14:48:03.360025 29461 net.cpp:217] relu4_2 needs backward computation.
I0625 14:48:03.360029 29461 net.cpp:217] scale4_2 needs backward computation.
I0625 14:48:03.360033 29461 net.cpp:217] bn4_2 needs backward computation.
I0625 14:48:03.360036 29461 net.cpp:217] conv4_2 needs backward computation.
I0625 14:48:03.360041 29461 net.cpp:217] relu4_1 needs backward computation.
I0625 14:48:03.360045 29461 net.cpp:217] scale4_1 needs backward computation.
I0625 14:48:03.360055 29461 net.cpp:217] bn4_1 needs backward computation.
I0625 14:48:03.360060 29461 net.cpp:217] conv4_1 needs backward computation.
I0625 14:48:03.360064 29461 net.cpp:217] pool3 needs backward computation.
I0625 14:48:03.360069 29461 net.cpp:217] relu3_2 needs backward computation.
I0625 14:48:03.360072 29461 net.cpp:217] scale3_2 needs backward computation.
I0625 14:48:03.360076 29461 net.cpp:217] bn3_2 needs backward computation.
I0625 14:48:03.360080 29461 net.cpp:217] conv3_2 needs backward computation.
I0625 14:48:03.360085 29461 net.cpp:217] relu3_1 needs backward computation.
I0625 14:48:03.360088 29461 net.cpp:217] scale3_1 needs backward computation.
I0625 14:48:03.360092 29461 net.cpp:217] bn3_1 needs backward computation.
I0625 14:48:03.360095 29461 net.cpp:217] conv3_1 needs backward computation.
I0625 14:48:03.360100 29461 net.cpp:217] pool2 needs backward computation.
I0625 14:48:03.360105 29461 net.cpp:217] relu2_2 needs backward computation.
I0625 14:48:03.360108 29461 net.cpp:217] scale2_2 needs backward computation.
I0625 14:48:03.360112 29461 net.cpp:217] bn2_2 needs backward computation.
I0625 14:48:03.360116 29461 net.cpp:217] conv2_2 needs backward computation.
I0625 14:48:03.360121 29461 net.cpp:217] relu2_1 needs backward computation.
I0625 14:48:03.360126 29461 net.cpp:217] scale2_1 needs backward computation.
I0625 14:48:03.360129 29461 net.cpp:217] bn2_1 needs backward computation.
I0625 14:48:03.360133 29461 net.cpp:217] conv2_1 needs backward computation.
I0625 14:48:03.360137 29461 net.cpp:217] pool1 needs backward computation.
I0625 14:48:03.360141 29461 net.cpp:217] relu1_2 needs backward computation.
I0625 14:48:03.360146 29461 net.cpp:217] scale1_2 needs backward computation.
I0625 14:48:03.360149 29461 net.cpp:217] bn1_2 needs backward computation.
I0625 14:48:03.360153 29461 net.cpp:217] conv1_2 needs backward computation.
I0625 14:48:03.360157 29461 net.cpp:217] relu1_1 needs backward computation.
I0625 14:48:03.360162 29461 net.cpp:217] scale1_1 needs backward computation.
I0625 14:48:03.360165 29461 net.cpp:217] bn1_1 needs backward computation.
I0625 14:48:03.360169 29461 net.cpp:217] conv1_1 needs backward computation.
I0625 14:48:03.360174 29461 net.cpp:219] label_data_1_split does not need backward computation.
I0625 14:48:03.360179 29461 net.cpp:219] data does not need backward computation.
I0625 14:48:03.360183 29461 net.cpp:261] This network produces output accuracy
I0625 14:48:03.360188 29461 net.cpp:261] This network produces output loss
I0625 14:48:03.360229 29461 net.cpp:274] Network initialization done.
I0625 14:48:03.361716 29461 solver.cpp:181] Creating test net (#0) specified by net file: models/segnet/train_val.prototxt
I0625 14:48:03.361840 29461 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0625 14:48:03.362241 29461 net.cpp:49] Initializing net from parameters: 
name: "segnet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "DenseImageData"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mean_value: 100
    crop_h: 224
    crop_w: 288
  }
  dense_image_data_param {
    source: "data/val_seg.txt"
    batch_size: 1
    shuffle: true
    new_height: 224
    new_width: 310
    is_color: false
    root_folder: "data/raw/train/"
  }
}
layer {
  name: "conv1_1"
  type: "Convolution"
  bottom: "data"
  top: "conv1_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_1"
  type: "BatchNorm"
  bottom: "conv1_1"
  top: "conv1_1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale1_1"
  type: "Scale"
  bottom: "conv1_1"
  top: "conv1_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1_1"
  type: "ReLU"
  bottom: "conv1_1"
  top: "conv1_1"
}
layer {
  name: "conv1_2"
  type: "Convolution"
  bottom: "conv1_1"
  top: "conv1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_2"
  type: "BatchNorm"
  bottom: "conv1_2"
  top: "conv1_2"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale1_2"
  type: "Scale"
  bottom: "conv1_2"
  top: "conv1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1_2"
  type: "ReLU"
  bottom: "conv1_2"
  top: "conv1_2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1_2"
  top: "pool1"
  top: "pool1_mask"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2"
  top: "pool2_mask"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_1"
  type: "BatchNorm"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale3_1"
  type: "Scale"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv3_2"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv3_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_2"
  type: "BatchNorm"
  bottom: "conv3_2"
  top: "conv3_2"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale3_2"
  type: "Scale"
  bottom: "conv3_2"
  top: "conv3_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_2"
  type: "ReLU"
  bottom: "conv3_2"
  top: "conv3_2"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3_2"
  top: "pool3"
  top: "pool3_mask"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4"
  top: "pool4_mask"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv5_1"
  type: "Convolution"
  bottom: "pool4"
  top: "conv5_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn5_1"
  type: "BatchNorm"
  bottom: "conv5_1"
  top: "conv5_1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale5_1"
  type: "Scale"
  bottom: "conv5_1"
  top: "conv5_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5_1"
  type: "ReLU"
  bottom: "conv5_1"
  top: "conv5_1"
}
layer {
  name: "conv5_2"
  type: "Convolution"
  bottom: "conv5_1"
  top: "conv5_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn5_2"
  type: "BatchNorm"
  bottom: "conv5_2"
  top: "conv5_2"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale5_2"
  type: "Scale"
  bottom: "conv5_2"
  top: "conv5_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5_2"
  type: "ReLU"
  bottom: "conv5_2"
  top: "conv5_2"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5_2"
  top: "pool5"
  top: "pool5_mask"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "upsample5"
  type: "Upsample"
  bottom: "pool5"
  bottom: "pool5_mask"
  top: "pool5_D"
  upsample_param {
    scale: 2
  }
}
layer {
  name: "conv5_2_D"
  type: "Convolution"
  bottom: "pool5_D"
  top: "conv5_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn5_2_D"
  type: "BatchNorm"
  bottom: "conv5_2_D"
  top: "conv5_2_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale5_2_D"
  type: "Scale"
  bottom: "conv5_2_D"
  top: "conv5_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5_2_D"
  type: "ReLU"
  bottom: "conv5_2_D"
  top: "conv5_2_D"
}
layer {
  name: "conv5_1_D"
  type: "Convolution"
  bottom: "conv5_2_D"
  top: "conv5_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn5_1_D"
  type: "BatchNorm"
  bottom: "conv5_1_D"
  top: "conv5_1_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale5_1_D"
  type: "Scale"
  bottom: "conv5_1_D"
  top: "conv5_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5_1_D"
  type: "ReLU"
  bottom: "conv5_1_D"
  top: "conv5_1_D"
}
layer {
  name: "upsample4"
  type: "Upsample"
  bottom: "conv5_1_D"
  bottom: "pool4_mask"
  top: "pool4_D"
  upsample_param {
    scale: 2
  }
}
layer {
  name: "conv4_2_D"
  type: "Convolution"
  bottom: "pool4_D"
  top: "conv4_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2_D"
  type: "BatchNorm"
  bottom: "conv4_2_D"
  top: "conv4_2_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale4_2_D"
  type: "Scale"
  bottom: "conv4_2_D"
  top: "conv4_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4_2_D"
  type: "ReLU"
  bottom: "conv4_2_D"
  top: "conv4_2_D"
}
layer {
  name: "conv4_1_D"
  type: "Convolution"
  bottom: "conv4_2_D"
  top: "conv4_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1_D"
  type: "BatchNorm"
  bottom: "conv4_1_D"
  top: "conv4_1_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale4_1_D"
  type: "Scale"
  bottom: "conv4_1_D"
  top: "conv4_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4_1_D"
  type: "ReLU"
  bottom: "conv4_1_D"
  top: "conv4_1_D"
}
layer {
  name: "upsample3"
  type: "Upsample"
  bottom: "conv4_1_D"
  bottom: "pool3_mask"
  top: "pool3_D"
  upsample_param {
    scale: 2
  }
}
layer {
  name: "conv3_2_D"
  type: "Convolution"
  bottom: "pool3_D"
  top: "conv3_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3_2_D"
  type: "BatchNorm"
  bottom: "conv3_2_D"
  top: "conv3_2_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale3_2_D"
  type: "Scale"
  bottom: "conv3_2_D"
  top: "conv3_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_2_D"
  type: "ReLU"
  bottom: "conv3_2_D"
  top: "conv3_2_D"
}
layer {
  name: "conv3_1_D"
  type: "Convolution"
  bottom: "conv3_2_D"
  top: "conv3_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_1_D"
  type: "BatchNorm"
  bottom: "conv3_1_D"
  top: "conv3_1_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale3_1_D"
  type: "Scale"
  bottom: "conv3_1_D"
  top: "conv3_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_1_D"
  type: "ReLU"
  bottom: "conv3_1_D"
  top: "conv3_1_D"
}
layer {
  name: "upsample2"
  type: "Upsample"
  bottom: "conv3_1_D"
  bottom: "pool2_mask"
  top: "pool2_D"
  upsample_param {
    scale: 2
  }
}
layer {
  name: "conv2_2_D"
  type: "Convolution"
  bottom: "pool2_D"
  top: "conv2_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2_D"
  type: "BatchNorm"
  bottom: "conv2_2_D"
  top: "conv2_2_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale2_2_D"
  type: "Scale"
  bottom: "conv2_2_D"
  top: "conv2_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2_2_D"
  type: "ReLU"
  bottom: "conv2_2_D"
  top: "conv2_2_D"
}
layer {
  name: "conv2_1_D"
  type: "Convolution"
  bottom: "conv2_2_D"
  top: "conv2_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_1_D"
  type: "BatchNorm"
  bottom: "conv2_1_D"
  top: "conv2_1_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale2_1_D"
  type: "Scale"
  bottom: "conv2_1_D"
  top: "conv2_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2_1_D"
  type: "ReLU"
  bottom: "conv2_1_D"
  top: "conv2_1_D"
}
layer {
  name: "upsample1"
  type: "Upsample"
  bottom: "conv2_1_D"
  bottom: "pool1_mask"
  top: "pool1_D"
  upsample_param {
    scale: 2
  }
}
layer {
  name: "conv1_2_D"
  type: "Convolution"
  bottom: "pool1_D"
  top: "conv1_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_2_D"
  type: "BatchNorm"
  bottom: "conv1_2_D"
  top: "conv1_2_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale1_2_D"
  type: "Scale"
  bottom: "conv1_2_D"
  top: "conv1_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1_2_D"
  type: "ReLU"
  bottom: "conv1_2_D"
  top: "conv1_2_D"
}
layer {
  name: "conv1_1_D"
  type: "Convolution"
  bottom: "conv1_2_D"
  top: "conv1_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 2
    bias_term: true
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "conv1_1_D"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "conv1_1_D"
  bottom: "label"
  top: "accuracy"
}
I0625 14:48:03.362591 29461 layer_factory.hpp:77] Creating layer data
I0625 14:48:03.362608 29461 net.cpp:91] Creating Layer data
I0625 14:48:03.362615 29461 net.cpp:399] data -> data
I0625 14:48:03.362625 29461 net.cpp:399] data -> label
I0625 14:48:03.362637 29461 dense_image_data_layer.cpp:38] Opening file data/val_seg.txt
I0625 14:48:03.362889 29461 dense_image_data_layer.cpp:48] Shuffling data
I0625 14:48:03.362941 29461 dense_image_data_layer.cpp:53] A total of 299 examples.
I0625 14:48:03.367590 29461 dense_image_data_layer.cpp:92] output data size: 1,1,224,288
I0625 14:48:03.368937 29461 net.cpp:141] Setting up data
I0625 14:48:03.368948 29461 net.cpp:148] Top shape: 1 1 224 288 (64512)
I0625 14:48:03.368952 29461 net.cpp:148] Top shape: 1 1 224 288 (64512)
I0625 14:48:03.368954 29461 net.cpp:156] Memory required for data: 516096
I0625 14:48:03.368957 29461 layer_factory.hpp:77] Creating layer label_data_1_split
I0625 14:48:03.368964 29461 net.cpp:91] Creating Layer label_data_1_split
I0625 14:48:03.368966 29461 net.cpp:425] label_data_1_split <- label
I0625 14:48:03.368970 29461 net.cpp:399] label_data_1_split -> label_data_1_split_0
I0625 14:48:03.368976 29461 net.cpp:399] label_data_1_split -> label_data_1_split_1
I0625 14:48:03.369089 29461 net.cpp:141] Setting up label_data_1_split
I0625 14:48:03.369097 29461 net.cpp:148] Top shape: 1 1 224 288 (64512)
I0625 14:48:03.369101 29461 net.cpp:148] Top shape: 1 1 224 288 (64512)
I0625 14:48:03.369102 29461 net.cpp:156] Memory required for data: 1032192
I0625 14:48:03.369104 29461 layer_factory.hpp:77] Creating layer conv1_1
I0625 14:48:03.369112 29461 net.cpp:91] Creating Layer conv1_1
I0625 14:48:03.369115 29461 net.cpp:425] conv1_1 <- data
I0625 14:48:03.369118 29461 net.cpp:399] conv1_1 -> conv1_1
I0625 14:48:03.370383 29461 net.cpp:141] Setting up conv1_1
I0625 14:48:03.370396 29461 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 14:48:03.370398 29461 net.cpp:156] Memory required for data: 9289728
I0625 14:48:03.370404 29461 layer_factory.hpp:77] Creating layer bn1_1
I0625 14:48:03.370411 29461 net.cpp:91] Creating Layer bn1_1
I0625 14:48:03.370414 29461 net.cpp:425] bn1_1 <- conv1_1
I0625 14:48:03.370417 29461 net.cpp:386] bn1_1 -> conv1_1 (in-place)
I0625 14:48:03.371271 29461 net.cpp:141] Setting up bn1_1
I0625 14:48:03.371281 29461 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 14:48:03.371284 29461 net.cpp:156] Memory required for data: 17547264
I0625 14:48:03.371305 29461 layer_factory.hpp:77] Creating layer scale1_1
I0625 14:48:03.371315 29461 net.cpp:91] Creating Layer scale1_1
I0625 14:48:03.371321 29461 net.cpp:425] scale1_1 <- conv1_1
I0625 14:48:03.371326 29461 net.cpp:386] scale1_1 -> conv1_1 (in-place)
I0625 14:48:03.371369 29461 layer_factory.hpp:77] Creating layer scale1_1
I0625 14:48:03.371542 29461 net.cpp:141] Setting up scale1_1
I0625 14:48:03.371551 29461 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 14:48:03.371552 29461 net.cpp:156] Memory required for data: 25804800
I0625 14:48:03.371558 29461 layer_factory.hpp:77] Creating layer relu1_1
I0625 14:48:03.371563 29461 net.cpp:91] Creating Layer relu1_1
I0625 14:48:03.371567 29461 net.cpp:425] relu1_1 <- conv1_1
I0625 14:48:03.371569 29461 net.cpp:386] relu1_1 -> conv1_1 (in-place)
I0625 14:48:03.371855 29461 net.cpp:141] Setting up relu1_1
I0625 14:48:03.371867 29461 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 14:48:03.371870 29461 net.cpp:156] Memory required for data: 34062336
I0625 14:48:03.371872 29461 layer_factory.hpp:77] Creating layer conv1_2
I0625 14:48:03.371879 29461 net.cpp:91] Creating Layer conv1_2
I0625 14:48:03.371882 29461 net.cpp:425] conv1_2 <- conv1_1
I0625 14:48:03.371886 29461 net.cpp:399] conv1_2 -> conv1_2
I0625 14:48:03.372794 29461 net.cpp:141] Setting up conv1_2
I0625 14:48:03.372805 29461 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 14:48:03.372808 29461 net.cpp:156] Memory required for data: 42319872
I0625 14:48:03.372812 29461 layer_factory.hpp:77] Creating layer bn1_2
I0625 14:48:03.372818 29461 net.cpp:91] Creating Layer bn1_2
I0625 14:48:03.372820 29461 net.cpp:425] bn1_2 <- conv1_2
I0625 14:48:03.372824 29461 net.cpp:386] bn1_2 -> conv1_2 (in-place)
I0625 14:48:03.373044 29461 net.cpp:141] Setting up bn1_2
I0625 14:48:03.373051 29461 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 14:48:03.373054 29461 net.cpp:156] Memory required for data: 50577408
I0625 14:48:03.373061 29461 layer_factory.hpp:77] Creating layer scale1_2
I0625 14:48:03.373069 29461 net.cpp:91] Creating Layer scale1_2
I0625 14:48:03.373070 29461 net.cpp:425] scale1_2 <- conv1_2
I0625 14:48:03.373075 29461 net.cpp:386] scale1_2 -> conv1_2 (in-place)
I0625 14:48:03.373111 29461 layer_factory.hpp:77] Creating layer scale1_2
I0625 14:48:03.373881 29461 net.cpp:141] Setting up scale1_2
I0625 14:48:03.373893 29461 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 14:48:03.373894 29461 net.cpp:156] Memory required for data: 58834944
I0625 14:48:03.373899 29461 layer_factory.hpp:77] Creating layer relu1_2
I0625 14:48:03.373904 29461 net.cpp:91] Creating Layer relu1_2
I0625 14:48:03.373908 29461 net.cpp:425] relu1_2 <- conv1_2
I0625 14:48:03.373910 29461 net.cpp:386] relu1_2 -> conv1_2 (in-place)
I0625 14:48:03.374191 29461 net.cpp:141] Setting up relu1_2
I0625 14:48:03.374203 29461 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 14:48:03.374208 29461 net.cpp:156] Memory required for data: 67092480
I0625 14:48:03.374212 29461 layer_factory.hpp:77] Creating layer pool1
I0625 14:48:03.374217 29461 layer_factory.cpp:90] cuDNN does not support multiple tops. Using Caffe's own pooling layer.
I0625 14:48:03.374224 29461 net.cpp:91] Creating Layer pool1
I0625 14:48:03.374228 29461 net.cpp:425] pool1 <- conv1_2
I0625 14:48:03.374235 29461 net.cpp:399] pool1 -> pool1
I0625 14:48:03.374243 29461 net.cpp:399] pool1 -> pool1_mask
I0625 14:48:03.374303 29461 net.cpp:141] Setting up pool1
I0625 14:48:03.374312 29461 net.cpp:148] Top shape: 1 32 112 144 (516096)
I0625 14:48:03.374318 29461 net.cpp:148] Top shape: 1 32 112 144 (516096)
I0625 14:48:03.374321 29461 net.cpp:156] Memory required for data: 71221248
I0625 14:48:03.374325 29461 layer_factory.hpp:77] Creating layer conv2_1
I0625 14:48:03.374336 29461 net.cpp:91] Creating Layer conv2_1
I0625 14:48:03.374341 29461 net.cpp:425] conv2_1 <- pool1
I0625 14:48:03.374351 29461 net.cpp:399] conv2_1 -> conv2_1
I0625 14:48:03.375470 29461 net.cpp:141] Setting up conv2_1
I0625 14:48:03.375485 29461 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 14:48:03.375502 29461 net.cpp:156] Memory required for data: 75350016
I0625 14:48:03.375511 29461 layer_factory.hpp:77] Creating layer bn2_1
I0625 14:48:03.375520 29461 net.cpp:91] Creating Layer bn2_1
I0625 14:48:03.375525 29461 net.cpp:425] bn2_1 <- conv2_1
I0625 14:48:03.375532 29461 net.cpp:386] bn2_1 -> conv2_1 (in-place)
I0625 14:48:03.375741 29461 net.cpp:141] Setting up bn2_1
I0625 14:48:03.375751 29461 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 14:48:03.375756 29461 net.cpp:156] Memory required for data: 79478784
I0625 14:48:03.375766 29461 layer_factory.hpp:77] Creating layer scale2_1
I0625 14:48:03.375774 29461 net.cpp:91] Creating Layer scale2_1
I0625 14:48:03.375778 29461 net.cpp:425] scale2_1 <- conv2_1
I0625 14:48:03.375785 29461 net.cpp:386] scale2_1 -> conv2_1 (in-place)
I0625 14:48:03.375841 29461 layer_factory.hpp:77] Creating layer scale2_1
I0625 14:48:03.376618 29461 net.cpp:141] Setting up scale2_1
I0625 14:48:03.376631 29461 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 14:48:03.376636 29461 net.cpp:156] Memory required for data: 83607552
I0625 14:48:03.376647 29461 layer_factory.hpp:77] Creating layer relu2_1
I0625 14:48:03.376657 29461 net.cpp:91] Creating Layer relu2_1
I0625 14:48:03.376662 29461 net.cpp:425] relu2_1 <- conv2_1
I0625 14:48:03.376668 29461 net.cpp:386] relu2_1 -> conv2_1 (in-place)
I0625 14:48:03.376832 29461 net.cpp:141] Setting up relu2_1
I0625 14:48:03.376844 29461 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 14:48:03.376848 29461 net.cpp:156] Memory required for data: 87736320
I0625 14:48:03.376853 29461 layer_factory.hpp:77] Creating layer conv2_2
I0625 14:48:03.376863 29461 net.cpp:91] Creating Layer conv2_2
I0625 14:48:03.376868 29461 net.cpp:425] conv2_2 <- conv2_1
I0625 14:48:03.376876 29461 net.cpp:399] conv2_2 -> conv2_2
I0625 14:48:03.378216 29461 net.cpp:141] Setting up conv2_2
I0625 14:48:03.378228 29461 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 14:48:03.378233 29461 net.cpp:156] Memory required for data: 91865088
I0625 14:48:03.378240 29461 layer_factory.hpp:77] Creating layer bn2_2
I0625 14:48:03.378252 29461 net.cpp:91] Creating Layer bn2_2
I0625 14:48:03.378257 29461 net.cpp:425] bn2_2 <- conv2_2
I0625 14:48:03.378263 29461 net.cpp:386] bn2_2 -> conv2_2 (in-place)
I0625 14:48:03.378476 29461 net.cpp:141] Setting up bn2_2
I0625 14:48:03.378486 29461 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 14:48:03.378490 29461 net.cpp:156] Memory required for data: 95993856
I0625 14:48:03.378499 29461 layer_factory.hpp:77] Creating layer scale2_2
I0625 14:48:03.378509 29461 net.cpp:91] Creating Layer scale2_2
I0625 14:48:03.378515 29461 net.cpp:425] scale2_2 <- conv2_2
I0625 14:48:03.378521 29461 net.cpp:386] scale2_2 -> conv2_2 (in-place)
I0625 14:48:03.378573 29461 layer_factory.hpp:77] Creating layer scale2_2
I0625 14:48:03.378710 29461 net.cpp:141] Setting up scale2_2
I0625 14:48:03.378720 29461 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 14:48:03.378722 29461 net.cpp:156] Memory required for data: 100122624
I0625 14:48:03.378731 29461 layer_factory.hpp:77] Creating layer relu2_2
I0625 14:48:03.378737 29461 net.cpp:91] Creating Layer relu2_2
I0625 14:48:03.378741 29461 net.cpp:425] relu2_2 <- conv2_2
I0625 14:48:03.378747 29461 net.cpp:386] relu2_2 -> conv2_2 (in-place)
I0625 14:48:03.379050 29461 net.cpp:141] Setting up relu2_2
I0625 14:48:03.379063 29461 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 14:48:03.379067 29461 net.cpp:156] Memory required for data: 104251392
I0625 14:48:03.379071 29461 layer_factory.hpp:77] Creating layer pool2
I0625 14:48:03.379076 29461 layer_factory.cpp:90] cuDNN does not support multiple tops. Using Caffe's own pooling layer.
I0625 14:48:03.379083 29461 net.cpp:91] Creating Layer pool2
I0625 14:48:03.379087 29461 net.cpp:425] pool2 <- conv2_2
I0625 14:48:03.379094 29461 net.cpp:399] pool2 -> pool2
I0625 14:48:03.379102 29461 net.cpp:399] pool2 -> pool2_mask
I0625 14:48:03.379163 29461 net.cpp:141] Setting up pool2
I0625 14:48:03.379171 29461 net.cpp:148] Top shape: 1 64 56 72 (258048)
I0625 14:48:03.379191 29461 net.cpp:148] Top shape: 1 64 56 72 (258048)
I0625 14:48:03.379195 29461 net.cpp:156] Memory required for data: 106315776
I0625 14:48:03.379200 29461 layer_factory.hpp:77] Creating layer conv3_1
I0625 14:48:03.379210 29461 net.cpp:91] Creating Layer conv3_1
I0625 14:48:03.379215 29461 net.cpp:425] conv3_1 <- pool2
I0625 14:48:03.379223 29461 net.cpp:399] conv3_1 -> conv3_1
I0625 14:48:03.380518 29461 net.cpp:141] Setting up conv3_1
I0625 14:48:03.380532 29461 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 14:48:03.380537 29461 net.cpp:156] Memory required for data: 108380160
I0625 14:48:03.380543 29461 layer_factory.hpp:77] Creating layer bn3_1
I0625 14:48:03.380553 29461 net.cpp:91] Creating Layer bn3_1
I0625 14:48:03.380556 29461 net.cpp:425] bn3_1 <- conv3_1
I0625 14:48:03.380563 29461 net.cpp:386] bn3_1 -> conv3_1 (in-place)
I0625 14:48:03.380764 29461 net.cpp:141] Setting up bn3_1
I0625 14:48:03.380775 29461 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 14:48:03.380779 29461 net.cpp:156] Memory required for data: 110444544
I0625 14:48:03.380789 29461 layer_factory.hpp:77] Creating layer scale3_1
I0625 14:48:03.380797 29461 net.cpp:91] Creating Layer scale3_1
I0625 14:48:03.380802 29461 net.cpp:425] scale3_1 <- conv3_1
I0625 14:48:03.380810 29461 net.cpp:386] scale3_1 -> conv3_1 (in-place)
I0625 14:48:03.380858 29461 layer_factory.hpp:77] Creating layer scale3_1
I0625 14:48:03.380983 29461 net.cpp:141] Setting up scale3_1
I0625 14:48:03.380993 29461 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 14:48:03.380997 29461 net.cpp:156] Memory required for data: 112508928
I0625 14:48:03.381005 29461 layer_factory.hpp:77] Creating layer relu3_1
I0625 14:48:03.381012 29461 net.cpp:91] Creating Layer relu3_1
I0625 14:48:03.381016 29461 net.cpp:425] relu3_1 <- conv3_1
I0625 14:48:03.381022 29461 net.cpp:386] relu3_1 -> conv3_1 (in-place)
I0625 14:48:03.381319 29461 net.cpp:141] Setting up relu3_1
I0625 14:48:03.381331 29461 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 14:48:03.381335 29461 net.cpp:156] Memory required for data: 114573312
I0625 14:48:03.381340 29461 layer_factory.hpp:77] Creating layer conv3_2
I0625 14:48:03.381351 29461 net.cpp:91] Creating Layer conv3_2
I0625 14:48:03.381356 29461 net.cpp:425] conv3_2 <- conv3_1
I0625 14:48:03.381364 29461 net.cpp:399] conv3_2 -> conv3_2
I0625 14:48:03.383983 29461 net.cpp:141] Setting up conv3_2
I0625 14:48:03.383998 29461 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 14:48:03.384002 29461 net.cpp:156] Memory required for data: 116637696
I0625 14:48:03.384011 29461 layer_factory.hpp:77] Creating layer bn3_2
I0625 14:48:03.384019 29461 net.cpp:91] Creating Layer bn3_2
I0625 14:48:03.384026 29461 net.cpp:425] bn3_2 <- conv3_2
I0625 14:48:03.384032 29461 net.cpp:386] bn3_2 -> conv3_2 (in-place)
I0625 14:48:03.384243 29461 net.cpp:141] Setting up bn3_2
I0625 14:48:03.384253 29461 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 14:48:03.384255 29461 net.cpp:156] Memory required for data: 118702080
I0625 14:48:03.384271 29461 layer_factory.hpp:77] Creating layer scale3_2
I0625 14:48:03.384281 29461 net.cpp:91] Creating Layer scale3_2
I0625 14:48:03.384290 29461 net.cpp:425] scale3_2 <- conv3_2
I0625 14:48:03.384297 29461 net.cpp:386] scale3_2 -> conv3_2 (in-place)
I0625 14:48:03.384349 29461 layer_factory.hpp:77] Creating layer scale3_2
I0625 14:48:03.384475 29461 net.cpp:141] Setting up scale3_2
I0625 14:48:03.384485 29461 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 14:48:03.384490 29461 net.cpp:156] Memory required for data: 120766464
I0625 14:48:03.384496 29461 layer_factory.hpp:77] Creating layer relu3_2
I0625 14:48:03.384503 29461 net.cpp:91] Creating Layer relu3_2
I0625 14:48:03.384507 29461 net.cpp:425] relu3_2 <- conv3_2
I0625 14:48:03.384515 29461 net.cpp:386] relu3_2 -> conv3_2 (in-place)
I0625 14:48:03.384677 29461 net.cpp:141] Setting up relu3_2
I0625 14:48:03.384687 29461 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 14:48:03.384691 29461 net.cpp:156] Memory required for data: 122830848
I0625 14:48:03.384707 29461 layer_factory.hpp:77] Creating layer pool3
I0625 14:48:03.384713 29461 layer_factory.cpp:90] cuDNN does not support multiple tops. Using Caffe's own pooling layer.
I0625 14:48:03.384721 29461 net.cpp:91] Creating Layer pool3
I0625 14:48:03.384724 29461 net.cpp:425] pool3 <- conv3_2
I0625 14:48:03.384732 29461 net.cpp:399] pool3 -> pool3
I0625 14:48:03.384739 29461 net.cpp:399] pool3 -> pool3_mask
I0625 14:48:03.384794 29461 net.cpp:141] Setting up pool3
I0625 14:48:03.384804 29461 net.cpp:148] Top shape: 1 128 28 36 (129024)
I0625 14:48:03.384809 29461 net.cpp:148] Top shape: 1 128 28 36 (129024)
I0625 14:48:03.384811 29461 net.cpp:156] Memory required for data: 123863040
I0625 14:48:03.384815 29461 layer_factory.hpp:77] Creating layer conv4_1
I0625 14:48:03.384826 29461 net.cpp:91] Creating Layer conv4_1
I0625 14:48:03.384831 29461 net.cpp:425] conv4_1 <- pool3
I0625 14:48:03.384840 29461 net.cpp:399] conv4_1 -> conv4_1
I0625 14:48:03.388146 29461 net.cpp:141] Setting up conv4_1
I0625 14:48:03.388162 29461 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 14:48:03.388167 29461 net.cpp:156] Memory required for data: 124895232
I0625 14:48:03.388175 29461 layer_factory.hpp:77] Creating layer bn4_1
I0625 14:48:03.388185 29461 net.cpp:91] Creating Layer bn4_1
I0625 14:48:03.388190 29461 net.cpp:425] bn4_1 <- conv4_1
I0625 14:48:03.388198 29461 net.cpp:386] bn4_1 -> conv4_1 (in-place)
I0625 14:48:03.388413 29461 net.cpp:141] Setting up bn4_1
I0625 14:48:03.388432 29461 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 14:48:03.388435 29461 net.cpp:156] Memory required for data: 125927424
I0625 14:48:03.388444 29461 layer_factory.hpp:77] Creating layer scale4_1
I0625 14:48:03.388455 29461 net.cpp:91] Creating Layer scale4_1
I0625 14:48:03.388460 29461 net.cpp:425] scale4_1 <- conv4_1
I0625 14:48:03.388468 29461 net.cpp:386] scale4_1 -> conv4_1 (in-place)
I0625 14:48:03.388522 29461 layer_factory.hpp:77] Creating layer scale4_1
I0625 14:48:03.388662 29461 net.cpp:141] Setting up scale4_1
I0625 14:48:03.388671 29461 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 14:48:03.388675 29461 net.cpp:156] Memory required for data: 126959616
I0625 14:48:03.388684 29461 layer_factory.hpp:77] Creating layer relu4_1
I0625 14:48:03.388695 29461 net.cpp:91] Creating Layer relu4_1
I0625 14:48:03.388701 29461 net.cpp:425] relu4_1 <- conv4_1
I0625 14:48:03.388707 29461 net.cpp:386] relu4_1 -> conv4_1 (in-place)
I0625 14:48:03.389010 29461 net.cpp:141] Setting up relu4_1
I0625 14:48:03.389024 29461 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 14:48:03.389027 29461 net.cpp:156] Memory required for data: 127991808
I0625 14:48:03.389032 29461 layer_factory.hpp:77] Creating layer conv4_2
I0625 14:48:03.389046 29461 net.cpp:91] Creating Layer conv4_2
I0625 14:48:03.389051 29461 net.cpp:425] conv4_2 <- conv4_1
I0625 14:48:03.389060 29461 net.cpp:399] conv4_2 -> conv4_2
I0625 14:48:03.394435 29461 net.cpp:141] Setting up conv4_2
I0625 14:48:03.394450 29461 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 14:48:03.394455 29461 net.cpp:156] Memory required for data: 129024000
I0625 14:48:03.394462 29461 layer_factory.hpp:77] Creating layer bn4_2
I0625 14:48:03.394474 29461 net.cpp:91] Creating Layer bn4_2
I0625 14:48:03.394479 29461 net.cpp:425] bn4_2 <- conv4_2
I0625 14:48:03.394487 29461 net.cpp:386] bn4_2 -> conv4_2 (in-place)
I0625 14:48:03.394706 29461 net.cpp:141] Setting up bn4_2
I0625 14:48:03.394716 29461 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 14:48:03.394719 29461 net.cpp:156] Memory required for data: 130056192
I0625 14:48:03.394729 29461 layer_factory.hpp:77] Creating layer scale4_2
I0625 14:48:03.394738 29461 net.cpp:91] Creating Layer scale4_2
I0625 14:48:03.394743 29461 net.cpp:425] scale4_2 <- conv4_2
I0625 14:48:03.394752 29461 net.cpp:386] scale4_2 -> conv4_2 (in-place)
I0625 14:48:03.394806 29461 layer_factory.hpp:77] Creating layer scale4_2
I0625 14:48:03.394969 29461 net.cpp:141] Setting up scale4_2
I0625 14:48:03.394985 29461 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 14:48:03.395005 29461 net.cpp:156] Memory required for data: 131088384
I0625 14:48:03.395016 29461 layer_factory.hpp:77] Creating layer relu4_2
I0625 14:48:03.395025 29461 net.cpp:91] Creating Layer relu4_2
I0625 14:48:03.395030 29461 net.cpp:425] relu4_2 <- conv4_2
I0625 14:48:03.395037 29461 net.cpp:386] relu4_2 -> conv4_2 (in-place)
I0625 14:48:03.395387 29461 net.cpp:141] Setting up relu4_2
I0625 14:48:03.395411 29461 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 14:48:03.395416 29461 net.cpp:156] Memory required for data: 132120576
I0625 14:48:03.395421 29461 layer_factory.hpp:77] Creating layer pool4
I0625 14:48:03.395427 29461 layer_factory.cpp:90] cuDNN does not support multiple tops. Using Caffe's own pooling layer.
I0625 14:48:03.395434 29461 net.cpp:91] Creating Layer pool4
I0625 14:48:03.395439 29461 net.cpp:425] pool4 <- conv4_2
I0625 14:48:03.395447 29461 net.cpp:399] pool4 -> pool4
I0625 14:48:03.395457 29461 net.cpp:399] pool4 -> pool4_mask
I0625 14:48:03.395519 29461 net.cpp:141] Setting up pool4
I0625 14:48:03.395529 29461 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 14:48:03.395534 29461 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 14:48:03.395537 29461 net.cpp:156] Memory required for data: 132636672
I0625 14:48:03.395541 29461 layer_factory.hpp:77] Creating layer conv5_1
I0625 14:48:03.395556 29461 net.cpp:91] Creating Layer conv5_1
I0625 14:48:03.395561 29461 net.cpp:425] conv5_1 <- pool4
I0625 14:48:03.395570 29461 net.cpp:399] conv5_1 -> conv5_1
I0625 14:48:03.401082 29461 net.cpp:141] Setting up conv5_1
I0625 14:48:03.401098 29461 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 14:48:03.401103 29461 net.cpp:156] Memory required for data: 132894720
I0625 14:48:03.401109 29461 layer_factory.hpp:77] Creating layer bn5_1
I0625 14:48:03.401120 29461 net.cpp:91] Creating Layer bn5_1
I0625 14:48:03.401126 29461 net.cpp:425] bn5_1 <- conv5_1
I0625 14:48:03.401134 29461 net.cpp:386] bn5_1 -> conv5_1 (in-place)
I0625 14:48:03.401345 29461 net.cpp:141] Setting up bn5_1
I0625 14:48:03.401355 29461 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 14:48:03.401360 29461 net.cpp:156] Memory required for data: 133152768
I0625 14:48:03.401370 29461 layer_factory.hpp:77] Creating layer scale5_1
I0625 14:48:03.401381 29461 net.cpp:91] Creating Layer scale5_1
I0625 14:48:03.401386 29461 net.cpp:425] scale5_1 <- conv5_1
I0625 14:48:03.401392 29461 net.cpp:386] scale5_1 -> conv5_1 (in-place)
I0625 14:48:03.401448 29461 layer_factory.hpp:77] Creating layer scale5_1
I0625 14:48:03.401582 29461 net.cpp:141] Setting up scale5_1
I0625 14:48:03.401592 29461 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 14:48:03.401595 29461 net.cpp:156] Memory required for data: 133410816
I0625 14:48:03.401602 29461 layer_factory.hpp:77] Creating layer relu5_1
I0625 14:48:03.401612 29461 net.cpp:91] Creating Layer relu5_1
I0625 14:48:03.401618 29461 net.cpp:425] relu5_1 <- conv5_1
I0625 14:48:03.401623 29461 net.cpp:386] relu5_1 -> conv5_1 (in-place)
I0625 14:48:03.401800 29461 net.cpp:141] Setting up relu5_1
I0625 14:48:03.401810 29461 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 14:48:03.401814 29461 net.cpp:156] Memory required for data: 133668864
I0625 14:48:03.401821 29461 layer_factory.hpp:77] Creating layer conv5_2
I0625 14:48:03.401834 29461 net.cpp:91] Creating Layer conv5_2
I0625 14:48:03.401840 29461 net.cpp:425] conv5_2 <- conv5_1
I0625 14:48:03.401851 29461 net.cpp:399] conv5_2 -> conv5_2
I0625 14:48:03.407500 29461 net.cpp:141] Setting up conv5_2
I0625 14:48:03.407516 29461 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 14:48:03.407521 29461 net.cpp:156] Memory required for data: 133926912
I0625 14:48:03.407528 29461 layer_factory.hpp:77] Creating layer bn5_2
I0625 14:48:03.407539 29461 net.cpp:91] Creating Layer bn5_2
I0625 14:48:03.407544 29461 net.cpp:425] bn5_2 <- conv5_2
I0625 14:48:03.407552 29461 net.cpp:386] bn5_2 -> conv5_2 (in-place)
I0625 14:48:03.407768 29461 net.cpp:141] Setting up bn5_2
I0625 14:48:03.407778 29461 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 14:48:03.407783 29461 net.cpp:156] Memory required for data: 134184960
I0625 14:48:03.407806 29461 layer_factory.hpp:77] Creating layer scale5_2
I0625 14:48:03.407819 29461 net.cpp:91] Creating Layer scale5_2
I0625 14:48:03.407826 29461 net.cpp:425] scale5_2 <- conv5_2
I0625 14:48:03.407833 29461 net.cpp:386] scale5_2 -> conv5_2 (in-place)
I0625 14:48:03.407891 29461 layer_factory.hpp:77] Creating layer scale5_2
I0625 14:48:03.408025 29461 net.cpp:141] Setting up scale5_2
I0625 14:48:03.408033 29461 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 14:48:03.408037 29461 net.cpp:156] Memory required for data: 134443008
I0625 14:48:03.408046 29461 layer_factory.hpp:77] Creating layer relu5_2
I0625 14:48:03.408059 29461 net.cpp:91] Creating Layer relu5_2
I0625 14:48:03.408066 29461 net.cpp:425] relu5_2 <- conv5_2
I0625 14:48:03.408072 29461 net.cpp:386] relu5_2 -> conv5_2 (in-place)
I0625 14:48:03.408448 29461 net.cpp:141] Setting up relu5_2
I0625 14:48:03.408462 29461 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 14:48:03.408465 29461 net.cpp:156] Memory required for data: 134701056
I0625 14:48:03.408470 29461 layer_factory.hpp:77] Creating layer pool5
I0625 14:48:03.408475 29461 layer_factory.cpp:90] cuDNN does not support multiple tops. Using Caffe's own pooling layer.
I0625 14:48:03.408484 29461 net.cpp:91] Creating Layer pool5
I0625 14:48:03.408489 29461 net.cpp:425] pool5 <- conv5_2
I0625 14:48:03.408496 29461 net.cpp:399] pool5 -> pool5
I0625 14:48:03.408505 29461 net.cpp:399] pool5 -> pool5_mask
I0625 14:48:03.408568 29461 net.cpp:141] Setting up pool5
I0625 14:48:03.408577 29461 net.cpp:148] Top shape: 1 256 7 9 (16128)
I0625 14:48:03.408583 29461 net.cpp:148] Top shape: 1 256 7 9 (16128)
I0625 14:48:03.408586 29461 net.cpp:156] Memory required for data: 134830080
I0625 14:48:03.408591 29461 layer_factory.hpp:77] Creating layer upsample5
I0625 14:48:03.408598 29461 net.cpp:91] Creating Layer upsample5
I0625 14:48:03.408603 29461 net.cpp:425] upsample5 <- pool5
I0625 14:48:03.408608 29461 net.cpp:425] upsample5 <- pool5_mask
I0625 14:48:03.408617 29461 net.cpp:399] upsample5 -> pool5_D
I0625 14:48:03.408627 29461 upsample_layer.cpp:31] Params 'pad_out_{}_' are deprecated. Please declare upsample height and width useing the upsample_h, upsample_w parameters.
I0625 14:48:03.408665 29461 net.cpp:141] Setting up upsample5
I0625 14:48:03.408676 29461 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 14:48:03.408680 29461 net.cpp:156] Memory required for data: 135088128
I0625 14:48:03.408684 29461 layer_factory.hpp:77] Creating layer conv5_2_D
I0625 14:48:03.408695 29461 net.cpp:91] Creating Layer conv5_2_D
I0625 14:48:03.408700 29461 net.cpp:425] conv5_2_D <- pool5_D
I0625 14:48:03.408710 29461 net.cpp:399] conv5_2_D -> conv5_2_D
I0625 14:48:03.414214 29461 net.cpp:141] Setting up conv5_2_D
I0625 14:48:03.414232 29461 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 14:48:03.414235 29461 net.cpp:156] Memory required for data: 135346176
I0625 14:48:03.414242 29461 layer_factory.hpp:77] Creating layer bn5_2_D
I0625 14:48:03.414254 29461 net.cpp:91] Creating Layer bn5_2_D
I0625 14:48:03.414260 29461 net.cpp:425] bn5_2_D <- conv5_2_D
I0625 14:48:03.414268 29461 net.cpp:386] bn5_2_D -> conv5_2_D (in-place)
I0625 14:48:03.414516 29461 net.cpp:141] Setting up bn5_2_D
I0625 14:48:03.414527 29461 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 14:48:03.414530 29461 net.cpp:156] Memory required for data: 135604224
I0625 14:48:03.414541 29461 layer_factory.hpp:77] Creating layer scale5_2_D
I0625 14:48:03.414551 29461 net.cpp:91] Creating Layer scale5_2_D
I0625 14:48:03.414556 29461 net.cpp:425] scale5_2_D <- conv5_2_D
I0625 14:48:03.414562 29461 net.cpp:386] scale5_2_D -> conv5_2_D (in-place)
I0625 14:48:03.414623 29461 layer_factory.hpp:77] Creating layer scale5_2_D
I0625 14:48:03.414757 29461 net.cpp:141] Setting up scale5_2_D
I0625 14:48:03.414765 29461 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 14:48:03.414769 29461 net.cpp:156] Memory required for data: 135862272
I0625 14:48:03.414793 29461 layer_factory.hpp:77] Creating layer relu5_2_D
I0625 14:48:03.414813 29461 net.cpp:91] Creating Layer relu5_2_D
I0625 14:48:03.414819 29461 net.cpp:425] relu5_2_D <- conv5_2_D
I0625 14:48:03.414825 29461 net.cpp:386] relu5_2_D -> conv5_2_D (in-place)
I0625 14:48:03.415135 29461 net.cpp:141] Setting up relu5_2_D
I0625 14:48:03.415153 29461 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 14:48:03.415158 29461 net.cpp:156] Memory required for data: 136120320
I0625 14:48:03.415163 29461 layer_factory.hpp:77] Creating layer conv5_1_D
I0625 14:48:03.415177 29461 net.cpp:91] Creating Layer conv5_1_D
I0625 14:48:03.415184 29461 net.cpp:425] conv5_1_D <- conv5_2_D
I0625 14:48:03.415191 29461 net.cpp:399] conv5_1_D -> conv5_1_D
I0625 14:48:03.420940 29461 net.cpp:141] Setting up conv5_1_D
I0625 14:48:03.420955 29461 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 14:48:03.420959 29461 net.cpp:156] Memory required for data: 136378368
I0625 14:48:03.420966 29461 layer_factory.hpp:77] Creating layer bn5_1_D
I0625 14:48:03.420976 29461 net.cpp:91] Creating Layer bn5_1_D
I0625 14:48:03.420982 29461 net.cpp:425] bn5_1_D <- conv5_1_D
I0625 14:48:03.421001 29461 net.cpp:386] bn5_1_D -> conv5_1_D (in-place)
I0625 14:48:03.421224 29461 net.cpp:141] Setting up bn5_1_D
I0625 14:48:03.421234 29461 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 14:48:03.421238 29461 net.cpp:156] Memory required for data: 136636416
I0625 14:48:03.421248 29461 layer_factory.hpp:77] Creating layer scale5_1_D
I0625 14:48:03.421257 29461 net.cpp:91] Creating Layer scale5_1_D
I0625 14:48:03.421262 29461 net.cpp:425] scale5_1_D <- conv5_1_D
I0625 14:48:03.421272 29461 net.cpp:386] scale5_1_D -> conv5_1_D (in-place)
I0625 14:48:03.421326 29461 layer_factory.hpp:77] Creating layer scale5_1_D
I0625 14:48:03.421461 29461 net.cpp:141] Setting up scale5_1_D
I0625 14:48:03.421473 29461 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 14:48:03.421478 29461 net.cpp:156] Memory required for data: 136894464
I0625 14:48:03.421484 29461 layer_factory.hpp:77] Creating layer relu5_1_D
I0625 14:48:03.421491 29461 net.cpp:91] Creating Layer relu5_1_D
I0625 14:48:03.421496 29461 net.cpp:425] relu5_1_D <- conv5_1_D
I0625 14:48:03.421502 29461 net.cpp:386] relu5_1_D -> conv5_1_D (in-place)
I0625 14:48:03.421679 29461 net.cpp:141] Setting up relu5_1_D
I0625 14:48:03.421690 29461 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 14:48:03.421694 29461 net.cpp:156] Memory required for data: 137152512
I0625 14:48:03.421699 29461 layer_factory.hpp:77] Creating layer upsample4
I0625 14:48:03.421710 29461 net.cpp:91] Creating Layer upsample4
I0625 14:48:03.421715 29461 net.cpp:425] upsample4 <- conv5_1_D
I0625 14:48:03.421722 29461 net.cpp:425] upsample4 <- pool4_mask
I0625 14:48:03.421730 29461 net.cpp:399] upsample4 -> pool4_D
I0625 14:48:03.421739 29461 upsample_layer.cpp:31] Params 'pad_out_{}_' are deprecated. Please declare upsample height and width useing the upsample_h, upsample_w parameters.
I0625 14:48:03.421779 29461 net.cpp:141] Setting up upsample4
I0625 14:48:03.421788 29461 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 14:48:03.421792 29461 net.cpp:156] Memory required for data: 138184704
I0625 14:48:03.421797 29461 layer_factory.hpp:77] Creating layer conv4_2_D
I0625 14:48:03.421808 29461 net.cpp:91] Creating Layer conv4_2_D
I0625 14:48:03.421814 29461 net.cpp:425] conv4_2_D <- pool4_D
I0625 14:48:03.421823 29461 net.cpp:399] conv4_2_D -> conv4_2_D
I0625 14:48:03.427419 29461 net.cpp:141] Setting up conv4_2_D
I0625 14:48:03.427435 29461 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 14:48:03.427439 29461 net.cpp:156] Memory required for data: 139216896
I0625 14:48:03.427446 29461 layer_factory.hpp:77] Creating layer bn4_2_D
I0625 14:48:03.427459 29461 net.cpp:91] Creating Layer bn4_2_D
I0625 14:48:03.427464 29461 net.cpp:425] bn4_2_D <- conv4_2_D
I0625 14:48:03.427474 29461 net.cpp:386] bn4_2_D -> conv4_2_D (in-place)
I0625 14:48:03.427711 29461 net.cpp:141] Setting up bn4_2_D
I0625 14:48:03.427721 29461 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 14:48:03.427726 29461 net.cpp:156] Memory required for data: 140249088
I0625 14:48:03.427752 29461 layer_factory.hpp:77] Creating layer scale4_2_D
I0625 14:48:03.427767 29461 net.cpp:91] Creating Layer scale4_2_D
I0625 14:48:03.427774 29461 net.cpp:425] scale4_2_D <- conv4_2_D
I0625 14:48:03.427784 29461 net.cpp:386] scale4_2_D -> conv4_2_D (in-place)
I0625 14:48:03.427842 29461 layer_factory.hpp:77] Creating layer scale4_2_D
I0625 14:48:03.427988 29461 net.cpp:141] Setting up scale4_2_D
I0625 14:48:03.427996 29461 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 14:48:03.428000 29461 net.cpp:156] Memory required for data: 141281280
I0625 14:48:03.428007 29461 layer_factory.hpp:77] Creating layer relu4_2_D
I0625 14:48:03.428015 29461 net.cpp:91] Creating Layer relu4_2_D
I0625 14:48:03.428020 29461 net.cpp:425] relu4_2_D <- conv4_2_D
I0625 14:48:03.428025 29461 net.cpp:386] relu4_2_D -> conv4_2_D (in-place)
I0625 14:48:03.428341 29461 net.cpp:141] Setting up relu4_2_D
I0625 14:48:03.428354 29461 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 14:48:03.428359 29461 net.cpp:156] Memory required for data: 142313472
I0625 14:48:03.428364 29461 layer_factory.hpp:77] Creating layer conv4_1_D
I0625 14:48:03.428376 29461 net.cpp:91] Creating Layer conv4_1_D
I0625 14:48:03.428381 29461 net.cpp:425] conv4_1_D <- conv4_2_D
I0625 14:48:03.428390 29461 net.cpp:399] conv4_1_D -> conv4_1_D
I0625 14:48:03.431639 29461 net.cpp:141] Setting up conv4_1_D
I0625 14:48:03.431653 29461 net.cpp:148] Top shape: 1 128 28 36 (129024)
I0625 14:48:03.431658 29461 net.cpp:156] Memory required for data: 142829568
I0625 14:48:03.431664 29461 layer_factory.hpp:77] Creating layer bn4_1_D
I0625 14:48:03.431675 29461 net.cpp:91] Creating Layer bn4_1_D
I0625 14:48:03.431681 29461 net.cpp:425] bn4_1_D <- conv4_1_D
I0625 14:48:03.431689 29461 net.cpp:386] bn4_1_D -> conv4_1_D (in-place)
I0625 14:48:03.431931 29461 net.cpp:141] Setting up bn4_1_D
I0625 14:48:03.431942 29461 net.cpp:148] Top shape: 1 128 28 36 (129024)
I0625 14:48:03.431946 29461 net.cpp:156] Memory required for data: 143345664
I0625 14:48:03.431956 29461 layer_factory.hpp:77] Creating layer scale4_1_D
I0625 14:48:03.431965 29461 net.cpp:91] Creating Layer scale4_1_D
I0625 14:48:03.431970 29461 net.cpp:425] scale4_1_D <- conv4_1_D
I0625 14:48:03.431977 29461 net.cpp:386] scale4_1_D -> conv4_1_D (in-place)
I0625 14:48:03.432035 29461 layer_factory.hpp:77] Creating layer scale4_1_D
I0625 14:48:03.432179 29461 net.cpp:141] Setting up scale4_1_D
I0625 14:48:03.432189 29461 net.cpp:148] Top shape: 1 128 28 36 (129024)
I0625 14:48:03.432193 29461 net.cpp:156] Memory required for data: 143861760
I0625 14:48:03.432202 29461 layer_factory.hpp:77] Creating layer relu4_1_D
I0625 14:48:03.432230 29461 net.cpp:91] Creating Layer relu4_1_D
I0625 14:48:03.432237 29461 net.cpp:425] relu4_1_D <- conv4_1_D
I0625 14:48:03.432245 29461 net.cpp:386] relu4_1_D -> conv4_1_D (in-place)
I0625 14:48:03.432559 29461 net.cpp:141] Setting up relu4_1_D
I0625 14:48:03.432574 29461 net.cpp:148] Top shape: 1 128 28 36 (129024)
I0625 14:48:03.432579 29461 net.cpp:156] Memory required for data: 144377856
I0625 14:48:03.432584 29461 layer_factory.hpp:77] Creating layer upsample3
I0625 14:48:03.432591 29461 net.cpp:91] Creating Layer upsample3
I0625 14:48:03.432596 29461 net.cpp:425] upsample3 <- conv4_1_D
I0625 14:48:03.432602 29461 net.cpp:425] upsample3 <- pool3_mask
I0625 14:48:03.432608 29461 net.cpp:399] upsample3 -> pool3_D
I0625 14:48:03.432617 29461 upsample_layer.cpp:31] Params 'pad_out_{}_' are deprecated. Please declare upsample height and width useing the upsample_h, upsample_w parameters.
I0625 14:48:03.432658 29461 net.cpp:141] Setting up upsample3
I0625 14:48:03.432668 29461 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 14:48:03.432672 29461 net.cpp:156] Memory required for data: 146442240
I0625 14:48:03.432677 29461 layer_factory.hpp:77] Creating layer conv3_2_D
I0625 14:48:03.432688 29461 net.cpp:91] Creating Layer conv3_2_D
I0625 14:48:03.432693 29461 net.cpp:425] conv3_2_D <- pool3_D
I0625 14:48:03.432703 29461 net.cpp:399] conv3_2_D -> conv3_2_D
I0625 14:48:03.435292 29461 net.cpp:141] Setting up conv3_2_D
I0625 14:48:03.435307 29461 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 14:48:03.435312 29461 net.cpp:156] Memory required for data: 148506624
I0625 14:48:03.435320 29461 layer_factory.hpp:77] Creating layer bn3_2_D
I0625 14:48:03.435331 29461 net.cpp:91] Creating Layer bn3_2_D
I0625 14:48:03.435336 29461 net.cpp:425] bn3_2_D <- conv3_2_D
I0625 14:48:03.435348 29461 net.cpp:386] bn3_2_D -> conv3_2_D (in-place)
I0625 14:48:03.435587 29461 net.cpp:141] Setting up bn3_2_D
I0625 14:48:03.435597 29461 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 14:48:03.435601 29461 net.cpp:156] Memory required for data: 150571008
I0625 14:48:03.435611 29461 layer_factory.hpp:77] Creating layer scale3_2_D
I0625 14:48:03.435621 29461 net.cpp:91] Creating Layer scale3_2_D
I0625 14:48:03.435627 29461 net.cpp:425] scale3_2_D <- conv3_2_D
I0625 14:48:03.435634 29461 net.cpp:386] scale3_2_D -> conv3_2_D (in-place)
I0625 14:48:03.435691 29461 layer_factory.hpp:77] Creating layer scale3_2_D
I0625 14:48:03.435835 29461 net.cpp:141] Setting up scale3_2_D
I0625 14:48:03.435845 29461 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 14:48:03.435848 29461 net.cpp:156] Memory required for data: 152635392
I0625 14:48:03.435856 29461 layer_factory.hpp:77] Creating layer relu3_2_D
I0625 14:48:03.435864 29461 net.cpp:91] Creating Layer relu3_2_D
I0625 14:48:03.435868 29461 net.cpp:425] relu3_2_D <- conv3_2_D
I0625 14:48:03.435876 29461 net.cpp:386] relu3_2_D -> conv3_2_D (in-place)
I0625 14:48:03.436053 29461 net.cpp:141] Setting up relu3_2_D
I0625 14:48:03.436064 29461 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 14:48:03.436069 29461 net.cpp:156] Memory required for data: 154699776
I0625 14:48:03.436074 29461 layer_factory.hpp:77] Creating layer conv3_1_D
I0625 14:48:03.436086 29461 net.cpp:91] Creating Layer conv3_1_D
I0625 14:48:03.436091 29461 net.cpp:425] conv3_1_D <- conv3_2_D
I0625 14:48:03.436100 29461 net.cpp:399] conv3_1_D -> conv3_1_D
I0625 14:48:03.437638 29461 net.cpp:141] Setting up conv3_1_D
I0625 14:48:03.437655 29461 net.cpp:148] Top shape: 1 64 56 72 (258048)
I0625 14:48:03.437659 29461 net.cpp:156] Memory required for data: 155731968
I0625 14:48:03.437666 29461 layer_factory.hpp:77] Creating layer bn3_1_D
I0625 14:48:03.437675 29461 net.cpp:91] Creating Layer bn3_1_D
I0625 14:48:03.437680 29461 net.cpp:425] bn3_1_D <- conv3_1_D
I0625 14:48:03.437690 29461 net.cpp:386] bn3_1_D -> conv3_1_D (in-place)
I0625 14:48:03.437932 29461 net.cpp:141] Setting up bn3_1_D
I0625 14:48:03.437942 29461 net.cpp:148] Top shape: 1 64 56 72 (258048)
I0625 14:48:03.437947 29461 net.cpp:156] Memory required for data: 156764160
I0625 14:48:03.437957 29461 layer_factory.hpp:77] Creating layer scale3_1_D
I0625 14:48:03.437965 29461 net.cpp:91] Creating Layer scale3_1_D
I0625 14:48:03.437970 29461 net.cpp:425] scale3_1_D <- conv3_1_D
I0625 14:48:03.437979 29461 net.cpp:386] scale3_1_D -> conv3_1_D (in-place)
I0625 14:48:03.438036 29461 layer_factory.hpp:77] Creating layer scale3_1_D
I0625 14:48:03.438189 29461 net.cpp:141] Setting up scale3_1_D
I0625 14:48:03.438199 29461 net.cpp:148] Top shape: 1 64 56 72 (258048)
I0625 14:48:03.438204 29461 net.cpp:156] Memory required for data: 157796352
I0625 14:48:03.438210 29461 layer_factory.hpp:77] Creating layer relu3_1_D
I0625 14:48:03.438220 29461 net.cpp:91] Creating Layer relu3_1_D
I0625 14:48:03.438225 29461 net.cpp:425] relu3_1_D <- conv3_1_D
I0625 14:48:03.438230 29461 net.cpp:386] relu3_1_D -> conv3_1_D (in-place)
I0625 14:48:03.438556 29461 net.cpp:141] Setting up relu3_1_D
I0625 14:48:03.438570 29461 net.cpp:148] Top shape: 1 64 56 72 (258048)
I0625 14:48:03.438573 29461 net.cpp:156] Memory required for data: 158828544
I0625 14:48:03.438578 29461 layer_factory.hpp:77] Creating layer upsample2
I0625 14:48:03.438591 29461 net.cpp:91] Creating Layer upsample2
I0625 14:48:03.438596 29461 net.cpp:425] upsample2 <- conv3_1_D
I0625 14:48:03.438601 29461 net.cpp:425] upsample2 <- pool2_mask
I0625 14:48:03.438608 29461 net.cpp:399] upsample2 -> pool2_D
I0625 14:48:03.438628 29461 upsample_layer.cpp:31] Params 'pad_out_{}_' are deprecated. Please declare upsample height and width useing the upsample_h, upsample_w parameters.
I0625 14:48:03.438673 29461 net.cpp:141] Setting up upsample2
I0625 14:48:03.438683 29461 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 14:48:03.438686 29461 net.cpp:156] Memory required for data: 162957312
I0625 14:48:03.438690 29461 layer_factory.hpp:77] Creating layer conv2_2_D
I0625 14:48:03.438702 29461 net.cpp:91] Creating Layer conv2_2_D
I0625 14:48:03.438707 29461 net.cpp:425] conv2_2_D <- pool2_D
I0625 14:48:03.438716 29461 net.cpp:399] conv2_2_D -> conv2_2_D
I0625 14:48:03.439929 29461 net.cpp:141] Setting up conv2_2_D
I0625 14:48:03.439944 29461 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 14:48:03.439949 29461 net.cpp:156] Memory required for data: 167086080
I0625 14:48:03.439955 29461 layer_factory.hpp:77] Creating layer bn2_2_D
I0625 14:48:03.439966 29461 net.cpp:91] Creating Layer bn2_2_D
I0625 14:48:03.439971 29461 net.cpp:425] bn2_2_D <- conv2_2_D
I0625 14:48:03.439978 29461 net.cpp:386] bn2_2_D -> conv2_2_D (in-place)
I0625 14:48:03.440222 29461 net.cpp:141] Setting up bn2_2_D
I0625 14:48:03.440232 29461 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 14:48:03.440235 29461 net.cpp:156] Memory required for data: 171214848
I0625 14:48:03.440244 29461 layer_factory.hpp:77] Creating layer scale2_2_D
I0625 14:48:03.440255 29461 net.cpp:91] Creating Layer scale2_2_D
I0625 14:48:03.440260 29461 net.cpp:425] scale2_2_D <- conv2_2_D
I0625 14:48:03.440268 29461 net.cpp:386] scale2_2_D -> conv2_2_D (in-place)
I0625 14:48:03.440327 29461 layer_factory.hpp:77] Creating layer scale2_2_D
I0625 14:48:03.440495 29461 net.cpp:141] Setting up scale2_2_D
I0625 14:48:03.440505 29461 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 14:48:03.440508 29461 net.cpp:156] Memory required for data: 175343616
I0625 14:48:03.440515 29461 layer_factory.hpp:77] Creating layer relu2_2_D
I0625 14:48:03.440523 29461 net.cpp:91] Creating Layer relu2_2_D
I0625 14:48:03.440527 29461 net.cpp:425] relu2_2_D <- conv2_2_D
I0625 14:48:03.440536 29461 net.cpp:386] relu2_2_D -> conv2_2_D (in-place)
I0625 14:48:03.440850 29461 net.cpp:141] Setting up relu2_2_D
I0625 14:48:03.440865 29461 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 14:48:03.440870 29461 net.cpp:156] Memory required for data: 179472384
I0625 14:48:03.440873 29461 layer_factory.hpp:77] Creating layer conv2_1_D
I0625 14:48:03.440886 29461 net.cpp:91] Creating Layer conv2_1_D
I0625 14:48:03.440892 29461 net.cpp:425] conv2_1_D <- conv2_2_D
I0625 14:48:03.440901 29461 net.cpp:399] conv2_1_D -> conv2_1_D
I0625 14:48:03.442026 29461 net.cpp:141] Setting up conv2_1_D
I0625 14:48:03.442039 29461 net.cpp:148] Top shape: 1 32 112 144 (516096)
I0625 14:48:03.442044 29461 net.cpp:156] Memory required for data: 181536768
I0625 14:48:03.442051 29461 layer_factory.hpp:77] Creating layer bn2_1_D
I0625 14:48:03.442061 29461 net.cpp:91] Creating Layer bn2_1_D
I0625 14:48:03.442067 29461 net.cpp:425] bn2_1_D <- conv2_1_D
I0625 14:48:03.442073 29461 net.cpp:386] bn2_1_D -> conv2_1_D (in-place)
I0625 14:48:03.442361 29461 net.cpp:141] Setting up bn2_1_D
I0625 14:48:03.442373 29461 net.cpp:148] Top shape: 1 32 112 144 (516096)
I0625 14:48:03.442375 29461 net.cpp:156] Memory required for data: 183601152
I0625 14:48:03.442385 29461 layer_factory.hpp:77] Creating layer scale2_1_D
I0625 14:48:03.442394 29461 net.cpp:91] Creating Layer scale2_1_D
I0625 14:48:03.442399 29461 net.cpp:425] scale2_1_D <- conv2_1_D
I0625 14:48:03.442406 29461 net.cpp:386] scale2_1_D -> conv2_1_D (in-place)
I0625 14:48:03.442467 29461 layer_factory.hpp:77] Creating layer scale2_1_D
I0625 14:48:03.442637 29461 net.cpp:141] Setting up scale2_1_D
I0625 14:48:03.442647 29461 net.cpp:148] Top shape: 1 32 112 144 (516096)
I0625 14:48:03.442651 29461 net.cpp:156] Memory required for data: 185665536
I0625 14:48:03.442658 29461 layer_factory.hpp:77] Creating layer relu2_1_D
I0625 14:48:03.442667 29461 net.cpp:91] Creating Layer relu2_1_D
I0625 14:48:03.442682 29461 net.cpp:425] relu2_1_D <- conv2_1_D
I0625 14:48:03.442692 29461 net.cpp:386] relu2_1_D -> conv2_1_D (in-place)
I0625 14:48:03.442865 29461 net.cpp:141] Setting up relu2_1_D
I0625 14:48:03.442876 29461 net.cpp:148] Top shape: 1 32 112 144 (516096)
I0625 14:48:03.442880 29461 net.cpp:156] Memory required for data: 187729920
I0625 14:48:03.442885 29461 layer_factory.hpp:77] Creating layer upsample1
I0625 14:48:03.442894 29461 net.cpp:91] Creating Layer upsample1
I0625 14:48:03.442899 29461 net.cpp:425] upsample1 <- conv2_1_D
I0625 14:48:03.442905 29461 net.cpp:425] upsample1 <- pool1_mask
I0625 14:48:03.442912 29461 net.cpp:399] upsample1 -> pool1_D
I0625 14:48:03.442920 29461 upsample_layer.cpp:31] Params 'pad_out_{}_' are deprecated. Please declare upsample height and width useing the upsample_h, upsample_w parameters.
I0625 14:48:03.442962 29461 net.cpp:141] Setting up upsample1
I0625 14:48:03.442970 29461 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 14:48:03.442975 29461 net.cpp:156] Memory required for data: 195987456
I0625 14:48:03.442978 29461 layer_factory.hpp:77] Creating layer conv1_2_D
I0625 14:48:03.442991 29461 net.cpp:91] Creating Layer conv1_2_D
I0625 14:48:03.442996 29461 net.cpp:425] conv1_2_D <- pool1_D
I0625 14:48:03.443004 29461 net.cpp:399] conv1_2_D -> conv1_2_D
I0625 14:48:03.444197 29461 net.cpp:141] Setting up conv1_2_D
I0625 14:48:03.444212 29461 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 14:48:03.444217 29461 net.cpp:156] Memory required for data: 204244992
I0625 14:48:03.444224 29461 layer_factory.hpp:77] Creating layer bn1_2_D
I0625 14:48:03.444232 29461 net.cpp:91] Creating Layer bn1_2_D
I0625 14:48:03.444237 29461 net.cpp:425] bn1_2_D <- conv1_2_D
I0625 14:48:03.444247 29461 net.cpp:386] bn1_2_D -> conv1_2_D (in-place)
I0625 14:48:03.445118 29461 net.cpp:141] Setting up bn1_2_D
I0625 14:48:03.445132 29461 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 14:48:03.445135 29461 net.cpp:156] Memory required for data: 212502528
I0625 14:48:03.445145 29461 layer_factory.hpp:77] Creating layer scale1_2_D
I0625 14:48:03.445158 29461 net.cpp:91] Creating Layer scale1_2_D
I0625 14:48:03.445163 29461 net.cpp:425] scale1_2_D <- conv1_2_D
I0625 14:48:03.445169 29461 net.cpp:386] scale1_2_D -> conv1_2_D (in-place)
I0625 14:48:03.445231 29461 layer_factory.hpp:77] Creating layer scale1_2_D
I0625 14:48:03.445449 29461 net.cpp:141] Setting up scale1_2_D
I0625 14:48:03.445461 29461 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 14:48:03.445464 29461 net.cpp:156] Memory required for data: 220760064
I0625 14:48:03.445472 29461 layer_factory.hpp:77] Creating layer relu1_2_D
I0625 14:48:03.445482 29461 net.cpp:91] Creating Layer relu1_2_D
I0625 14:48:03.445487 29461 net.cpp:425] relu1_2_D <- conv1_2_D
I0625 14:48:03.445492 29461 net.cpp:386] relu1_2_D -> conv1_2_D (in-place)
I0625 14:48:03.445823 29461 net.cpp:141] Setting up relu1_2_D
I0625 14:48:03.445837 29461 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 14:48:03.445842 29461 net.cpp:156] Memory required for data: 229017600
I0625 14:48:03.445845 29461 layer_factory.hpp:77] Creating layer conv1_1_D
I0625 14:48:03.445864 29461 net.cpp:91] Creating Layer conv1_1_D
I0625 14:48:03.445873 29461 net.cpp:425] conv1_1_D <- conv1_2_D
I0625 14:48:03.445883 29461 net.cpp:399] conv1_1_D -> conv1_1_D
I0625 14:48:03.447130 29461 net.cpp:141] Setting up conv1_1_D
I0625 14:48:03.447144 29461 net.cpp:148] Top shape: 1 2 224 288 (129024)
I0625 14:48:03.447154 29461 net.cpp:156] Memory required for data: 229533696
I0625 14:48:03.447163 29461 layer_factory.hpp:77] Creating layer conv1_1_D_conv1_1_D_0_split
I0625 14:48:03.447172 29461 net.cpp:91] Creating Layer conv1_1_D_conv1_1_D_0_split
I0625 14:48:03.447177 29461 net.cpp:425] conv1_1_D_conv1_1_D_0_split <- conv1_1_D
I0625 14:48:03.447187 29461 net.cpp:399] conv1_1_D_conv1_1_D_0_split -> conv1_1_D_conv1_1_D_0_split_0
I0625 14:48:03.447197 29461 net.cpp:399] conv1_1_D_conv1_1_D_0_split -> conv1_1_D_conv1_1_D_0_split_1
I0625 14:48:03.447263 29461 net.cpp:141] Setting up conv1_1_D_conv1_1_D_0_split
I0625 14:48:03.447281 29461 net.cpp:148] Top shape: 1 2 224 288 (129024)
I0625 14:48:03.447288 29461 net.cpp:148] Top shape: 1 2 224 288 (129024)
I0625 14:48:03.447290 29461 net.cpp:156] Memory required for data: 230565888
I0625 14:48:03.447295 29461 layer_factory.hpp:77] Creating layer loss
I0625 14:48:03.447305 29461 net.cpp:91] Creating Layer loss
I0625 14:48:03.447310 29461 net.cpp:425] loss <- conv1_1_D_conv1_1_D_0_split_0
I0625 14:48:03.447315 29461 net.cpp:425] loss <- label_data_1_split_0
I0625 14:48:03.447322 29461 net.cpp:399] loss -> loss
I0625 14:48:03.447332 29461 layer_factory.hpp:77] Creating layer loss
I0625 14:48:03.448453 29461 net.cpp:141] Setting up loss
I0625 14:48:03.448467 29461 net.cpp:148] Top shape: (1)
I0625 14:48:03.448470 29461 net.cpp:151]     with loss weight 1
I0625 14:48:03.448483 29461 net.cpp:156] Memory required for data: 230565892
I0625 14:48:03.448488 29461 layer_factory.hpp:77] Creating layer accuracy
I0625 14:48:03.448498 29461 net.cpp:91] Creating Layer accuracy
I0625 14:48:03.448503 29461 net.cpp:425] accuracy <- conv1_1_D_conv1_1_D_0_split_1
I0625 14:48:03.448509 29461 net.cpp:425] accuracy <- label_data_1_split_1
I0625 14:48:03.448515 29461 net.cpp:399] accuracy -> accuracy
I0625 14:48:03.448526 29461 net.cpp:141] Setting up accuracy
I0625 14:48:03.448534 29461 net.cpp:148] Top shape: (1)
I0625 14:48:03.448539 29461 net.cpp:156] Memory required for data: 230565896
I0625 14:48:03.448542 29461 net.cpp:219] accuracy does not need backward computation.
I0625 14:48:03.448549 29461 net.cpp:217] loss needs backward computation.
I0625 14:48:03.448554 29461 net.cpp:217] conv1_1_D_conv1_1_D_0_split needs backward computation.
I0625 14:48:03.448559 29461 net.cpp:217] conv1_1_D needs backward computation.
I0625 14:48:03.448562 29461 net.cpp:217] relu1_2_D needs backward computation.
I0625 14:48:03.448566 29461 net.cpp:217] scale1_2_D needs backward computation.
I0625 14:48:03.448570 29461 net.cpp:217] bn1_2_D needs backward computation.
I0625 14:48:03.448573 29461 net.cpp:217] conv1_2_D needs backward computation.
I0625 14:48:03.448577 29461 net.cpp:217] upsample1 needs backward computation.
I0625 14:48:03.448581 29461 net.cpp:217] relu2_1_D needs backward computation.
I0625 14:48:03.448585 29461 net.cpp:217] scale2_1_D needs backward computation.
I0625 14:48:03.448590 29461 net.cpp:217] bn2_1_D needs backward computation.
I0625 14:48:03.448592 29461 net.cpp:217] conv2_1_D needs backward computation.
I0625 14:48:03.448596 29461 net.cpp:217] relu2_2_D needs backward computation.
I0625 14:48:03.448601 29461 net.cpp:217] scale2_2_D needs backward computation.
I0625 14:48:03.448604 29461 net.cpp:217] bn2_2_D needs backward computation.
I0625 14:48:03.448608 29461 net.cpp:217] conv2_2_D needs backward computation.
I0625 14:48:03.448612 29461 net.cpp:217] upsample2 needs backward computation.
I0625 14:48:03.448616 29461 net.cpp:217] relu3_1_D needs backward computation.
I0625 14:48:03.448621 29461 net.cpp:217] scale3_1_D needs backward computation.
I0625 14:48:03.448624 29461 net.cpp:217] bn3_1_D needs backward computation.
I0625 14:48:03.448628 29461 net.cpp:217] conv3_1_D needs backward computation.
I0625 14:48:03.448632 29461 net.cpp:217] relu3_2_D needs backward computation.
I0625 14:48:03.448637 29461 net.cpp:217] scale3_2_D needs backward computation.
I0625 14:48:03.448640 29461 net.cpp:217] bn3_2_D needs backward computation.
I0625 14:48:03.448643 29461 net.cpp:217] conv3_2_D needs backward computation.
I0625 14:48:03.448647 29461 net.cpp:217] upsample3 needs backward computation.
I0625 14:48:03.448652 29461 net.cpp:217] relu4_1_D needs backward computation.
I0625 14:48:03.448654 29461 net.cpp:217] scale4_1_D needs backward computation.
I0625 14:48:03.448658 29461 net.cpp:217] bn4_1_D needs backward computation.
I0625 14:48:03.448662 29461 net.cpp:217] conv4_1_D needs backward computation.
I0625 14:48:03.448667 29461 net.cpp:217] relu4_2_D needs backward computation.
I0625 14:48:03.448670 29461 net.cpp:217] scale4_2_D needs backward computation.
I0625 14:48:03.448683 29461 net.cpp:217] bn4_2_D needs backward computation.
I0625 14:48:03.448688 29461 net.cpp:217] conv4_2_D needs backward computation.
I0625 14:48:03.448693 29461 net.cpp:217] upsample4 needs backward computation.
I0625 14:48:03.448696 29461 net.cpp:217] relu5_1_D needs backward computation.
I0625 14:48:03.448700 29461 net.cpp:217] scale5_1_D needs backward computation.
I0625 14:48:03.448704 29461 net.cpp:217] bn5_1_D needs backward computation.
I0625 14:48:03.448707 29461 net.cpp:217] conv5_1_D needs backward computation.
I0625 14:48:03.448711 29461 net.cpp:217] relu5_2_D needs backward computation.
I0625 14:48:03.448715 29461 net.cpp:217] scale5_2_D needs backward computation.
I0625 14:48:03.448719 29461 net.cpp:217] bn5_2_D needs backward computation.
I0625 14:48:03.448724 29461 net.cpp:217] conv5_2_D needs backward computation.
I0625 14:48:03.448727 29461 net.cpp:217] upsample5 needs backward computation.
I0625 14:48:03.448734 29461 net.cpp:217] pool5 needs backward computation.
I0625 14:48:03.448739 29461 net.cpp:217] relu5_2 needs backward computation.
I0625 14:48:03.448742 29461 net.cpp:217] scale5_2 needs backward computation.
I0625 14:48:03.448746 29461 net.cpp:217] bn5_2 needs backward computation.
I0625 14:48:03.448750 29461 net.cpp:217] conv5_2 needs backward computation.
I0625 14:48:03.448753 29461 net.cpp:217] relu5_1 needs backward computation.
I0625 14:48:03.448757 29461 net.cpp:217] scale5_1 needs backward computation.
I0625 14:48:03.448761 29461 net.cpp:217] bn5_1 needs backward computation.
I0625 14:48:03.448765 29461 net.cpp:217] conv5_1 needs backward computation.
I0625 14:48:03.448770 29461 net.cpp:217] pool4 needs backward computation.
I0625 14:48:03.448773 29461 net.cpp:217] relu4_2 needs backward computation.
I0625 14:48:03.448777 29461 net.cpp:217] scale4_2 needs backward computation.
I0625 14:48:03.448781 29461 net.cpp:217] bn4_2 needs backward computation.
I0625 14:48:03.448784 29461 net.cpp:217] conv4_2 needs backward computation.
I0625 14:48:03.448788 29461 net.cpp:217] relu4_1 needs backward computation.
I0625 14:48:03.448792 29461 net.cpp:217] scale4_1 needs backward computation.
I0625 14:48:03.448796 29461 net.cpp:217] bn4_1 needs backward computation.
I0625 14:48:03.448801 29461 net.cpp:217] conv4_1 needs backward computation.
I0625 14:48:03.448803 29461 net.cpp:217] pool3 needs backward computation.
I0625 14:48:03.448807 29461 net.cpp:217] relu3_2 needs backward computation.
I0625 14:48:03.448812 29461 net.cpp:217] scale3_2 needs backward computation.
I0625 14:48:03.448815 29461 net.cpp:217] bn3_2 needs backward computation.
I0625 14:48:03.448819 29461 net.cpp:217] conv3_2 needs backward computation.
I0625 14:48:03.448823 29461 net.cpp:217] relu3_1 needs backward computation.
I0625 14:48:03.448827 29461 net.cpp:217] scale3_1 needs backward computation.
I0625 14:48:03.448832 29461 net.cpp:217] bn3_1 needs backward computation.
I0625 14:48:03.448834 29461 net.cpp:217] conv3_1 needs backward computation.
I0625 14:48:03.448839 29461 net.cpp:217] pool2 needs backward computation.
I0625 14:48:03.448843 29461 net.cpp:217] relu2_2 needs backward computation.
I0625 14:48:03.448846 29461 net.cpp:217] scale2_2 needs backward computation.
I0625 14:48:03.448850 29461 net.cpp:217] bn2_2 needs backward computation.
I0625 14:48:03.448854 29461 net.cpp:217] conv2_2 needs backward computation.
I0625 14:48:03.448858 29461 net.cpp:217] relu2_1 needs backward computation.
I0625 14:48:03.448863 29461 net.cpp:217] scale2_1 needs backward computation.
I0625 14:48:03.448866 29461 net.cpp:217] bn2_1 needs backward computation.
I0625 14:48:03.448869 29461 net.cpp:217] conv2_1 needs backward computation.
I0625 14:48:03.448873 29461 net.cpp:217] pool1 needs backward computation.
I0625 14:48:03.448878 29461 net.cpp:217] relu1_2 needs backward computation.
I0625 14:48:03.448882 29461 net.cpp:217] scale1_2 needs backward computation.
I0625 14:48:03.448885 29461 net.cpp:217] bn1_2 needs backward computation.
I0625 14:48:03.448889 29461 net.cpp:217] conv1_2 needs backward computation.
I0625 14:48:03.448894 29461 net.cpp:217] relu1_1 needs backward computation.
I0625 14:48:03.448904 29461 net.cpp:217] scale1_1 needs backward computation.
I0625 14:48:03.448909 29461 net.cpp:217] bn1_1 needs backward computation.
I0625 14:48:03.448912 29461 net.cpp:217] conv1_1 needs backward computation.
I0625 14:48:03.448917 29461 net.cpp:219] label_data_1_split does not need backward computation.
I0625 14:48:03.448922 29461 net.cpp:219] data does not need backward computation.
I0625 14:48:03.448925 29461 net.cpp:261] This network produces output accuracy
I0625 14:48:03.448930 29461 net.cpp:261] This network produces output loss
I0625 14:48:03.448977 29461 net.cpp:274] Network initialization done.
I0625 14:48:03.449249 29461 solver.cpp:60] Solver scaffolding done.
I0625 14:48:03.453806 29461 caffe.cpp:219] Starting Optimization
I0625 14:48:03.453814 29461 solver.cpp:279] Solving segnet
I0625 14:48:03.453819 29461 solver.cpp:280] Learning Rate Policy: step
I0625 14:48:03.458366 29461 solver.cpp:337] Iteration 0, Testing net (#0)
I0625 14:48:06.828500 29461 solver.cpp:404]     Test net output #0: accuracy = 0.514377
I0625 14:48:06.828524 29461 solver.cpp:404]     Test net output #1: loss = 0.743533 (* 1 = 0.743533 loss)
I0625 14:48:07.786799 29461 solver.cpp:228] Iteration 0, loss = 0.742868
I0625 14:48:07.786825 29461 solver.cpp:244]     Train net output #0: accuracy = 0.513033
I0625 14:48:07.786834 29461 solver.cpp:244]     Train net output #1: loss = 0.742868 (* 1 = 0.742868 loss)
I0625 14:48:07.786849 29461 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0625 14:48:26.092792 29461 solver.cpp:228] Iteration 20, loss = 0.257517
I0625 14:48:26.092816 29461 solver.cpp:244]     Train net output #0: accuracy = 0.96555
I0625 14:48:26.092823 29461 solver.cpp:244]     Train net output #1: loss = 0.257517 (* 1 = 0.257517 loss)
I0625 14:48:26.092828 29461 sgd_solver.cpp:106] Iteration 20, lr = 0.01
I0625 14:48:44.833962 29461 solver.cpp:228] Iteration 40, loss = 0.149094
I0625 14:48:44.834038 29461 solver.cpp:244]     Train net output #0: accuracy = 0.966997
I0625 14:48:44.834048 29461 solver.cpp:244]     Train net output #1: loss = 0.149094 (* 1 = 0.149094 loss)
I0625 14:48:44.834053 29461 sgd_solver.cpp:106] Iteration 40, lr = 0.01
I0625 14:49:03.648717 29461 solver.cpp:228] Iteration 60, loss = 0.145156
I0625 14:49:03.648742 29461 solver.cpp:244]     Train net output #0: accuracy = 0.967041
I0625 14:49:03.648749 29461 solver.cpp:244]     Train net output #1: loss = 0.145156 (* 1 = 0.145156 loss)
I0625 14:49:03.648754 29461 sgd_solver.cpp:106] Iteration 60, lr = 0.01
I0625 14:49:22.528688 29461 solver.cpp:228] Iteration 80, loss = 0.13533
I0625 14:49:22.528784 29461 solver.cpp:244]     Train net output #0: accuracy = 0.968259
I0625 14:49:22.528795 29461 solver.cpp:244]     Train net output #1: loss = 0.13533 (* 1 = 0.13533 loss)
I0625 14:49:22.528800 29461 sgd_solver.cpp:106] Iteration 80, lr = 0.01
I0625 14:49:40.879561 29461 solver.cpp:337] Iteration 100, Testing net (#0)
I0625 14:49:44.177528 29461 solver.cpp:404]     Test net output #0: accuracy = 0.969431
I0625 14:49:44.177552 29461 solver.cpp:404]     Test net output #1: loss = 0.127899 (* 1 = 0.127899 loss)
I0625 14:49:44.708513 29461 solver.cpp:228] Iteration 100, loss = 0.12918
I0625 14:49:44.708536 29461 solver.cpp:244]     Train net output #0: accuracy = 0.968426
I0625 14:49:44.708544 29461 solver.cpp:244]     Train net output #1: loss = 0.12918 (* 1 = 0.12918 loss)
I0625 14:49:44.708549 29461 sgd_solver.cpp:106] Iteration 100, lr = 0.01
I0625 14:50:03.607035 29461 solver.cpp:228] Iteration 120, loss = 0.126627
I0625 14:50:03.607136 29461 solver.cpp:244]     Train net output #0: accuracy = 0.967465
I0625 14:50:03.607146 29461 solver.cpp:244]     Train net output #1: loss = 0.126627 (* 1 = 0.126627 loss)
I0625 14:50:03.607153 29461 sgd_solver.cpp:106] Iteration 120, lr = 0.01
I0625 14:50:22.503597 29461 solver.cpp:228] Iteration 140, loss = 0.117438
I0625 14:50:22.503633 29461 solver.cpp:244]     Train net output #0: accuracy = 0.968464
I0625 14:50:22.503639 29461 solver.cpp:244]     Train net output #1: loss = 0.117438 (* 1 = 0.117438 loss)
I0625 14:50:22.503644 29461 sgd_solver.cpp:106] Iteration 140, lr = 0.01
I0625 14:50:41.567956 29461 solver.cpp:228] Iteration 160, loss = 0.102031
I0625 14:50:41.568081 29461 solver.cpp:244]     Train net output #0: accuracy = 0.969806
I0625 14:50:41.568091 29461 solver.cpp:244]     Train net output #1: loss = 0.102031 (* 1 = 0.102031 loss)
I0625 14:50:41.568096 29461 sgd_solver.cpp:106] Iteration 160, lr = 0.01
I0625 14:51:00.470219 29461 solver.cpp:228] Iteration 180, loss = 0.0931283
I0625 14:51:00.470255 29461 solver.cpp:244]     Train net output #0: accuracy = 0.968278
I0625 14:51:00.470263 29461 solver.cpp:244]     Train net output #1: loss = 0.0931283 (* 1 = 0.0931283 loss)
I0625 14:51:00.470268 29461 sgd_solver.cpp:106] Iteration 180, lr = 0.01
I0625 14:51:18.846441 29461 solver.cpp:337] Iteration 200, Testing net (#0)
I0625 14:51:22.149503 29461 solver.cpp:404]     Test net output #0: accuracy = 0.968215
I0625 14:51:22.149526 29461 solver.cpp:404]     Test net output #1: loss = 0.0874321 (* 1 = 0.0874321 loss)
I0625 14:51:22.677168 29461 solver.cpp:228] Iteration 200, loss = 0.084137
I0625 14:51:22.677192 29461 solver.cpp:244]     Train net output #0: accuracy = 0.969041
I0625 14:51:22.677201 29461 solver.cpp:244]     Train net output #1: loss = 0.084137 (* 1 = 0.084137 loss)
I0625 14:51:22.677204 29461 sgd_solver.cpp:106] Iteration 200, lr = 0.01
I0625 14:51:41.586110 29461 solver.cpp:228] Iteration 220, loss = 0.0875845
I0625 14:51:41.586135 29461 solver.cpp:244]     Train net output #0: accuracy = 0.967735
I0625 14:51:41.586143 29461 solver.cpp:244]     Train net output #1: loss = 0.0875845 (* 1 = 0.0875845 loss)
I0625 14:51:41.586148 29461 sgd_solver.cpp:106] Iteration 220, lr = 0.01
I0625 14:52:00.502509 29461 solver.cpp:228] Iteration 240, loss = 0.0728614
I0625 14:52:00.502617 29461 solver.cpp:244]     Train net output #0: accuracy = 0.970251
I0625 14:52:00.502627 29461 solver.cpp:244]     Train net output #1: loss = 0.0728614 (* 1 = 0.0728614 loss)
I0625 14:52:00.502632 29461 sgd_solver.cpp:106] Iteration 240, lr = 0.01
I0625 14:52:19.402381 29461 solver.cpp:228] Iteration 260, loss = 0.068906
I0625 14:52:19.402417 29461 solver.cpp:244]     Train net output #0: accuracy = 0.968391
I0625 14:52:19.402426 29461 solver.cpp:244]     Train net output #1: loss = 0.068906 (* 1 = 0.068906 loss)
I0625 14:52:19.402431 29461 sgd_solver.cpp:106] Iteration 260, lr = 0.01
I0625 14:52:38.321545 29461 solver.cpp:228] Iteration 280, loss = 0.0618376
I0625 14:52:38.321679 29461 solver.cpp:244]     Train net output #0: accuracy = 0.968034
I0625 14:52:38.321691 29461 solver.cpp:244]     Train net output #1: loss = 0.0618376 (* 1 = 0.0618376 loss)
I0625 14:52:38.321696 29461 sgd_solver.cpp:106] Iteration 280, lr = 0.01
I0625 14:52:56.784951 29461 solver.cpp:337] Iteration 300, Testing net (#0)
I0625 14:53:00.084096 29461 solver.cpp:404]     Test net output #0: accuracy = 0.97578
I0625 14:53:00.084132 29461 solver.cpp:404]     Test net output #1: loss = 0.0594981 (* 1 = 0.0594981 loss)
I0625 14:53:00.614656 29461 solver.cpp:228] Iteration 300, loss = 0.0618771
I0625 14:53:00.614691 29461 solver.cpp:244]     Train net output #0: accuracy = 0.975272
I0625 14:53:00.614699 29461 solver.cpp:244]     Train net output #1: loss = 0.0618771 (* 1 = 0.0618771 loss)
I0625 14:53:00.614704 29461 sgd_solver.cpp:106] Iteration 300, lr = 0.01
I0625 14:53:19.545650 29461 solver.cpp:228] Iteration 320, loss = 0.0544028
I0625 14:53:19.545754 29461 solver.cpp:244]     Train net output #0: accuracy = 0.976968
I0625 14:53:19.545763 29461 solver.cpp:244]     Train net output #1: loss = 0.0544028 (* 1 = 0.0544028 loss)
I0625 14:53:19.545768 29461 sgd_solver.cpp:106] Iteration 320, lr = 0.01
I0625 14:53:38.478932 29461 solver.cpp:228] Iteration 340, loss = 0.0498329
I0625 14:53:38.478956 29461 solver.cpp:244]     Train net output #0: accuracy = 0.980979
I0625 14:53:38.478965 29461 solver.cpp:244]     Train net output #1: loss = 0.0498329 (* 1 = 0.0498329 loss)
I0625 14:53:38.478970 29461 sgd_solver.cpp:106] Iteration 340, lr = 0.01
I0625 14:53:57.415447 29461 solver.cpp:228] Iteration 360, loss = 0.0565831
I0625 14:53:57.415555 29461 solver.cpp:244]     Train net output #0: accuracy = 0.980573
I0625 14:53:57.415570 29461 solver.cpp:244]     Train net output #1: loss = 0.0565831 (* 1 = 0.0565831 loss)
I0625 14:53:57.415578 29461 sgd_solver.cpp:106] Iteration 360, lr = 0.01
I0625 14:54:16.334592 29461 solver.cpp:228] Iteration 380, loss = 0.0523506
I0625 14:54:16.334616 29461 solver.cpp:244]     Train net output #0: accuracy = 0.978953
I0625 14:54:16.334624 29461 solver.cpp:244]     Train net output #1: loss = 0.0523506 (* 1 = 0.0523506 loss)
I0625 14:54:16.334628 29461 sgd_solver.cpp:106] Iteration 380, lr = 0.01
I0625 14:54:34.736404 29461 solver.cpp:337] Iteration 400, Testing net (#0)
I0625 14:54:38.040468 29461 solver.cpp:404]     Test net output #0: accuracy = 0.978628
I0625 14:54:38.040503 29461 solver.cpp:404]     Test net output #1: loss = 0.0569303 (* 1 = 0.0569303 loss)
I0625 14:54:38.572134 29461 solver.cpp:228] Iteration 400, loss = 0.0489218
I0625 14:54:38.572170 29461 solver.cpp:244]     Train net output #0: accuracy = 0.981883
I0625 14:54:38.572177 29461 solver.cpp:244]     Train net output #1: loss = 0.0489218 (* 1 = 0.0489218 loss)
I0625 14:54:38.572182 29461 sgd_solver.cpp:106] Iteration 400, lr = 0.01
I0625 14:54:57.494907 29461 solver.cpp:228] Iteration 420, loss = 0.0471169
I0625 14:54:57.494933 29461 solver.cpp:244]     Train net output #0: accuracy = 0.983479
I0625 14:54:57.494941 29461 solver.cpp:244]     Train net output #1: loss = 0.0471169 (* 1 = 0.0471169 loss)
I0625 14:54:57.494946 29461 sgd_solver.cpp:106] Iteration 420, lr = 0.01
I0625 14:55:16.419667 29461 solver.cpp:228] Iteration 440, loss = 0.0502033
I0625 14:55:16.419762 29461 solver.cpp:244]     Train net output #0: accuracy = 0.980991
I0625 14:55:16.419772 29461 solver.cpp:244]     Train net output #1: loss = 0.0502033 (* 1 = 0.0502033 loss)
I0625 14:55:16.419777 29461 sgd_solver.cpp:106] Iteration 440, lr = 0.01
I0625 14:55:35.320863 29461 solver.cpp:228] Iteration 460, loss = 0.0524175
I0625 14:55:35.320888 29461 solver.cpp:244]     Train net output #0: accuracy = 0.980189
I0625 14:55:35.320905 29461 solver.cpp:244]     Train net output #1: loss = 0.0524175 (* 1 = 0.0524175 loss)
I0625 14:55:35.320909 29461 sgd_solver.cpp:106] Iteration 460, lr = 0.01
I0625 14:55:54.219117 29461 solver.cpp:228] Iteration 480, loss = 0.0462327
I0625 14:55:54.219221 29461 solver.cpp:244]     Train net output #0: accuracy = 0.982243
I0625 14:55:54.219231 29461 solver.cpp:244]     Train net output #1: loss = 0.0462327 (* 1 = 0.0462327 loss)
I0625 14:55:54.219236 29461 sgd_solver.cpp:106] Iteration 480, lr = 0.01
I0625 14:56:12.594775 29461 solver.cpp:454] Snapshotting to binary proto file data/models/segnet_iter_500.caffemodel
I0625 14:56:12.676321 29461 sgd_solver.cpp:273] Snapshotting solver state to binary proto file data/models/segnet_iter_500.solverstate
I0625 14:56:12.717973 29461 solver.cpp:337] Iteration 500, Testing net (#0)
I0625 14:56:16.040639 29461 solver.cpp:404]     Test net output #0: accuracy = 0.980184
I0625 14:56:16.040663 29461 solver.cpp:404]     Test net output #1: loss = 0.0521971 (* 1 = 0.0521971 loss)
I0625 14:56:16.570633 29461 solver.cpp:228] Iteration 500, loss = 0.0465866
I0625 14:56:16.570658 29461 solver.cpp:244]     Train net output #0: accuracy = 0.981831
I0625 14:56:16.570665 29461 solver.cpp:244]     Train net output #1: loss = 0.0465866 (* 1 = 0.0465866 loss)
I0625 14:56:16.570670 29461 sgd_solver.cpp:106] Iteration 500, lr = 0.01
I0625 14:56:35.497103 29461 solver.cpp:228] Iteration 520, loss = 0.0445898
I0625 14:56:35.497200 29461 solver.cpp:244]     Train net output #0: accuracy = 0.982957
I0625 14:56:35.497210 29461 solver.cpp:244]     Train net output #1: loss = 0.0445898 (* 1 = 0.0445898 loss)
I0625 14:56:35.497213 29461 sgd_solver.cpp:106] Iteration 520, lr = 0.01
I0625 14:56:54.396996 29461 solver.cpp:228] Iteration 540, loss = 0.0446649
I0625 14:56:54.397018 29461 solver.cpp:244]     Train net output #0: accuracy = 0.983317
I0625 14:56:54.397025 29461 solver.cpp:244]     Train net output #1: loss = 0.0446649 (* 1 = 0.0446649 loss)
I0625 14:56:54.397029 29461 sgd_solver.cpp:106] Iteration 540, lr = 0.01
I0625 14:57:13.298539 29461 solver.cpp:228] Iteration 560, loss = 0.0405282
I0625 14:57:13.298656 29461 solver.cpp:244]     Train net output #0: accuracy = 0.983619
I0625 14:57:13.298667 29461 solver.cpp:244]     Train net output #1: loss = 0.0405282 (* 1 = 0.0405282 loss)
I0625 14:57:13.298672 29461 sgd_solver.cpp:106] Iteration 560, lr = 0.01
I0625 14:57:32.193400 29461 solver.cpp:228] Iteration 580, loss = 0.0414554
I0625 14:57:32.193424 29461 solver.cpp:244]     Train net output #0: accuracy = 0.983083
I0625 14:57:32.193431 29461 solver.cpp:244]     Train net output #1: loss = 0.0414554 (* 1 = 0.0414554 loss)
I0625 14:57:32.193436 29461 sgd_solver.cpp:106] Iteration 580, lr = 0.01
I0625 14:57:50.574122 29461 solver.cpp:337] Iteration 600, Testing net (#0)
I0625 14:57:53.873628 29461 solver.cpp:404]     Test net output #0: accuracy = 0.982396
I0625 14:57:53.873651 29461 solver.cpp:404]     Test net output #1: loss = 0.0464311 (* 1 = 0.0464311 loss)
I0625 14:57:54.401600 29461 solver.cpp:228] Iteration 600, loss = 0.0472857
I0625 14:57:54.401635 29461 solver.cpp:244]     Train net output #0: accuracy = 0.982788
I0625 14:57:54.401643 29461 solver.cpp:244]     Train net output #1: loss = 0.0472857 (* 1 = 0.0472857 loss)
I0625 14:57:54.401648 29461 sgd_solver.cpp:106] Iteration 600, lr = 0.01
I0625 14:58:13.324591 29461 solver.cpp:228] Iteration 620, loss = 0.0369812
I0625 14:58:13.324615 29461 solver.cpp:244]     Train net output #0: accuracy = 0.985011
I0625 14:58:13.324622 29461 solver.cpp:244]     Train net output #1: loss = 0.0369812 (* 1 = 0.0369812 loss)
I0625 14:58:13.324627 29461 sgd_solver.cpp:106] Iteration 620, lr = 0.01
I0625 14:58:32.260824 29461 solver.cpp:228] Iteration 640, loss = 0.0370331
I0625 14:58:32.260918 29461 solver.cpp:244]     Train net output #0: accuracy = 0.985783
I0625 14:58:32.260928 29461 solver.cpp:244]     Train net output #1: loss = 0.0370331 (* 1 = 0.0370331 loss)
I0625 14:58:32.260932 29461 sgd_solver.cpp:106] Iteration 640, lr = 0.01
I0625 14:58:51.181089 29461 solver.cpp:228] Iteration 660, loss = 0.0326939
I0625 14:58:51.181113 29461 solver.cpp:244]     Train net output #0: accuracy = 0.986519
I0625 14:58:51.181120 29461 solver.cpp:244]     Train net output #1: loss = 0.0326939 (* 1 = 0.0326939 loss)
I0625 14:58:51.181125 29461 sgd_solver.cpp:106] Iteration 660, lr = 0.01
I0625 14:59:10.101843 29461 solver.cpp:228] Iteration 680, loss = 0.03212
I0625 14:59:10.101954 29461 solver.cpp:244]     Train net output #0: accuracy = 0.987482
I0625 14:59:10.101965 29461 solver.cpp:244]     Train net output #1: loss = 0.03212 (* 1 = 0.03212 loss)
I0625 14:59:10.101969 29461 sgd_solver.cpp:106] Iteration 680, lr = 0.01
I0625 14:59:28.510771 29461 solver.cpp:337] Iteration 700, Testing net (#0)
I0625 14:59:31.825582 29461 solver.cpp:404]     Test net output #0: accuracy = 0.982292
I0625 14:59:31.825608 29461 solver.cpp:404]     Test net output #1: loss = 0.0463542 (* 1 = 0.0463542 loss)
I0625 14:59:32.355737 29461 solver.cpp:228] Iteration 700, loss = 0.037909
I0625 14:59:32.355762 29461 solver.cpp:244]     Train net output #0: accuracy = 0.984455
I0625 14:59:32.355769 29461 solver.cpp:244]     Train net output #1: loss = 0.037909 (* 1 = 0.037909 loss)
I0625 14:59:32.355774 29461 sgd_solver.cpp:106] Iteration 700, lr = 0.01
I0625 14:59:51.253417 29461 solver.cpp:228] Iteration 720, loss = 0.0373582
I0625 14:59:51.253509 29461 solver.cpp:244]     Train net output #0: accuracy = 0.984501
I0625 14:59:51.253525 29461 solver.cpp:244]     Train net output #1: loss = 0.0373582 (* 1 = 0.0373582 loss)
I0625 14:59:51.253530 29461 sgd_solver.cpp:106] Iteration 720, lr = 0.01
I0625 15:00:10.145756 29461 solver.cpp:228] Iteration 740, loss = 0.0495554
I0625 15:00:10.145778 29461 solver.cpp:244]     Train net output #0: accuracy = 0.983387
I0625 15:00:10.145787 29461 solver.cpp:244]     Train net output #1: loss = 0.0495554 (* 1 = 0.0495554 loss)
I0625 15:00:10.145790 29461 sgd_solver.cpp:106] Iteration 740, lr = 0.01
I0625 15:00:29.046859 29461 solver.cpp:228] Iteration 760, loss = 0.0331179
I0625 15:00:29.046985 29461 solver.cpp:244]     Train net output #0: accuracy = 0.98704
I0625 15:00:29.046996 29461 solver.cpp:244]     Train net output #1: loss = 0.0331179 (* 1 = 0.0331179 loss)
I0625 15:00:29.047001 29461 sgd_solver.cpp:106] Iteration 760, lr = 0.01
I0625 15:00:47.990442 29461 solver.cpp:228] Iteration 780, loss = 0.0394471
I0625 15:00:47.990466 29461 solver.cpp:244]     Train net output #0: accuracy = 0.983949
I0625 15:00:47.990485 29461 solver.cpp:244]     Train net output #1: loss = 0.0394471 (* 1 = 0.0394471 loss)
I0625 15:00:47.990489 29461 sgd_solver.cpp:106] Iteration 780, lr = 0.01
I0625 15:01:06.356061 29461 solver.cpp:337] Iteration 800, Testing net (#0)
I0625 15:01:09.653569 29461 solver.cpp:404]     Test net output #0: accuracy = 0.982946
I0625 15:01:09.653604 29461 solver.cpp:404]     Test net output #1: loss = 0.0454853 (* 1 = 0.0454853 loss)
I0625 15:01:10.179760 29461 solver.cpp:228] Iteration 800, loss = 0.0336517
I0625 15:01:10.179783 29461 solver.cpp:244]     Train net output #0: accuracy = 0.986305
I0625 15:01:10.179791 29461 solver.cpp:244]     Train net output #1: loss = 0.0336517 (* 1 = 0.0336517 loss)
I0625 15:01:10.179795 29461 sgd_solver.cpp:106] Iteration 800, lr = 0.01
I0625 15:01:29.068688 29461 solver.cpp:228] Iteration 820, loss = 0.0349015
I0625 15:01:29.068712 29461 solver.cpp:244]     Train net output #0: accuracy = 0.98693
I0625 15:01:29.068719 29461 solver.cpp:244]     Train net output #1: loss = 0.0349015 (* 1 = 0.0349015 loss)
I0625 15:01:29.068724 29461 sgd_solver.cpp:106] Iteration 820, lr = 0.01
I0625 15:01:47.994979 29461 solver.cpp:228] Iteration 840, loss = 0.032774
I0625 15:01:47.995062 29461 solver.cpp:244]     Train net output #0: accuracy = 0.986854
I0625 15:01:47.995072 29461 solver.cpp:244]     Train net output #1: loss = 0.032774 (* 1 = 0.032774 loss)
I0625 15:01:47.995077 29461 sgd_solver.cpp:106] Iteration 840, lr = 0.01
I0625 15:02:06.903008 29461 solver.cpp:228] Iteration 860, loss = 0.0337447
I0625 15:02:06.903043 29461 solver.cpp:244]     Train net output #0: accuracy = 0.986105
I0625 15:02:06.903050 29461 solver.cpp:244]     Train net output #1: loss = 0.0337447 (* 1 = 0.0337447 loss)
I0625 15:02:06.903056 29461 sgd_solver.cpp:106] Iteration 860, lr = 0.01
I0625 15:02:25.805318 29461 solver.cpp:228] Iteration 880, loss = 0.0345007
I0625 15:02:25.805480 29461 solver.cpp:244]     Train net output #0: accuracy = 0.985681
I0625 15:02:25.805490 29461 solver.cpp:244]     Train net output #1: loss = 0.0345007 (* 1 = 0.0345007 loss)
I0625 15:02:25.805495 29461 sgd_solver.cpp:106] Iteration 880, lr = 0.01
I0625 15:02:44.204087 29461 solver.cpp:337] Iteration 900, Testing net (#0)
I0625 15:02:47.506871 29461 solver.cpp:404]     Test net output #0: accuracy = 0.982836
I0625 15:02:47.506896 29461 solver.cpp:404]     Test net output #1: loss = 0.0469578 (* 1 = 0.0469578 loss)
I0625 15:02:48.034749 29461 solver.cpp:228] Iteration 900, loss = 0.0334486
I0625 15:02:48.034790 29461 solver.cpp:244]     Train net output #0: accuracy = 0.986661
I0625 15:02:48.034798 29461 solver.cpp:244]     Train net output #1: loss = 0.0334486 (* 1 = 0.0334486 loss)
I0625 15:02:48.034804 29461 sgd_solver.cpp:106] Iteration 900, lr = 0.01
I0625 15:03:06.938676 29461 solver.cpp:228] Iteration 920, loss = 0.0280943
I0625 15:03:06.938779 29461 solver.cpp:244]     Train net output #0: accuracy = 0.988638
I0625 15:03:06.938788 29461 solver.cpp:244]     Train net output #1: loss = 0.0280943 (* 1 = 0.0280943 loss)
I0625 15:03:06.938793 29461 sgd_solver.cpp:106] Iteration 920, lr = 0.01
I0625 15:03:25.847815 29461 solver.cpp:228] Iteration 940, loss = 0.0307917
I0625 15:03:25.847838 29461 solver.cpp:244]     Train net output #0: accuracy = 0.987445
I0625 15:03:25.847846 29461 solver.cpp:244]     Train net output #1: loss = 0.0307917 (* 1 = 0.0307917 loss)
I0625 15:03:25.847851 29461 sgd_solver.cpp:106] Iteration 940, lr = 0.01
I0625 15:03:44.736802 29461 solver.cpp:228] Iteration 960, loss = 0.0270959
I0625 15:03:44.736927 29461 solver.cpp:244]     Train net output #0: accuracy = 0.989131
I0625 15:03:44.736937 29461 solver.cpp:244]     Train net output #1: loss = 0.0270959 (* 1 = 0.0270959 loss)
I0625 15:03:44.736943 29461 sgd_solver.cpp:106] Iteration 960, lr = 0.01
I0625 15:04:03.623916 29461 solver.cpp:228] Iteration 980, loss = 0.0339705
I0625 15:04:03.623952 29461 solver.cpp:244]     Train net output #0: accuracy = 0.986492
I0625 15:04:03.623960 29461 solver.cpp:244]     Train net output #1: loss = 0.0339705 (* 1 = 0.0339705 loss)
I0625 15:04:03.623965 29461 sgd_solver.cpp:106] Iteration 980, lr = 0.01
I0625 15:04:21.995563 29461 solver.cpp:454] Snapshotting to binary proto file data/models/segnet_iter_1000.caffemodel
I0625 15:04:22.046893 29461 sgd_solver.cpp:273] Snapshotting solver state to binary proto file data/models/segnet_iter_1000.solverstate
I0625 15:04:22.070030 29461 solver.cpp:337] Iteration 1000, Testing net (#0)
I0625 15:04:25.374018 29461 solver.cpp:404]     Test net output #0: accuracy = 0.983786
I0625 15:04:25.374053 29461 solver.cpp:404]     Test net output #1: loss = 0.0411483 (* 1 = 0.0411483 loss)
I0625 15:04:25.903508 29461 solver.cpp:228] Iteration 1000, loss = 0.0321908
I0625 15:04:25.903532 29461 solver.cpp:244]     Train net output #0: accuracy = 0.985957
I0625 15:04:25.903539 29461 solver.cpp:244]     Train net output #1: loss = 0.0321908 (* 1 = 0.0321908 loss)
I0625 15:04:25.903543 29461 sgd_solver.cpp:106] Iteration 1000, lr = 0.01
I0625 15:04:44.846158 29461 solver.cpp:228] Iteration 1020, loss = 0.0281741
I0625 15:04:44.846182 29461 solver.cpp:244]     Train net output #0: accuracy = 0.988592
I0625 15:04:44.846201 29461 solver.cpp:244]     Train net output #1: loss = 0.0281741 (* 1 = 0.0281741 loss)
I0625 15:04:44.846205 29461 sgd_solver.cpp:106] Iteration 1020, lr = 0.01
I0625 15:05:03.741888 29461 solver.cpp:228] Iteration 1040, loss = 0.0291334
I0625 15:05:03.741989 29461 solver.cpp:244]     Train net output #0: accuracy = 0.988008
I0625 15:05:03.741999 29461 solver.cpp:244]     Train net output #1: loss = 0.0291334 (* 1 = 0.0291334 loss)
I0625 15:05:03.742004 29461 sgd_solver.cpp:106] Iteration 1040, lr = 0.01
I0625 15:05:22.639134 29461 solver.cpp:228] Iteration 1060, loss = 0.040189
I0625 15:05:22.639160 29461 solver.cpp:244]     Train net output #0: accuracy = 0.983743
I0625 15:05:22.639168 29461 solver.cpp:244]     Train net output #1: loss = 0.040189 (* 1 = 0.040189 loss)
I0625 15:05:22.639173 29461 sgd_solver.cpp:106] Iteration 1060, lr = 0.01
I0625 15:05:41.536294 29461 solver.cpp:228] Iteration 1080, loss = 0.0298351
I0625 15:05:41.536394 29461 solver.cpp:244]     Train net output #0: accuracy = 0.98791
I0625 15:05:41.536403 29461 solver.cpp:244]     Train net output #1: loss = 0.0298351 (* 1 = 0.0298351 loss)
I0625 15:05:41.536408 29461 sgd_solver.cpp:106] Iteration 1080, lr = 0.01
I0625 15:05:59.915937 29461 solver.cpp:337] Iteration 1100, Testing net (#0)
I0625 15:06:03.215276 29461 solver.cpp:404]     Test net output #0: accuracy = 0.984106
I0625 15:06:03.215299 29461 solver.cpp:404]     Test net output #1: loss = 0.0460304 (* 1 = 0.0460304 loss)
I0625 15:06:03.742552 29461 solver.cpp:228] Iteration 1100, loss = 0.0356874
I0625 15:06:03.742588 29461 solver.cpp:244]     Train net output #0: accuracy = 0.98615
I0625 15:06:03.742594 29461 solver.cpp:244]     Train net output #1: loss = 0.0356874 (* 1 = 0.0356874 loss)
I0625 15:06:03.742609 29461 sgd_solver.cpp:106] Iteration 1100, lr = 0.01
I0625 15:06:22.646976 29461 solver.cpp:228] Iteration 1120, loss = 0.0298881
I0625 15:06:22.647052 29461 solver.cpp:244]     Train net output #0: accuracy = 0.988388
I0625 15:06:22.647063 29461 solver.cpp:244]     Train net output #1: loss = 0.0298881 (* 1 = 0.0298881 loss)
I0625 15:06:22.647068 29461 sgd_solver.cpp:106] Iteration 1120, lr = 0.01
I0625 15:06:41.571112 29461 solver.cpp:228] Iteration 1140, loss = 0.0308196
I0625 15:06:41.571154 29461 solver.cpp:244]     Train net output #0: accuracy = 0.987639
I0625 15:06:41.571162 29461 solver.cpp:244]     Train net output #1: loss = 0.0308196 (* 1 = 0.0308196 loss)
I0625 15:06:41.571167 29461 sgd_solver.cpp:106] Iteration 1140, lr = 0.01
I0625 15:07:00.475813 29461 solver.cpp:228] Iteration 1160, loss = 0.0273843
I0625 15:07:00.475944 29461 solver.cpp:244]     Train net output #0: accuracy = 0.989225
I0625 15:07:00.475955 29461 solver.cpp:244]     Train net output #1: loss = 0.0273843 (* 1 = 0.0273843 loss)
I0625 15:07:00.475960 29461 sgd_solver.cpp:106] Iteration 1160, lr = 0.01
I0625 15:07:19.381341 29461 solver.cpp:228] Iteration 1180, loss = 0.0379756
I0625 15:07:19.381379 29461 solver.cpp:244]     Train net output #0: accuracy = 0.987044
I0625 15:07:19.381386 29461 solver.cpp:244]     Train net output #1: loss = 0.0379756 (* 1 = 0.0379756 loss)
I0625 15:07:19.381392 29461 sgd_solver.cpp:106] Iteration 1180, lr = 0.01
I0625 15:07:37.771267 29461 solver.cpp:337] Iteration 1200, Testing net (#0)
I0625 15:07:41.070055 29461 solver.cpp:404]     Test net output #0: accuracy = 0.983132
I0625 15:07:41.070078 29461 solver.cpp:404]     Test net output #1: loss = 0.0452532 (* 1 = 0.0452532 loss)
I0625 15:07:41.598104 29461 solver.cpp:228] Iteration 1200, loss = 0.0266093
I0625 15:07:41.598139 29461 solver.cpp:244]     Train net output #0: accuracy = 0.989135
I0625 15:07:41.598145 29461 solver.cpp:244]     Train net output #1: loss = 0.0266093 (* 1 = 0.0266093 loss)
I0625 15:07:41.598150 29461 sgd_solver.cpp:106] Iteration 1200, lr = 0.01
I0625 15:08:00.504765 29461 solver.cpp:228] Iteration 1220, loss = 0.0315424
I0625 15:08:00.504791 29461 solver.cpp:244]     Train net output #0: accuracy = 0.987773
I0625 15:08:00.504798 29461 solver.cpp:244]     Train net output #1: loss = 0.0315424 (* 1 = 0.0315424 loss)
I0625 15:08:00.504803 29461 sgd_solver.cpp:106] Iteration 1220, lr = 0.01
I0625 15:08:19.398576 29461 solver.cpp:228] Iteration 1240, loss = 0.029927
I0625 15:08:19.398679 29461 solver.cpp:244]     Train net output #0: accuracy = 0.987666
I0625 15:08:19.398687 29461 solver.cpp:244]     Train net output #1: loss = 0.029927 (* 1 = 0.029927 loss)
I0625 15:08:19.398692 29461 sgd_solver.cpp:106] Iteration 1240, lr = 0.01
I0625 15:08:38.332146 29461 solver.cpp:228] Iteration 1260, loss = 0.0256941
I0625 15:08:38.332185 29461 solver.cpp:244]     Train net output #0: accuracy = 0.989237
I0625 15:08:38.332191 29461 solver.cpp:244]     Train net output #1: loss = 0.0256941 (* 1 = 0.0256941 loss)
I0625 15:08:38.332196 29461 sgd_solver.cpp:106] Iteration 1260, lr = 0.01
I0625 15:08:57.249142 29461 solver.cpp:228] Iteration 1280, loss = 0.0259588
I0625 15:08:57.249233 29461 solver.cpp:244]     Train net output #0: accuracy = 0.989672
I0625 15:08:57.249243 29461 solver.cpp:244]     Train net output #1: loss = 0.0259588 (* 1 = 0.0259588 loss)
I0625 15:08:57.249248 29461 sgd_solver.cpp:106] Iteration 1280, lr = 0.01
I0625 15:09:15.617177 29461 solver.cpp:337] Iteration 1300, Testing net (#0)
I0625 15:09:18.923847 29461 solver.cpp:404]     Test net output #0: accuracy = 0.983301
I0625 15:09:18.923872 29461 solver.cpp:404]     Test net output #1: loss = 0.0458028 (* 1 = 0.0458028 loss)
I0625 15:09:19.452167 29461 solver.cpp:228] Iteration 1300, loss = 0.0338077
I0625 15:09:19.452200 29461 solver.cpp:244]     Train net output #0: accuracy = 0.987602
I0625 15:09:19.452208 29461 solver.cpp:244]     Train net output #1: loss = 0.0338077 (* 1 = 0.0338077 loss)
I0625 15:09:19.452214 29461 sgd_solver.cpp:106] Iteration 1300, lr = 0.01
I0625 15:09:38.365312 29461 solver.cpp:228] Iteration 1320, loss = 0.0259207
I0625 15:09:38.365407 29461 solver.cpp:244]     Train net output #0: accuracy = 0.989268
I0625 15:09:38.365417 29461 solver.cpp:244]     Train net output #1: loss = 0.0259207 (* 1 = 0.0259207 loss)
I0625 15:09:38.365422 29461 sgd_solver.cpp:106] Iteration 1320, lr = 0.01
I0625 15:09:57.254549 29461 solver.cpp:228] Iteration 1340, loss = 0.0286854
I0625 15:09:57.254572 29461 solver.cpp:244]     Train net output #0: accuracy = 0.988518
I0625 15:09:57.254580 29461 solver.cpp:244]     Train net output #1: loss = 0.0286854 (* 1 = 0.0286854 loss)
I0625 15:09:57.254585 29461 sgd_solver.cpp:106] Iteration 1340, lr = 0.01
I0625 15:10:16.148277 29461 solver.cpp:228] Iteration 1360, loss = 0.028278
I0625 15:10:16.148396 29461 solver.cpp:244]     Train net output #0: accuracy = 0.988851
I0625 15:10:16.148406 29461 solver.cpp:244]     Train net output #1: loss = 0.028278 (* 1 = 0.028278 loss)
I0625 15:10:16.148411 29461 sgd_solver.cpp:106] Iteration 1360, lr = 0.01
I0625 15:10:35.088850 29461 solver.cpp:228] Iteration 1380, loss = 0.0273679
I0625 15:10:35.088886 29461 solver.cpp:244]     Train net output #0: accuracy = 0.98846
I0625 15:10:35.088892 29461 solver.cpp:244]     Train net output #1: loss = 0.0273679 (* 1 = 0.0273679 loss)
I0625 15:10:35.088897 29461 sgd_solver.cpp:106] Iteration 1380, lr = 0.01
I0625 15:10:53.451712 29461 solver.cpp:337] Iteration 1400, Testing net (#0)
I0625 15:10:56.754021 29461 solver.cpp:404]     Test net output #0: accuracy = 0.984279
I0625 15:10:56.754046 29461 solver.cpp:404]     Test net output #1: loss = 0.0445643 (* 1 = 0.0445643 loss)
I0625 15:10:57.282217 29461 solver.cpp:228] Iteration 1400, loss = 0.0271914
I0625 15:10:57.282241 29461 solver.cpp:244]     Train net output #0: accuracy = 0.989172
I0625 15:10:57.282259 29461 solver.cpp:244]     Train net output #1: loss = 0.0271914 (* 1 = 0.0271914 loss)
I0625 15:10:57.282263 29461 sgd_solver.cpp:106] Iteration 1400, lr = 0.01
I0625 15:11:16.157263 29461 solver.cpp:228] Iteration 1420, loss = 0.0233258
I0625 15:11:16.157300 29461 solver.cpp:244]     Train net output #0: accuracy = 0.990468
I0625 15:11:16.157307 29461 solver.cpp:244]     Train net output #1: loss = 0.0233258 (* 1 = 0.0233258 loss)
I0625 15:11:16.157312 29461 sgd_solver.cpp:106] Iteration 1420, lr = 0.01
I0625 15:11:35.041470 29461 solver.cpp:228] Iteration 1440, loss = 0.0217018
I0625 15:11:35.041569 29461 solver.cpp:244]     Train net output #0: accuracy = 0.991023
I0625 15:11:35.041579 29461 solver.cpp:244]     Train net output #1: loss = 0.0217018 (* 1 = 0.0217018 loss)
I0625 15:11:35.041584 29461 sgd_solver.cpp:106] Iteration 1440, lr = 0.01
I0625 15:11:53.939625 29461 solver.cpp:228] Iteration 1460, loss = 0.0267635
I0625 15:11:53.939649 29461 solver.cpp:244]     Train net output #0: accuracy = 0.989364
I0625 15:11:53.939656 29461 solver.cpp:244]     Train net output #1: loss = 0.0267635 (* 1 = 0.0267635 loss)
I0625 15:11:53.939661 29461 sgd_solver.cpp:106] Iteration 1460, lr = 0.01
I0625 15:12:12.849352 29461 solver.cpp:228] Iteration 1480, loss = 0.0272249
I0625 15:12:12.849444 29461 solver.cpp:244]     Train net output #0: accuracy = 0.98856
I0625 15:12:12.849454 29461 solver.cpp:244]     Train net output #1: loss = 0.0272249 (* 1 = 0.0272249 loss)
I0625 15:12:12.849458 29461 sgd_solver.cpp:106] Iteration 1480, lr = 0.01
I0625 15:12:31.220777 29461 solver.cpp:454] Snapshotting to binary proto file data/models/segnet_iter_1500.caffemodel
I0625 15:12:31.269721 29461 sgd_solver.cpp:273] Snapshotting solver state to binary proto file data/models/segnet_iter_1500.solverstate
I0625 15:12:31.292625 29461 solver.cpp:337] Iteration 1500, Testing net (#0)
I0625 15:12:34.664444 29461 solver.cpp:404]     Test net output #0: accuracy = 0.983719
I0625 15:12:34.664468 29461 solver.cpp:404]     Test net output #1: loss = 0.0482611 (* 1 = 0.0482611 loss)
I0625 15:12:35.193693 29461 solver.cpp:228] Iteration 1500, loss = 0.0259237
I0625 15:12:35.193718 29461 solver.cpp:244]     Train net output #0: accuracy = 0.989601
I0625 15:12:35.193725 29461 solver.cpp:244]     Train net output #1: loss = 0.0259237 (* 1 = 0.0259237 loss)
I0625 15:12:35.193730 29461 sgd_solver.cpp:106] Iteration 1500, lr = 0.01
I0625 15:12:54.097332 29461 solver.cpp:228] Iteration 1520, loss = 0.022822
I0625 15:12:54.097434 29461 solver.cpp:244]     Train net output #0: accuracy = 0.990391
I0625 15:12:54.097445 29461 solver.cpp:244]     Train net output #1: loss = 0.022822 (* 1 = 0.022822 loss)
I0625 15:12:54.097450 29461 sgd_solver.cpp:106] Iteration 1520, lr = 0.01
I0625 15:13:12.988978 29461 solver.cpp:228] Iteration 1540, loss = 0.028211
I0625 15:13:12.989003 29461 solver.cpp:244]     Train net output #0: accuracy = 0.988562
I0625 15:13:12.989012 29461 solver.cpp:244]     Train net output #1: loss = 0.028211 (* 1 = 0.028211 loss)
I0625 15:13:12.989017 29461 sgd_solver.cpp:106] Iteration 1540, lr = 0.01
I0625 15:13:31.894140 29461 solver.cpp:228] Iteration 1560, loss = 0.0223328
I0625 15:13:31.894273 29461 solver.cpp:244]     Train net output #0: accuracy = 0.990488
I0625 15:13:31.894284 29461 solver.cpp:244]     Train net output #1: loss = 0.0223328 (* 1 = 0.0223328 loss)
I0625 15:13:31.894289 29461 sgd_solver.cpp:106] Iteration 1560, lr = 0.01
I0625 15:13:50.800643 29461 solver.cpp:228] Iteration 1580, loss = 0.0286416
I0625 15:13:50.800679 29461 solver.cpp:244]     Train net output #0: accuracy = 0.988431
I0625 15:13:50.800688 29461 solver.cpp:244]     Train net output #1: loss = 0.0286416 (* 1 = 0.0286416 loss)
I0625 15:13:50.800691 29461 sgd_solver.cpp:106] Iteration 1580, lr = 0.01
I0625 15:14:09.176590 29461 solver.cpp:337] Iteration 1600, Testing net (#0)
I0625 15:14:12.473289 29461 solver.cpp:404]     Test net output #0: accuracy = 0.984233
I0625 15:14:12.473312 29461 solver.cpp:404]     Test net output #1: loss = 0.0456407 (* 1 = 0.0456407 loss)
I0625 15:14:13.002301 29461 solver.cpp:228] Iteration 1600, loss = 0.0236113
I0625 15:14:13.002327 29461 solver.cpp:244]     Train net output #0: accuracy = 0.989952
I0625 15:14:13.002334 29461 solver.cpp:244]     Train net output #1: loss = 0.0236113 (* 1 = 0.0236113 loss)
I0625 15:14:13.002341 29461 sgd_solver.cpp:106] Iteration 1600, lr = 0.01
I0625 15:14:31.932029 29461 solver.cpp:228] Iteration 1620, loss = 0.0250162
I0625 15:14:31.932057 29461 solver.cpp:244]     Train net output #0: accuracy = 0.989688
I0625 15:14:31.932066 29461 solver.cpp:244]     Train net output #1: loss = 0.0250162 (* 1 = 0.0250162 loss)
I0625 15:14:31.932072 29461 sgd_solver.cpp:106] Iteration 1620, lr = 0.01
I0625 15:14:50.849993 29461 solver.cpp:228] Iteration 1640, loss = 0.0267802
I0625 15:14:50.850148 29461 solver.cpp:244]     Train net output #0: accuracy = 0.988587
I0625 15:14:50.850157 29461 solver.cpp:244]     Train net output #1: loss = 0.0267802 (* 1 = 0.0267802 loss)
I0625 15:14:50.850162 29461 sgd_solver.cpp:106] Iteration 1640, lr = 0.01
I0625 15:15:09.748410 29461 solver.cpp:228] Iteration 1660, loss = 0.0226882
I0625 15:15:09.748436 29461 solver.cpp:244]     Train net output #0: accuracy = 0.990354
I0625 15:15:09.748443 29461 solver.cpp:244]     Train net output #1: loss = 0.0226882 (* 1 = 0.0226882 loss)
I0625 15:15:09.748448 29461 sgd_solver.cpp:106] Iteration 1660, lr = 0.01
I0625 15:15:28.645808 29461 solver.cpp:228] Iteration 1680, loss = 0.0213049
I0625 15:15:28.645912 29461 solver.cpp:244]     Train net output #0: accuracy = 0.991288
I0625 15:15:28.645921 29461 solver.cpp:244]     Train net output #1: loss = 0.0213049 (* 1 = 0.0213049 loss)
I0625 15:15:28.645926 29461 sgd_solver.cpp:106] Iteration 1680, lr = 0.01
I0625 15:15:47.040004 29461 solver.cpp:337] Iteration 1700, Testing net (#0)
I0625 15:15:50.344063 29461 solver.cpp:404]     Test net output #0: accuracy = 0.984996
I0625 15:15:50.344097 29461 solver.cpp:404]     Test net output #1: loss = 0.0421433 (* 1 = 0.0421433 loss)
I0625 15:15:50.873203 29461 solver.cpp:228] Iteration 1700, loss = 0.0225246
I0625 15:15:50.873227 29461 solver.cpp:244]     Train net output #0: accuracy = 0.990568
I0625 15:15:50.873234 29461 solver.cpp:244]     Train net output #1: loss = 0.0225246 (* 1 = 0.0225246 loss)
I0625 15:15:50.873239 29461 sgd_solver.cpp:106] Iteration 1700, lr = 0.01
I0625 15:16:09.788873 29461 solver.cpp:228] Iteration 1720, loss = 0.0224651
I0625 15:16:09.788975 29461 solver.cpp:244]     Train net output #0: accuracy = 0.990446
I0625 15:16:09.788985 29461 solver.cpp:244]     Train net output #1: loss = 0.0224651 (* 1 = 0.0224651 loss)
I0625 15:16:09.788991 29461 sgd_solver.cpp:106] Iteration 1720, lr = 0.01
I0625 15:16:28.691817 29461 solver.cpp:228] Iteration 1740, loss = 0.0265269
I0625 15:16:28.691851 29461 solver.cpp:244]     Train net output #0: accuracy = 0.988869
I0625 15:16:28.691859 29461 solver.cpp:244]     Train net output #1: loss = 0.0265269 (* 1 = 0.0265269 loss)
I0625 15:16:28.691864 29461 sgd_solver.cpp:106] Iteration 1740, lr = 0.01
I0625 15:16:47.612787 29461 solver.cpp:228] Iteration 1760, loss = 0.0245227
I0625 15:16:47.612911 29461 solver.cpp:244]     Train net output #0: accuracy = 0.990251
I0625 15:16:47.612921 29461 solver.cpp:244]     Train net output #1: loss = 0.0245227 (* 1 = 0.0245227 loss)
I0625 15:16:47.612926 29461 sgd_solver.cpp:106] Iteration 1760, lr = 0.01
I0625 15:17:06.510401 29461 solver.cpp:228] Iteration 1780, loss = 0.0243136
I0625 15:17:06.510426 29461 solver.cpp:244]     Train net output #0: accuracy = 0.989867
I0625 15:17:06.510432 29461 solver.cpp:244]     Train net output #1: loss = 0.0243136 (* 1 = 0.0243136 loss)
I0625 15:17:06.510437 29461 sgd_solver.cpp:106] Iteration 1780, lr = 0.01
I0625 15:17:24.865067 29461 solver.cpp:337] Iteration 1800, Testing net (#0)
I0625 15:17:28.157630 29461 solver.cpp:404]     Test net output #0: accuracy = 0.983243
I0625 15:17:28.157654 29461 solver.cpp:404]     Test net output #1: loss = 0.0489419 (* 1 = 0.0489419 loss)
I0625 15:17:28.685425 29461 solver.cpp:228] Iteration 1800, loss = 0.0212378
I0625 15:17:28.685448 29461 solver.cpp:244]     Train net output #0: accuracy = 0.991235
I0625 15:17:28.685456 29461 solver.cpp:244]     Train net output #1: loss = 0.0212378 (* 1 = 0.0212378 loss)
I0625 15:17:28.685461 29461 sgd_solver.cpp:106] Iteration 1800, lr = 0.01
I0625 15:17:47.567363 29461 solver.cpp:228] Iteration 1820, loss = 0.0246589
I0625 15:17:47.567389 29461 solver.cpp:244]     Train net output #0: accuracy = 0.990299
I0625 15:17:47.567396 29461 solver.cpp:244]     Train net output #1: loss = 0.0246589 (* 1 = 0.0246589 loss)
I0625 15:17:47.567400 29461 sgd_solver.cpp:106] Iteration 1820, lr = 0.01
I0625 15:18:06.470741 29461 solver.cpp:228] Iteration 1840, loss = 0.0251611
I0625 15:18:06.470839 29461 solver.cpp:244]     Train net output #0: accuracy = 0.990449
I0625 15:18:06.470849 29461 solver.cpp:244]     Train net output #1: loss = 0.0251611 (* 1 = 0.0251611 loss)
I0625 15:18:06.470854 29461 sgd_solver.cpp:106] Iteration 1840, lr = 0.01
I0625 15:18:25.377173 29461 solver.cpp:228] Iteration 1860, loss = 0.0208661
I0625 15:18:25.377197 29461 solver.cpp:244]     Train net output #0: accuracy = 0.991376
I0625 15:18:25.377215 29461 solver.cpp:244]     Train net output #1: loss = 0.0208661 (* 1 = 0.0208661 loss)
I0625 15:18:25.377220 29461 sgd_solver.cpp:106] Iteration 1860, lr = 0.01
I0625 15:18:44.305982 29461 solver.cpp:228] Iteration 1880, loss = 0.0203173
I0625 15:18:44.306087 29461 solver.cpp:244]     Train net output #0: accuracy = 0.991228
I0625 15:18:44.306095 29461 solver.cpp:244]     Train net output #1: loss = 0.0203173 (* 1 = 0.0203173 loss)
I0625 15:18:44.306100 29461 sgd_solver.cpp:106] Iteration 1880, lr = 0.01
I0625 15:19:02.670369 29461 solver.cpp:337] Iteration 1900, Testing net (#0)
I0625 15:19:05.969321 29461 solver.cpp:404]     Test net output #0: accuracy = 0.985334
I0625 15:19:05.969343 29461 solver.cpp:404]     Test net output #1: loss = 0.042412 (* 1 = 0.042412 loss)
I0625 15:19:06.500074 29461 solver.cpp:228] Iteration 1900, loss = 0.0221537
I0625 15:19:06.500099 29461 solver.cpp:244]     Train net output #0: accuracy = 0.990907
I0625 15:19:06.500108 29461 solver.cpp:244]     Train net output #1: loss = 0.0221537 (* 1 = 0.0221537 loss)
I0625 15:19:06.500113 29461 sgd_solver.cpp:106] Iteration 1900, lr = 0.01
I0625 15:19:25.389345 29461 solver.cpp:228] Iteration 1920, loss = 0.0222384
I0625 15:19:25.389438 29461 solver.cpp:244]     Train net output #0: accuracy = 0.990817
I0625 15:19:25.389447 29461 solver.cpp:244]     Train net output #1: loss = 0.0222384 (* 1 = 0.0222384 loss)
I0625 15:19:25.389452 29461 sgd_solver.cpp:106] Iteration 1920, lr = 0.01
I0625 15:19:44.297461 29461 solver.cpp:228] Iteration 1940, loss = 0.0211777
I0625 15:19:44.297485 29461 solver.cpp:244]     Train net output #0: accuracy = 0.991341
I0625 15:19:44.297493 29461 solver.cpp:244]     Train net output #1: loss = 0.0211777 (* 1 = 0.0211777 loss)
I0625 15:19:44.297497 29461 sgd_solver.cpp:106] Iteration 1940, lr = 0.01
I0625 15:20:03.209131 29461 solver.cpp:228] Iteration 1960, loss = 0.0207183
I0625 15:20:03.209249 29461 solver.cpp:244]     Train net output #0: accuracy = 0.991395
I0625 15:20:03.209259 29461 solver.cpp:244]     Train net output #1: loss = 0.0207183 (* 1 = 0.0207183 loss)
I0625 15:20:03.209264 29461 sgd_solver.cpp:106] Iteration 1960, lr = 0.01
I0625 15:20:22.120309 29461 solver.cpp:228] Iteration 1980, loss = 0.0177
I0625 15:20:22.120335 29461 solver.cpp:244]     Train net output #0: accuracy = 0.992675
I0625 15:20:22.120342 29461 solver.cpp:244]     Train net output #1: loss = 0.0177 (* 1 = 0.0177 loss)
I0625 15:20:22.120347 29461 sgd_solver.cpp:106] Iteration 1980, lr = 0.01
I0625 15:20:40.519255 29461 solver.cpp:454] Snapshotting to binary proto file data/models/segnet_iter_2000.caffemodel
I0625 15:20:40.566870 29461 sgd_solver.cpp:273] Snapshotting solver state to binary proto file data/models/segnet_iter_2000.solverstate
I0625 15:20:40.589648 29461 solver.cpp:337] Iteration 2000, Testing net (#0)
I0625 15:20:43.886656 29461 solver.cpp:404]     Test net output #0: accuracy = 0.984978
I0625 15:20:43.886679 29461 solver.cpp:404]     Test net output #1: loss = 0.0457479 (* 1 = 0.0457479 loss)
I0625 15:20:44.413521 29461 solver.cpp:228] Iteration 2000, loss = 0.0215505
I0625 15:20:44.413545 29461 solver.cpp:244]     Train net output #0: accuracy = 0.991177
I0625 15:20:44.413552 29461 solver.cpp:244]     Train net output #1: loss = 0.0215505 (* 1 = 0.0215505 loss)
I0625 15:20:44.413557 29461 sgd_solver.cpp:106] Iteration 2000, lr = 0.01
I0625 15:21:03.311478 29461 solver.cpp:228] Iteration 2020, loss = 0.024863
I0625 15:21:03.311514 29461 solver.cpp:244]     Train net output #0: accuracy = 0.989114
I0625 15:21:03.311522 29461 solver.cpp:244]     Train net output #1: loss = 0.024863 (* 1 = 0.024863 loss)
I0625 15:21:03.311527 29461 sgd_solver.cpp:106] Iteration 2020, lr = 0.01
I0625 15:21:22.216261 29461 solver.cpp:228] Iteration 2040, loss = 0.0192363
I0625 15:21:22.216348 29461 solver.cpp:244]     Train net output #0: accuracy = 0.991899
I0625 15:21:22.216357 29461 solver.cpp:244]     Train net output #1: loss = 0.0192363 (* 1 = 0.0192363 loss)
I0625 15:21:22.216362 29461 sgd_solver.cpp:106] Iteration 2040, lr = 0.01
I0625 15:21:41.135790 29461 solver.cpp:228] Iteration 2060, loss = 0.0233689
I0625 15:21:41.135814 29461 solver.cpp:244]     Train net output #0: accuracy = 0.989968
I0625 15:21:41.135823 29461 solver.cpp:244]     Train net output #1: loss = 0.0233689 (* 1 = 0.0233689 loss)
I0625 15:21:41.135828 29461 sgd_solver.cpp:106] Iteration 2060, lr = 0.01
I0625 15:22:00.054852 29461 solver.cpp:228] Iteration 2080, loss = 0.0234429
I0625 15:22:00.054942 29461 solver.cpp:244]     Train net output #0: accuracy = 0.990138
I0625 15:22:00.054952 29461 solver.cpp:244]     Train net output #1: loss = 0.0234429 (* 1 = 0.0234429 loss)
I0625 15:22:00.054957 29461 sgd_solver.cpp:106] Iteration 2080, lr = 0.01
I0625 15:22:18.419802 29461 solver.cpp:337] Iteration 2100, Testing net (#0)
I0625 15:22:21.720721 29461 solver.cpp:404]     Test net output #0: accuracy = 0.984193
I0625 15:22:21.720754 29461 solver.cpp:404]     Test net output #1: loss = 0.0467301 (* 1 = 0.0467301 loss)
I0625 15:22:22.249702 29461 solver.cpp:228] Iteration 2100, loss = 0.0208126
I0625 15:22:22.249727 29461 solver.cpp:244]     Train net output #0: accuracy = 0.991211
I0625 15:22:22.249735 29461 solver.cpp:244]     Train net output #1: loss = 0.0208126 (* 1 = 0.0208126 loss)
I0625 15:22:22.249740 29461 sgd_solver.cpp:106] Iteration 2100, lr = 0.01
I0625 15:22:41.150646 29461 solver.cpp:228] Iteration 2120, loss = 0.0224824
I0625 15:22:41.150743 29461 solver.cpp:244]     Train net output #0: accuracy = 0.990733
I0625 15:22:41.150751 29461 solver.cpp:244]     Train net output #1: loss = 0.0224824 (* 1 = 0.0224824 loss)
I0625 15:22:41.150756 29461 sgd_solver.cpp:106] Iteration 2120, lr = 0.01
I0625 15:23:00.045704 29461 solver.cpp:228] Iteration 2140, loss = 0.0197636
I0625 15:23:00.045728 29461 solver.cpp:244]     Train net output #0: accuracy = 0.991804
I0625 15:23:00.045735 29461 solver.cpp:244]     Train net output #1: loss = 0.0197636 (* 1 = 0.0197636 loss)
I0625 15:23:00.045740 29461 sgd_solver.cpp:106] Iteration 2140, lr = 0.01
I0625 15:23:18.947731 29461 solver.cpp:228] Iteration 2160, loss = 0.0199865
I0625 15:23:18.947973 29461 solver.cpp:244]     Train net output #0: accuracy = 0.991657
I0625 15:23:18.947983 29461 solver.cpp:244]     Train net output #1: loss = 0.0199865 (* 1 = 0.0199865 loss)
I0625 15:23:18.947988 29461 sgd_solver.cpp:106] Iteration 2160, lr = 0.01
I0625 15:23:37.853829 29461 solver.cpp:228] Iteration 2180, loss = 0.022512
I0625 15:23:37.853854 29461 solver.cpp:244]     Train net output #0: accuracy = 0.990697
I0625 15:23:37.853862 29461 solver.cpp:244]     Train net output #1: loss = 0.022512 (* 1 = 0.022512 loss)
I0625 15:23:37.853866 29461 sgd_solver.cpp:106] Iteration 2180, lr = 0.01
I0625 15:23:56.231891 29461 solver.cpp:337] Iteration 2200, Testing net (#0)
I0625 15:23:59.538203 29461 solver.cpp:404]     Test net output #0: accuracy = 0.984112
I0625 15:23:59.538228 29461 solver.cpp:404]     Test net output #1: loss = 0.0456091 (* 1 = 0.0456091 loss)
I0625 15:24:00.065140 29461 solver.cpp:228] Iteration 2200, loss = 0.0191165
I0625 15:24:00.065163 29461 solver.cpp:244]     Train net output #0: accuracy = 0.99219
I0625 15:24:00.065181 29461 solver.cpp:244]     Train net output #1: loss = 0.0191165 (* 1 = 0.0191165 loss)
I0625 15:24:00.065186 29461 sgd_solver.cpp:106] Iteration 2200, lr = 0.01
I0625 15:24:18.967996 29461 solver.cpp:228] Iteration 2220, loss = 0.0181613
I0625 15:24:18.968021 29461 solver.cpp:244]     Train net output #0: accuracy = 0.992374
I0625 15:24:18.968029 29461 solver.cpp:244]     Train net output #1: loss = 0.0181613 (* 1 = 0.0181613 loss)
I0625 15:24:18.968034 29461 sgd_solver.cpp:106] Iteration 2220, lr = 0.01
I0625 15:24:37.858872 29461 solver.cpp:228] Iteration 2240, loss = 0.0212363
I0625 15:24:37.858963 29461 solver.cpp:244]     Train net output #0: accuracy = 0.991362
I0625 15:24:37.858973 29461 solver.cpp:244]     Train net output #1: loss = 0.0212363 (* 1 = 0.0212363 loss)
I0625 15:24:37.858978 29461 sgd_solver.cpp:106] Iteration 2240, lr = 0.01
I0625 15:24:56.733979 29461 solver.cpp:228] Iteration 2260, loss = 0.0203023
I0625 15:24:56.734004 29461 solver.cpp:244]     Train net output #0: accuracy = 0.99132
I0625 15:24:56.734011 29461 solver.cpp:244]     Train net output #1: loss = 0.0203023 (* 1 = 0.0203023 loss)
I0625 15:24:56.734016 29461 sgd_solver.cpp:106] Iteration 2260, lr = 0.01
I0625 15:25:15.626462 29461 solver.cpp:228] Iteration 2280, loss = 0.0214039
I0625 15:25:15.626562 29461 solver.cpp:244]     Train net output #0: accuracy = 0.990872
I0625 15:25:15.626571 29461 solver.cpp:244]     Train net output #1: loss = 0.0214039 (* 1 = 0.0214039 loss)
I0625 15:25:15.626576 29461 sgd_solver.cpp:106] Iteration 2280, lr = 0.01
I0625 15:25:33.988976 29461 solver.cpp:337] Iteration 2300, Testing net (#0)
I0625 15:25:37.287274 29461 solver.cpp:404]     Test net output #0: accuracy = 0.985426
I0625 15:25:37.287298 29461 solver.cpp:404]     Test net output #1: loss = 0.0445802 (* 1 = 0.0445802 loss)
I0625 15:25:37.815033 29461 solver.cpp:228] Iteration 2300, loss = 0.0231973
I0625 15:25:37.815057 29461 solver.cpp:244]     Train net output #0: accuracy = 0.990541
I0625 15:25:37.815065 29461 solver.cpp:244]     Train net output #1: loss = 0.0231973 (* 1 = 0.0231973 loss)
I0625 15:25:37.815069 29461 sgd_solver.cpp:106] Iteration 2300, lr = 0.01
I0625 15:25:56.705456 29461 solver.cpp:228] Iteration 2320, loss = 0.0209399
I0625 15:25:56.705552 29461 solver.cpp:244]     Train net output #0: accuracy = 0.990964
I0625 15:25:56.705561 29461 solver.cpp:244]     Train net output #1: loss = 0.0209399 (* 1 = 0.0209399 loss)
I0625 15:25:56.705566 29461 sgd_solver.cpp:106] Iteration 2320, lr = 0.01
I0625 15:26:15.584434 29461 solver.cpp:228] Iteration 2340, loss = 0.0195706
I0625 15:26:15.584460 29461 solver.cpp:244]     Train net output #0: accuracy = 0.992135
I0625 15:26:15.584466 29461 solver.cpp:244]     Train net output #1: loss = 0.0195706 (* 1 = 0.0195706 loss)
I0625 15:26:15.584470 29461 sgd_solver.cpp:106] Iteration 2340, lr = 0.01
I0625 15:26:34.490340 29461 solver.cpp:228] Iteration 2360, loss = 0.022376
I0625 15:26:34.490469 29461 solver.cpp:244]     Train net output #0: accuracy = 0.991025
I0625 15:26:34.490479 29461 solver.cpp:244]     Train net output #1: loss = 0.022376 (* 1 = 0.022376 loss)
I0625 15:26:34.490485 29461 sgd_solver.cpp:106] Iteration 2360, lr = 0.01
I0625 15:26:53.380369 29461 solver.cpp:228] Iteration 2380, loss = 0.0193428
I0625 15:26:53.380401 29461 solver.cpp:244]     Train net output #0: accuracy = 0.991872
I0625 15:26:53.380410 29461 solver.cpp:244]     Train net output #1: loss = 0.0193428 (* 1 = 0.0193428 loss)
I0625 15:26:53.380415 29461 sgd_solver.cpp:106] Iteration 2380, lr = 0.01
I0625 15:27:11.749722 29461 solver.cpp:337] Iteration 2400, Testing net (#0)
I0625 15:27:15.052448 29461 solver.cpp:404]     Test net output #0: accuracy = 0.984816
I0625 15:27:15.052474 29461 solver.cpp:404]     Test net output #1: loss = 0.046277 (* 1 = 0.046277 loss)
I0625 15:27:15.582288 29461 solver.cpp:228] Iteration 2400, loss = 0.0194125
I0625 15:27:15.582324 29461 solver.cpp:244]     Train net output #0: accuracy = 0.991892
I0625 15:27:15.582331 29461 solver.cpp:244]     Train net output #1: loss = 0.0194125 (* 1 = 0.0194125 loss)
I0625 15:27:15.582336 29461 sgd_solver.cpp:106] Iteration 2400, lr = 0.01
I0625 15:27:34.469202 29461 solver.cpp:228] Iteration 2420, loss = 0.0177558
I0625 15:27:34.469226 29461 solver.cpp:244]     Train net output #0: accuracy = 0.992555
I0625 15:27:34.469233 29461 solver.cpp:244]     Train net output #1: loss = 0.0177558 (* 1 = 0.0177558 loss)
I0625 15:27:34.469238 29461 sgd_solver.cpp:106] Iteration 2420, lr = 0.01
I0625 15:27:53.356158 29461 solver.cpp:228] Iteration 2440, loss = 0.0196384
I0625 15:27:53.356252 29461 solver.cpp:244]     Train net output #0: accuracy = 0.991598
I0625 15:27:53.356262 29461 solver.cpp:244]     Train net output #1: loss = 0.0196384 (* 1 = 0.0196384 loss)
I0625 15:27:53.356267 29461 sgd_solver.cpp:106] Iteration 2440, lr = 0.01
I0625 15:28:12.247485 29461 solver.cpp:228] Iteration 2460, loss = 0.0220578
I0625 15:28:12.247520 29461 solver.cpp:244]     Train net output #0: accuracy = 0.990716
I0625 15:28:12.247529 29461 solver.cpp:244]     Train net output #1: loss = 0.0220578 (* 1 = 0.0220578 loss)
I0625 15:28:12.247534 29461 sgd_solver.cpp:106] Iteration 2460, lr = 0.01
I0625 15:28:31.163496 29461 solver.cpp:228] Iteration 2480, loss = 0.0213355
I0625 15:28:31.163583 29461 solver.cpp:244]     Train net output #0: accuracy = 0.990994
I0625 15:28:31.163591 29461 solver.cpp:244]     Train net output #1: loss = 0.0213355 (* 1 = 0.0213355 loss)
I0625 15:28:31.163596 29461 sgd_solver.cpp:106] Iteration 2480, lr = 0.01
I0625 15:28:49.549954 29461 solver.cpp:454] Snapshotting to binary proto file data/models/segnet_iter_2500.caffemodel
I0625 15:28:49.598630 29461 sgd_solver.cpp:273] Snapshotting solver state to binary proto file data/models/segnet_iter_2500.solverstate
I0625 15:28:49.621760 29461 solver.cpp:337] Iteration 2500, Testing net (#0)
I0625 15:28:52.924496 29461 solver.cpp:404]     Test net output #0: accuracy = 0.98519
I0625 15:28:52.924520 29461 solver.cpp:404]     Test net output #1: loss = 0.0449613 (* 1 = 0.0449613 loss)
I0625 15:28:53.451172 29461 solver.cpp:228] Iteration 2500, loss = 0.0186118
I0625 15:28:53.451196 29461 solver.cpp:244]     Train net output #0: accuracy = 0.99245
I0625 15:28:53.451203 29461 solver.cpp:244]     Train net output #1: loss = 0.0186118 (* 1 = 0.0186118 loss)
I0625 15:28:53.451207 29461 sgd_solver.cpp:106] Iteration 2500, lr = 0.01
I0625 15:29:12.328096 29461 solver.cpp:228] Iteration 2520, loss = 0.0193539
I0625 15:29:12.328227 29461 solver.cpp:244]     Train net output #0: accuracy = 0.991775
I0625 15:29:12.328236 29461 solver.cpp:244]     Train net output #1: loss = 0.0193539 (* 1 = 0.0193539 loss)
I0625 15:29:12.328241 29461 sgd_solver.cpp:106] Iteration 2520, lr = 0.01
I0625 15:29:31.212530 29461 solver.cpp:228] Iteration 2540, loss = 0.0186537
I0625 15:29:31.212555 29461 solver.cpp:244]     Train net output #0: accuracy = 0.99239
I0625 15:29:31.212563 29461 solver.cpp:244]     Train net output #1: loss = 0.0186537 (* 1 = 0.0186537 loss)
I0625 15:29:31.212568 29461 sgd_solver.cpp:106] Iteration 2540, lr = 0.01
I0625 15:29:50.086386 29461 solver.cpp:228] Iteration 2560, loss = 0.0189719
I0625 15:29:50.086485 29461 solver.cpp:244]     Train net output #0: accuracy = 0.992178
I0625 15:29:50.086494 29461 solver.cpp:244]     Train net output #1: loss = 0.0189719 (* 1 = 0.0189719 loss)
I0625 15:29:50.086499 29461 sgd_solver.cpp:106] Iteration 2560, lr = 0.01
I0625 15:30:08.977533 29461 solver.cpp:228] Iteration 2580, loss = 0.0192674
I0625 15:30:08.977558 29461 solver.cpp:244]     Train net output #0: accuracy = 0.991899
I0625 15:30:08.977566 29461 solver.cpp:244]     Train net output #1: loss = 0.0192674 (* 1 = 0.0192674 loss)
I0625 15:30:08.977571 29461 sgd_solver.cpp:106] Iteration 2580, lr = 0.01
I0625 15:30:27.339629 29461 solver.cpp:337] Iteration 2600, Testing net (#0)
I0625 15:30:30.647528 29461 solver.cpp:404]     Test net output #0: accuracy = 0.985265
I0625 15:30:30.647552 29461 solver.cpp:404]     Test net output #1: loss = 0.0488038 (* 1 = 0.0488038 loss)
I0625 15:30:31.176930 29461 solver.cpp:228] Iteration 2600, loss = 0.0203842
I0625 15:30:31.176954 29461 solver.cpp:244]     Train net output #0: accuracy = 0.991725
I0625 15:30:31.176960 29461 solver.cpp:244]     Train net output #1: loss = 0.0203842 (* 1 = 0.0203842 loss)
I0625 15:30:31.176965 29461 sgd_solver.cpp:106] Iteration 2600, lr = 0.01
I0625 15:30:50.060119 29461 solver.cpp:228] Iteration 2620, loss = 0.0231849
I0625 15:30:50.060144 29461 solver.cpp:244]     Train net output #0: accuracy = 0.990896
I0625 15:30:50.060151 29461 solver.cpp:244]     Train net output #1: loss = 0.0231849 (* 1 = 0.0231849 loss)
I0625 15:30:50.060155 29461 sgd_solver.cpp:106] Iteration 2620, lr = 0.01
I0625 15:31:08.939721 29461 solver.cpp:228] Iteration 2640, loss = 0.01874
I0625 15:31:08.939821 29461 solver.cpp:244]     Train net output #0: accuracy = 0.99217
I0625 15:31:08.939829 29461 solver.cpp:244]     Train net output #1: loss = 0.01874 (* 1 = 0.01874 loss)
I0625 15:31:08.939833 29461 sgd_solver.cpp:106] Iteration 2640, lr = 0.01
I0625 15:31:27.824170 29461 solver.cpp:228] Iteration 2660, loss = 0.0217709
I0625 15:31:27.824205 29461 solver.cpp:244]     Train net output #0: accuracy = 0.991087
I0625 15:31:27.824213 29461 solver.cpp:244]     Train net output #1: loss = 0.0217709 (* 1 = 0.0217709 loss)
I0625 15:31:27.824218 29461 sgd_solver.cpp:106] Iteration 2660, lr = 0.01
I0625 15:31:46.708405 29461 solver.cpp:228] Iteration 2680, loss = 0.0208819
I0625 15:31:46.708514 29461 solver.cpp:244]     Train net output #0: accuracy = 0.991116
I0625 15:31:46.708523 29461 solver.cpp:244]     Train net output #1: loss = 0.0208819 (* 1 = 0.0208819 loss)
I0625 15:31:46.708528 29461 sgd_solver.cpp:106] Iteration 2680, lr = 0.01
I0625 15:32:05.059658 29461 solver.cpp:337] Iteration 2700, Testing net (#0)
I0625 15:32:08.362630 29461 solver.cpp:404]     Test net output #0: accuracy = 0.984465
I0625 15:32:08.362663 29461 solver.cpp:404]     Test net output #1: loss = 0.0474322 (* 1 = 0.0474322 loss)
I0625 15:32:08.892712 29461 solver.cpp:228] Iteration 2700, loss = 0.0193326
I0625 15:32:08.892736 29461 solver.cpp:244]     Train net output #0: accuracy = 0.991603
I0625 15:32:08.892742 29461 solver.cpp:244]     Train net output #1: loss = 0.0193326 (* 1 = 0.0193326 loss)
I0625 15:32:08.892747 29461 sgd_solver.cpp:106] Iteration 2700, lr = 0.01
I0625 15:32:27.793143 29461 solver.cpp:228] Iteration 2720, loss = 0.0209795
I0625 15:32:27.793269 29461 solver.cpp:244]     Train net output #0: accuracy = 0.9912
I0625 15:32:27.793279 29461 solver.cpp:244]     Train net output #1: loss = 0.0209795 (* 1 = 0.0209795 loss)
I0625 15:32:27.793284 29461 sgd_solver.cpp:106] Iteration 2720, lr = 0.01
I0625 15:32:46.693251 29461 solver.cpp:228] Iteration 2740, loss = 0.0195151
I0625 15:32:46.693282 29461 solver.cpp:244]     Train net output #0: accuracy = 0.992028
I0625 15:32:46.693290 29461 solver.cpp:244]     Train net output #1: loss = 0.0195151 (* 1 = 0.0195151 loss)
I0625 15:32:46.693295 29461 sgd_solver.cpp:106] Iteration 2740, lr = 0.01
I0625 15:33:05.581967 29461 solver.cpp:228] Iteration 2760, loss = 0.018171
I0625 15:33:05.582082 29461 solver.cpp:244]     Train net output #0: accuracy = 0.992333
I0625 15:33:05.582092 29461 solver.cpp:244]     Train net output #1: loss = 0.018171 (* 1 = 0.018171 loss)
I0625 15:33:05.582098 29461 sgd_solver.cpp:106] Iteration 2760, lr = 0.01
I0625 15:33:24.460542 29461 solver.cpp:228] Iteration 2780, loss = 0.0171068
I0625 15:33:24.460568 29461 solver.cpp:244]     Train net output #0: accuracy = 0.992801
I0625 15:33:24.460575 29461 solver.cpp:244]     Train net output #1: loss = 0.0171068 (* 1 = 0.0171068 loss)
I0625 15:33:24.460580 29461 sgd_solver.cpp:106] Iteration 2780, lr = 0.01
I0625 15:33:42.829013 29461 solver.cpp:337] Iteration 2800, Testing net (#0)
I0625 15:33:46.120354 29461 solver.cpp:404]     Test net output #0: accuracy = 0.9859
I0625 15:33:46.120388 29461 solver.cpp:404]     Test net output #1: loss = 0.0425075 (* 1 = 0.0425075 loss)
I0625 15:33:46.645908 29461 solver.cpp:228] Iteration 2800, loss = 0.0182202
I0625 15:33:46.645932 29461 solver.cpp:244]     Train net output #0: accuracy = 0.992349
I0625 15:33:46.645939 29461 solver.cpp:244]     Train net output #1: loss = 0.0182202 (* 1 = 0.0182202 loss)
I0625 15:33:46.645944 29461 sgd_solver.cpp:106] Iteration 2800, lr = 0.01
I0625 15:34:05.541230 29461 solver.cpp:228] Iteration 2820, loss = 0.0192541
I0625 15:34:05.541255 29461 solver.cpp:244]     Train net output #0: accuracy = 0.992006
I0625 15:34:05.541261 29461 solver.cpp:244]     Train net output #1: loss = 0.0192541 (* 1 = 0.0192541 loss)
I0625 15:34:05.541266 29461 sgd_solver.cpp:106] Iteration 2820, lr = 0.01
I0625 15:34:24.434352 29461 solver.cpp:228] Iteration 2840, loss = 0.0175527
I0625 15:34:24.434447 29461 solver.cpp:244]     Train net output #0: accuracy = 0.992857
I0625 15:34:24.434456 29461 solver.cpp:244]     Train net output #1: loss = 0.0175527 (* 1 = 0.0175527 loss)
I0625 15:34:24.434460 29461 sgd_solver.cpp:106] Iteration 2840, lr = 0.01
I0625 15:34:43.367610 29461 solver.cpp:228] Iteration 2860, loss = 0.0182139
I0625 15:34:43.367635 29461 solver.cpp:244]     Train net output #0: accuracy = 0.992224
I0625 15:34:43.367641 29461 solver.cpp:244]     Train net output #1: loss = 0.0182139 (* 1 = 0.0182139 loss)
I0625 15:34:43.367646 29461 sgd_solver.cpp:106] Iteration 2860, lr = 0.01
I0625 15:35:02.253213 29461 solver.cpp:228] Iteration 2880, loss = 0.0212622
I0625 15:35:02.253310 29461 solver.cpp:244]     Train net output #0: accuracy = 0.990903
I0625 15:35:02.253320 29461 solver.cpp:244]     Train net output #1: loss = 0.0212622 (* 1 = 0.0212622 loss)
I0625 15:35:02.253325 29461 sgd_solver.cpp:106] Iteration 2880, lr = 0.01
I0625 15:35:20.619132 29461 solver.cpp:337] Iteration 2900, Testing net (#0)
I0625 15:35:23.921211 29461 solver.cpp:404]     Test net output #0: accuracy = 0.983674
I0625 15:35:23.921247 29461 solver.cpp:404]     Test net output #1: loss = 0.055256 (* 1 = 0.055256 loss)
I0625 15:35:24.451557 29461 solver.cpp:228] Iteration 2900, loss = 0.0174498
I0625 15:35:24.451581 29461 solver.cpp:244]     Train net output #0: accuracy = 0.992814
I0625 15:35:24.451589 29461 solver.cpp:244]     Train net output #1: loss = 0.0174498 (* 1 = 0.0174498 loss)
I0625 15:35:24.451593 29461 sgd_solver.cpp:106] Iteration 2900, lr = 0.01
I0625 15:35:43.433425 29461 solver.cpp:228] Iteration 2920, loss = 0.020734
I0625 15:35:43.433517 29461 solver.cpp:244]     Train net output #0: accuracy = 0.99157
I0625 15:35:43.433526 29461 solver.cpp:244]     Train net output #1: loss = 0.020734 (* 1 = 0.020734 loss)
I0625 15:35:43.433532 29461 sgd_solver.cpp:106] Iteration 2920, lr = 0.01
I0625 15:36:02.390924 29461 solver.cpp:228] Iteration 2940, loss = 0.0198621
I0625 15:36:02.390952 29461 solver.cpp:244]     Train net output #0: accuracy = 0.991917
I0625 15:36:02.390960 29461 solver.cpp:244]     Train net output #1: loss = 0.0198621 (* 1 = 0.0198621 loss)
I0625 15:36:02.390965 29461 sgd_solver.cpp:106] Iteration 2940, lr = 0.01
I0625 15:36:21.333827 29461 solver.cpp:228] Iteration 2960, loss = 0.0181555
I0625 15:36:21.333947 29461 solver.cpp:244]     Train net output #0: accuracy = 0.992415
I0625 15:36:21.333957 29461 solver.cpp:244]     Train net output #1: loss = 0.0181555 (* 1 = 0.0181555 loss)
I0625 15:36:21.333963 29461 sgd_solver.cpp:106] Iteration 2960, lr = 0.01
I0625 15:36:40.270699 29461 solver.cpp:228] Iteration 2980, loss = 0.0198668
I0625 15:36:40.270725 29461 solver.cpp:244]     Train net output #0: accuracy = 0.991769
I0625 15:36:40.270733 29461 solver.cpp:244]     Train net output #1: loss = 0.0198668 (* 1 = 0.0198668 loss)
I0625 15:36:40.270737 29461 sgd_solver.cpp:106] Iteration 2980, lr = 0.01
I0625 15:36:58.642210 29461 solver.cpp:454] Snapshotting to binary proto file data/models/segnet_iter_3000.caffemodel
I0625 15:36:58.689968 29461 sgd_solver.cpp:273] Snapshotting solver state to binary proto file data/models/segnet_iter_3000.solverstate
I0625 15:36:58.713071 29461 solver.cpp:337] Iteration 3000, Testing net (#0)
I0625 15:37:02.009295 29461 solver.cpp:404]     Test net output #0: accuracy = 0.986595
I0625 15:37:02.009330 29461 solver.cpp:404]     Test net output #1: loss = 0.0400389 (* 1 = 0.0400389 loss)
I0625 15:37:02.536336 29461 solver.cpp:228] Iteration 3000, loss = 0.0228567
I0625 15:37:02.536360 29461 solver.cpp:244]     Train net output #0: accuracy = 0.990459
I0625 15:37:02.536366 29461 solver.cpp:244]     Train net output #1: loss = 0.0228567 (* 1 = 0.0228567 loss)
I0625 15:37:02.536371 29461 sgd_solver.cpp:106] Iteration 3000, lr = 0.01
I0625 15:37:21.410784 29461 solver.cpp:228] Iteration 3020, loss = 0.0170361
I0625 15:37:21.410807 29461 solver.cpp:244]     Train net output #0: accuracy = 0.992653
I0625 15:37:21.410815 29461 solver.cpp:244]     Train net output #1: loss = 0.0170361 (* 1 = 0.0170361 loss)
I0625 15:37:21.410818 29461 sgd_solver.cpp:106] Iteration 3020, lr = 0.01
I0625 15:37:40.296097 29461 solver.cpp:228] Iteration 3040, loss = 0.0175641
I0625 15:37:40.296200 29461 solver.cpp:244]     Train net output #0: accuracy = 0.992586
I0625 15:37:40.296208 29461 solver.cpp:244]     Train net output #1: loss = 0.0175641 (* 1 = 0.0175641 loss)
I0625 15:37:40.296213 29461 sgd_solver.cpp:106] Iteration 3040, lr = 0.01
I0625 15:37:59.212587 29461 solver.cpp:228] Iteration 3060, loss = 0.0185981
I0625 15:37:59.212611 29461 solver.cpp:244]     Train net output #0: accuracy = 0.992412
I0625 15:37:59.212618 29461 solver.cpp:244]     Train net output #1: loss = 0.0185981 (* 1 = 0.0185981 loss)
I0625 15:37:59.212622 29461 sgd_solver.cpp:106] Iteration 3060, lr = 0.01
I0625 15:38:18.089638 29461 solver.cpp:228] Iteration 3080, loss = 0.0207287
I0625 15:38:18.089738 29461 solver.cpp:244]     Train net output #0: accuracy = 0.99122
I0625 15:38:18.089747 29461 solver.cpp:244]     Train net output #1: loss = 0.0207287 (* 1 = 0.0207287 loss)
I0625 15:38:18.089752 29461 sgd_solver.cpp:106] Iteration 3080, lr = 0.01
I0625 15:38:36.458575 29461 solver.cpp:337] Iteration 3100, Testing net (#0)
I0625 15:38:39.756307 29461 solver.cpp:404]     Test net output #0: accuracy = 0.984284
I0625 15:38:39.756331 29461 solver.cpp:404]     Test net output #1: loss = 0.0485762 (* 1 = 0.0485762 loss)
I0625 15:38:40.281379 29461 solver.cpp:228] Iteration 3100, loss = 0.0185569
I0625 15:38:40.281404 29461 solver.cpp:244]     Train net output #0: accuracy = 0.992267
I0625 15:38:40.281410 29461 solver.cpp:244]     Train net output #1: loss = 0.0185569 (* 1 = 0.0185569 loss)
I0625 15:38:40.281415 29461 sgd_solver.cpp:106] Iteration 3100, lr = 0.01
I0625 15:38:59.167327 29461 solver.cpp:228] Iteration 3120, loss = 0.0212349
I0625 15:38:59.167448 29461 solver.cpp:244]     Train net output #0: accuracy = 0.991622
I0625 15:38:59.167459 29461 solver.cpp:244]     Train net output #1: loss = 0.0212349 (* 1 = 0.0212349 loss)
I0625 15:38:59.167464 29461 sgd_solver.cpp:106] Iteration 3120, lr = 0.01
I0625 15:39:18.048689 29461 solver.cpp:228] Iteration 3140, loss = 0.0176587
I0625 15:39:18.048714 29461 solver.cpp:244]     Train net output #0: accuracy = 0.992715
I0625 15:39:18.048722 29461 solver.cpp:244]     Train net output #1: loss = 0.0176587 (* 1 = 0.0176587 loss)
I0625 15:39:18.048727 29461 sgd_solver.cpp:106] Iteration 3140, lr = 0.01
I0625 15:39:36.942895 29461 solver.cpp:228] Iteration 3160, loss = 0.017582
I0625 15:39:36.943660 29461 solver.cpp:244]     Train net output #0: accuracy = 0.992783
I0625 15:39:36.943670 29461 solver.cpp:244]     Train net output #1: loss = 0.017582 (* 1 = 0.017582 loss)
I0625 15:39:36.943675 29461 sgd_solver.cpp:106] Iteration 3160, lr = 0.01
I0625 15:39:55.836781 29461 solver.cpp:228] Iteration 3180, loss = 0.0149925
I0625 15:39:55.836804 29461 solver.cpp:244]     Train net output #0: accuracy = 0.993713
I0625 15:39:55.836812 29461 solver.cpp:244]     Train net output #1: loss = 0.0149925 (* 1 = 0.0149925 loss)
I0625 15:39:55.836817 29461 sgd_solver.cpp:106] Iteration 3180, lr = 0.01
I0625 15:40:14.203763 29461 solver.cpp:337] Iteration 3200, Testing net (#0)
I0625 15:40:17.500609 29461 solver.cpp:404]     Test net output #0: accuracy = 0.984663
I0625 15:40:17.500643 29461 solver.cpp:404]     Test net output #1: loss = 0.0505993 (* 1 = 0.0505993 loss)
I0625 15:40:18.028229 29461 solver.cpp:228] Iteration 3200, loss = 0.0161083
I0625 15:40:18.028252 29461 solver.cpp:244]     Train net output #0: accuracy = 0.993389
I0625 15:40:18.028259 29461 solver.cpp:244]     Train net output #1: loss = 0.0161083 (* 1 = 0.0161083 loss)
I0625 15:40:18.028264 29461 sgd_solver.cpp:106] Iteration 3200, lr = 0.01
I0625 15:40:36.946660 29461 solver.cpp:228] Iteration 3220, loss = 0.0159227
I0625 15:40:36.946686 29461 solver.cpp:244]     Train net output #0: accuracy = 0.993451
I0625 15:40:36.946692 29461 solver.cpp:244]     Train net output #1: loss = 0.0159227 (* 1 = 0.0159227 loss)
I0625 15:40:36.946696 29461 sgd_solver.cpp:106] Iteration 3220, lr = 0.01
I0625 15:40:55.850322 29461 solver.cpp:228] Iteration 3240, loss = 0.0176745
I0625 15:40:55.850416 29461 solver.cpp:244]     Train net output #0: accuracy = 0.99239
I0625 15:40:55.850425 29461 solver.cpp:244]     Train net output #1: loss = 0.0176745 (* 1 = 0.0176745 loss)
I0625 15:40:55.850430 29461 sgd_solver.cpp:106] Iteration 3240, lr = 0.01
I0625 15:41:14.734454 29461 solver.cpp:228] Iteration 3260, loss = 0.0171293
I0625 15:41:14.734477 29461 solver.cpp:244]     Train net output #0: accuracy = 0.992624
I0625 15:41:14.734483 29461 solver.cpp:244]     Train net output #1: loss = 0.0171293 (* 1 = 0.0171293 loss)
I0625 15:41:14.734488 29461 sgd_solver.cpp:106] Iteration 3260, lr = 0.01
I0625 15:41:33.626500 29461 solver.cpp:228] Iteration 3280, loss = 0.0180181
I0625 15:41:33.626598 29461 solver.cpp:244]     Train net output #0: accuracy = 0.992244
I0625 15:41:33.626606 29461 solver.cpp:244]     Train net output #1: loss = 0.0180181 (* 1 = 0.0180181 loss)
I0625 15:41:33.626611 29461 sgd_solver.cpp:106] Iteration 3280, lr = 0.01
I0625 15:41:51.988179 29461 solver.cpp:337] Iteration 3300, Testing net (#0)
I0625 15:41:55.283277 29461 solver.cpp:404]     Test net output #0: accuracy = 0.98606
I0625 15:41:55.283298 29461 solver.cpp:404]     Test net output #1: loss = 0.0402648 (* 1 = 0.0402648 loss)
I0625 15:41:55.812558 29461 solver.cpp:228] Iteration 3300, loss = 0.0162889
I0625 15:41:55.812582 29461 solver.cpp:244]     Train net output #0: accuracy = 0.993057
I0625 15:41:55.812602 29461 solver.cpp:244]     Train net output #1: loss = 0.0162889 (* 1 = 0.0162889 loss)
I0625 15:41:55.812607 29461 sgd_solver.cpp:106] Iteration 3300, lr = 0.01
I0625 15:42:14.761925 29461 solver.cpp:228] Iteration 3320, loss = 0.0183872
I0625 15:42:14.762024 29461 solver.cpp:244]     Train net output #0: accuracy = 0.992289
I0625 15:42:14.762034 29461 solver.cpp:244]     Train net output #1: loss = 0.0183872 (* 1 = 0.0183872 loss)
I0625 15:42:14.762039 29461 sgd_solver.cpp:106] Iteration 3320, lr = 0.01
I0625 15:42:33.709167 29461 solver.cpp:228] Iteration 3340, loss = 0.0182123
I0625 15:42:33.709206 29461 solver.cpp:244]     Train net output #0: accuracy = 0.992428
I0625 15:42:33.709214 29461 solver.cpp:244]     Train net output #1: loss = 0.0182123 (* 1 = 0.0182123 loss)
I0625 15:42:33.709220 29461 sgd_solver.cpp:106] Iteration 3340, lr = 0.01
I0625 15:42:52.658048 29461 solver.cpp:228] Iteration 3360, loss = 0.0183397
I0625 15:42:52.658149 29461 solver.cpp:244]     Train net output #0: accuracy = 0.992548
I0625 15:42:52.658159 29461 solver.cpp:244]     Train net output #1: loss = 0.0183397 (* 1 = 0.0183397 loss)
I0625 15:42:52.658164 29461 sgd_solver.cpp:106] Iteration 3360, lr = 0.01
I0625 15:43:11.593565 29461 solver.cpp:228] Iteration 3380, loss = 0.0175283
I0625 15:43:11.593601 29461 solver.cpp:244]     Train net output #0: accuracy = 0.992837
I0625 15:43:11.593610 29461 solver.cpp:244]     Train net output #1: loss = 0.0175283 (* 1 = 0.0175283 loss)
I0625 15:43:11.593616 29461 sgd_solver.cpp:106] Iteration 3380, lr = 0.01
I0625 15:43:29.954993 29461 solver.cpp:337] Iteration 3400, Testing net (#0)
I0625 15:43:33.257129 29461 solver.cpp:404]     Test net output #0: accuracy = 0.984622
I0625 15:43:33.257164 29461 solver.cpp:404]     Test net output #1: loss = 0.0529194 (* 1 = 0.0529194 loss)
I0625 15:43:33.783527 29461 solver.cpp:228] Iteration 3400, loss = 0.0177552
I0625 15:43:33.783552 29461 solver.cpp:244]     Train net output #0: accuracy = 0.992524
I0625 15:43:33.783560 29461 solver.cpp:244]     Train net output #1: loss = 0.0177552 (* 1 = 0.0177552 loss)
I0625 15:43:33.783563 29461 sgd_solver.cpp:106] Iteration 3400, lr = 0.01
I0625 15:43:52.658107 29461 solver.cpp:228] Iteration 3420, loss = 0.0180655
I0625 15:43:52.658130 29461 solver.cpp:244]     Train net output #0: accuracy = 0.992441
I0625 15:43:52.658138 29461 solver.cpp:244]     Train net output #1: loss = 0.0180655 (* 1 = 0.0180655 loss)
I0625 15:43:52.658141 29461 sgd_solver.cpp:106] Iteration 3420, lr = 0.01
I0625 15:44:11.528357 29461 solver.cpp:228] Iteration 3440, loss = 0.0179516
I0625 15:44:11.528451 29461 solver.cpp:244]     Train net output #0: accuracy = 0.992345
I0625 15:44:11.528460 29461 solver.cpp:244]     Train net output #1: loss = 0.0179516 (* 1 = 0.0179516 loss)
I0625 15:44:11.528465 29461 sgd_solver.cpp:106] Iteration 3440, lr = 0.01
I0625 15:44:30.438791 29461 solver.cpp:228] Iteration 3460, loss = 0.0170552
I0625 15:44:30.438824 29461 solver.cpp:244]     Train net output #0: accuracy = 0.993099
I0625 15:44:30.438832 29461 solver.cpp:244]     Train net output #1: loss = 0.0170552 (* 1 = 0.0170552 loss)
I0625 15:44:30.438835 29461 sgd_solver.cpp:106] Iteration 3460, lr = 0.01
