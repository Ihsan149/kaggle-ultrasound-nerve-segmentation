I0525 12:57:03.494832 25608 caffe.cpp:185] Using GPUs 0
I0525 12:57:03.500799 25608 caffe.cpp:190] GPU 0: GeForce GTX TITAN X
I0525 12:57:03.787153 25608 solver.cpp:48] Initializing solver from parameters: 
test_iter: 16
test_interval: 100
base_lr: 0.01
display: 20
max_iter: 10000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 3000
snapshot: 500
snapshot_prefix: "data/models/segnet"
solver_mode: GPU
device_id: 0
net: "models/segnet/train_val.prototxt"
test_initialization: true
I0525 12:57:03.787277 25608 solver.cpp:91] Creating training net from net file: models/segnet/train_val.prototxt
I0525 12:57:03.788280 25608 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0525 12:57:03.788542 25608 net.cpp:49] Initializing net from parameters: 
name: "segnet"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "DenseImageData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    crop_size: 224
    mean_value: 100
  }
  dense_image_data_param {
    source: "data/train.txt"
    batch_size: 64
    shuffle: true
    new_height: 224
    new_width: 310
    is_color: false
    root_folder: "data/raw/train/"
  }
}
layer {
  name: "conv1_1"
  type: "Convolution"
  bottom: "data"
  top: "conv1_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn1_1"
  type: "BatchNorm"
  bottom: "conv1_1"
  top: "conv1_1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale1_1"
  type: "Scale"
  bottom: "conv1_1"
  top: "conv1_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1_1"
  type: "ReLU"
  bottom: "conv1_1"
  top: "conv1_1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1_1"
  top: "pool1"
  top: "pool1_mask"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2_1"
  top: "pool2"
  top: "pool2_mask"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn3_1"
  type: "BatchNorm"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale3_1"
  type: "Scale"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3_1"
  top: "pool3"
  top: "pool3_mask"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4_1"
  top: "pool4"
  top: "pool4_mask"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv5_1"
  type: "Convolution"
  bottom: "pool4"
  top: "conv5_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn5_1"
  type: "BatchNorm"
  bottom: "conv5_1"
  top: "conv5_1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale5_1"
  type: "Scale"
  bottom: "conv5_1"
  top: "conv5_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5_1"
  type: "ReLU"
  bottom: "conv5_1"
  top: "conv5_1"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5_1"
  top: "pool5"
  top: "pool5_mask"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "upsample5"
  type: "Upsample"
  bottom: "pool5"
  bottom: "pool5_mask"
  top: "pool5_D"
  upsample_param {
    scale: 2
    upsample_h: 14
    upsample_w: 14
  }
}
layer {
  name: "conv5_1_D"
  type: "Convolution"
  bottom: "pool5_D"
  top: "conv5_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn5_1_D"
  type: "BatchNorm"
  bottom: "conv5_1_D"
  top: "conv5_1_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale5_1_D"
  type: "Scale"
  bottom: "conv5_1_D"
  top: "conv5_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5_1_D"
  type: "ReLU"
  bottom: "conv5_1_D"
  top: "conv5_1_D"
}
layer {
  name: "upsample4"
  type: "Upsample"
  bottom: "conv5_1_D"
  bottom: "pool4_mask"
  top: "pool4_D"
  upsample_param {
    scale: 2
    upsample_h: 28
    upsample_w: 28
  }
}
layer {
  name: "conv4_1_D"
  type: "Convolution"
  bottom: "pool4_D"
  top: "conv4_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn4_1_D"
  type: "BatchNorm"
  bottom: "conv4_1_D"
  top: "conv4_1_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale4_1_D"
  type: "Scale"
  bottom: "conv4_1_D"
  top: "conv4_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4_1_D"
  type: "ReLU"
  bottom: "conv4_1_D"
  top: "conv4_1_D"
}
layer {
  name: "upsample3"
  type: "Upsample"
  bottom: "conv4_1_D"
  bottom: "pool3_mask"
  top: "pool3_D"
  upsample_param {
    scale: 2
    upsample_h: 56
    upsample_w: 56
  }
}
layer {
  name: "conv3_1_D"
  type: "Convolution"
  bottom: "pool3_D"
  top: "conv3_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn3_1_D"
  type: "BatchNorm"
  bottom: "conv3_1_D"
  top: "conv3_1_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale3_1_D"
  type: "Scale"
  bottom: "conv3_1_D"
  top: "conv3_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_1_D"
  type: "ReLU"
  bottom: "conv3_1_D"
  top: "conv3_1_D"
}
layer {
  name: "upsample2"
  type: "Upsample"
  bottom: "conv3_1_D"
  bottom: "pool2_mask"
  top: "pool2_D"
  upsample_param {
    scale: 2
    upsample_h: 112
    upsample_w: 112
  }
}
layer {
  name: "conv2_1_D"
  type: "Convolution"
  bottom: "pool2_D"
  top: "conv2_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn2_1_D"
  type: "BatchNorm"
  bottom: "conv2_1_D"
  top: "conv2_1_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale2_1_D"
  type: "Scale"
  bottom: "conv2_1_D"
  top: "conv2_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2_1_D"
  type: "ReLU"
  bottom: "conv2_1_D"
  top: "conv2_1_D"
}
layer {
  name: "upsample1"
  type: "Upsample"
  bottom: "conv2_1_D"
  bottom: "pool1_mask"
  top: "pool1_D"
  upsample_param {
    scale: 2
    upsample_h: 224
    upsample_w: 224
  }
}
layer {
  name: "conv1_1_D"
  type: "Convolution"
  bottom: "pool1_D"
  top: "conv1_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 2
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "conv1_1_D"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "conv1_1_D"
  bottom: "label"
  top: "accuracy"
}
I0525 12:57:03.788750 25608 layer_factory.hpp:77] Creating layer data
I0525 12:57:03.788784 25608 net.cpp:91] Creating Layer data
I0525 12:57:03.788791 25608 net.cpp:399] data -> data
I0525 12:57:03.788817 25608 net.cpp:399] data -> label
I0525 12:57:03.789172 25608 dense_image_data_layer.cpp:38] Opening file data/train.txt
I0525 12:57:03.791726 25608 dense_image_data_layer.cpp:48] Shuffling data
I0525 12:57:03.792259 25608 dense_image_data_layer.cpp:53] A total of 4930 examples.
I0525 12:57:03.962079 25608 dense_image_data_layer.cpp:92] output data size: 1,1,224,224
I0525 12:57:03.963289 25608 net.cpp:141] Setting up data
I0525 12:57:03.963307 25608 net.cpp:148] Top shape: 1 1 224 224 (50176)
I0525 12:57:03.963311 25608 net.cpp:148] Top shape: 1 1 224 224 (50176)
I0525 12:57:03.963315 25608 net.cpp:156] Memory required for data: 401408
I0525 12:57:03.963321 25608 layer_factory.hpp:77] Creating layer label_data_1_split
I0525 12:57:03.963352 25608 net.cpp:91] Creating Layer label_data_1_split
I0525 12:57:03.963371 25608 net.cpp:425] label_data_1_split <- label
I0525 12:57:03.963382 25608 net.cpp:399] label_data_1_split -> label_data_1_split_0
I0525 12:57:03.963390 25608 net.cpp:399] label_data_1_split -> label_data_1_split_1
I0525 12:57:03.963459 25608 net.cpp:141] Setting up label_data_1_split
I0525 12:57:03.963465 25608 net.cpp:148] Top shape: 1 1 224 224 (50176)
I0525 12:57:03.963469 25608 net.cpp:148] Top shape: 1 1 224 224 (50176)
I0525 12:57:03.963470 25608 net.cpp:156] Memory required for data: 802816
I0525 12:57:03.963474 25608 layer_factory.hpp:77] Creating layer conv1_1
I0525 12:57:03.963487 25608 net.cpp:91] Creating Layer conv1_1
I0525 12:57:03.963490 25608 net.cpp:425] conv1_1 <- data
I0525 12:57:03.963495 25608 net.cpp:399] conv1_1 -> conv1_1
I0525 12:57:04.123031 25608 net.cpp:141] Setting up conv1_1
I0525 12:57:04.123057 25608 net.cpp:148] Top shape: 1 32 224 224 (1605632)
I0525 12:57:04.123060 25608 net.cpp:156] Memory required for data: 7225344
I0525 12:57:04.123072 25608 layer_factory.hpp:77] Creating layer bn1_1
I0525 12:57:04.123085 25608 net.cpp:91] Creating Layer bn1_1
I0525 12:57:04.123090 25608 net.cpp:425] bn1_1 <- conv1_1
I0525 12:57:04.123095 25608 net.cpp:386] bn1_1 -> conv1_1 (in-place)
I0525 12:57:04.123286 25608 net.cpp:141] Setting up bn1_1
I0525 12:57:04.123293 25608 net.cpp:148] Top shape: 1 32 224 224 (1605632)
I0525 12:57:04.123296 25608 net.cpp:156] Memory required for data: 13647872
I0525 12:57:04.123304 25608 layer_factory.hpp:77] Creating layer scale1_1
I0525 12:57:04.123314 25608 net.cpp:91] Creating Layer scale1_1
I0525 12:57:04.123317 25608 net.cpp:425] scale1_1 <- conv1_1
I0525 12:57:04.123320 25608 net.cpp:386] scale1_1 -> conv1_1 (in-place)
I0525 12:57:04.123364 25608 layer_factory.hpp:77] Creating layer scale1_1
I0525 12:57:04.123536 25608 net.cpp:141] Setting up scale1_1
I0525 12:57:04.123544 25608 net.cpp:148] Top shape: 1 32 224 224 (1605632)
I0525 12:57:04.123548 25608 net.cpp:156] Memory required for data: 20070400
I0525 12:57:04.123553 25608 layer_factory.hpp:77] Creating layer relu1_1
I0525 12:57:04.123559 25608 net.cpp:91] Creating Layer relu1_1
I0525 12:57:04.123563 25608 net.cpp:425] relu1_1 <- conv1_1
I0525 12:57:04.123566 25608 net.cpp:386] relu1_1 -> conv1_1 (in-place)
I0525 12:57:04.123817 25608 net.cpp:141] Setting up relu1_1
I0525 12:57:04.123828 25608 net.cpp:148] Top shape: 1 32 224 224 (1605632)
I0525 12:57:04.123831 25608 net.cpp:156] Memory required for data: 26492928
I0525 12:57:04.123834 25608 layer_factory.hpp:77] Creating layer pool1
I0525 12:57:04.123838 25608 layer_factory.cpp:90] cuDNN does not support multiple tops. Using Caffe's own pooling layer.
I0525 12:57:04.123843 25608 net.cpp:91] Creating Layer pool1
I0525 12:57:04.123847 25608 net.cpp:425] pool1 <- conv1_1
I0525 12:57:04.123850 25608 net.cpp:399] pool1 -> pool1
I0525 12:57:04.123857 25608 net.cpp:399] pool1 -> pool1_mask
I0525 12:57:04.123899 25608 net.cpp:141] Setting up pool1
I0525 12:57:04.123908 25608 net.cpp:148] Top shape: 1 32 112 112 (401408)
I0525 12:57:04.123910 25608 net.cpp:148] Top shape: 1 32 112 112 (401408)
I0525 12:57:04.123913 25608 net.cpp:156] Memory required for data: 29704192
I0525 12:57:04.123914 25608 layer_factory.hpp:77] Creating layer conv2_1
I0525 12:57:04.123924 25608 net.cpp:91] Creating Layer conv2_1
I0525 12:57:04.123925 25608 net.cpp:425] conv2_1 <- pool1
I0525 12:57:04.123930 25608 net.cpp:399] conv2_1 -> conv2_1
I0525 12:57:04.125202 25608 net.cpp:141] Setting up conv2_1
I0525 12:57:04.125216 25608 net.cpp:148] Top shape: 1 64 112 112 (802816)
I0525 12:57:04.125218 25608 net.cpp:156] Memory required for data: 32915456
I0525 12:57:04.125222 25608 layer_factory.hpp:77] Creating layer bn2_1
I0525 12:57:04.125229 25608 net.cpp:91] Creating Layer bn2_1
I0525 12:57:04.125231 25608 net.cpp:425] bn2_1 <- conv2_1
I0525 12:57:04.125237 25608 net.cpp:386] bn2_1 -> conv2_1 (in-place)
I0525 12:57:04.125726 25608 net.cpp:141] Setting up bn2_1
I0525 12:57:04.125737 25608 net.cpp:148] Top shape: 1 64 112 112 (802816)
I0525 12:57:04.125740 25608 net.cpp:156] Memory required for data: 36126720
I0525 12:57:04.125762 25608 layer_factory.hpp:77] Creating layer scale2_1
I0525 12:57:04.125771 25608 net.cpp:91] Creating Layer scale2_1
I0525 12:57:04.125773 25608 net.cpp:425] scale2_1 <- conv2_1
I0525 12:57:04.125777 25608 net.cpp:386] scale2_1 -> conv2_1 (in-place)
I0525 12:57:04.125811 25608 layer_factory.hpp:77] Creating layer scale2_1
I0525 12:57:04.125906 25608 net.cpp:141] Setting up scale2_1
I0525 12:57:04.125915 25608 net.cpp:148] Top shape: 1 64 112 112 (802816)
I0525 12:57:04.125916 25608 net.cpp:156] Memory required for data: 39337984
I0525 12:57:04.125921 25608 layer_factory.hpp:77] Creating layer relu2_1
I0525 12:57:04.125926 25608 net.cpp:91] Creating Layer relu2_1
I0525 12:57:04.125928 25608 net.cpp:425] relu2_1 <- conv2_1
I0525 12:57:04.125931 25608 net.cpp:386] relu2_1 -> conv2_1 (in-place)
I0525 12:57:04.126068 25608 net.cpp:141] Setting up relu2_1
I0525 12:57:04.126076 25608 net.cpp:148] Top shape: 1 64 112 112 (802816)
I0525 12:57:04.126080 25608 net.cpp:156] Memory required for data: 42549248
I0525 12:57:04.126081 25608 layer_factory.hpp:77] Creating layer pool2
I0525 12:57:04.126085 25608 layer_factory.cpp:90] cuDNN does not support multiple tops. Using Caffe's own pooling layer.
I0525 12:57:04.126088 25608 net.cpp:91] Creating Layer pool2
I0525 12:57:04.126091 25608 net.cpp:425] pool2 <- conv2_1
I0525 12:57:04.126096 25608 net.cpp:399] pool2 -> pool2
I0525 12:57:04.126101 25608 net.cpp:399] pool2 -> pool2_mask
I0525 12:57:04.126132 25608 net.cpp:141] Setting up pool2
I0525 12:57:04.126138 25608 net.cpp:148] Top shape: 1 64 56 56 (200704)
I0525 12:57:04.126142 25608 net.cpp:148] Top shape: 1 64 56 56 (200704)
I0525 12:57:04.126143 25608 net.cpp:156] Memory required for data: 44154880
I0525 12:57:04.126145 25608 layer_factory.hpp:77] Creating layer conv3_1
I0525 12:57:04.126154 25608 net.cpp:91] Creating Layer conv3_1
I0525 12:57:04.126157 25608 net.cpp:425] conv3_1 <- pool2
I0525 12:57:04.126160 25608 net.cpp:399] conv3_1 -> conv3_1
I0525 12:57:04.129096 25608 net.cpp:141] Setting up conv3_1
I0525 12:57:04.129109 25608 net.cpp:148] Top shape: 1 128 56 56 (401408)
I0525 12:57:04.129112 25608 net.cpp:156] Memory required for data: 45760512
I0525 12:57:04.129117 25608 layer_factory.hpp:77] Creating layer bn3_1
I0525 12:57:04.129123 25608 net.cpp:91] Creating Layer bn3_1
I0525 12:57:04.129127 25608 net.cpp:425] bn3_1 <- conv3_1
I0525 12:57:04.129129 25608 net.cpp:386] bn3_1 -> conv3_1 (in-place)
I0525 12:57:04.129596 25608 net.cpp:141] Setting up bn3_1
I0525 12:57:04.129607 25608 net.cpp:148] Top shape: 1 128 56 56 (401408)
I0525 12:57:04.129609 25608 net.cpp:156] Memory required for data: 47366144
I0525 12:57:04.129616 25608 layer_factory.hpp:77] Creating layer scale3_1
I0525 12:57:04.129622 25608 net.cpp:91] Creating Layer scale3_1
I0525 12:57:04.129624 25608 net.cpp:425] scale3_1 <- conv3_1
I0525 12:57:04.129628 25608 net.cpp:386] scale3_1 -> conv3_1 (in-place)
I0525 12:57:04.129662 25608 layer_factory.hpp:77] Creating layer scale3_1
I0525 12:57:04.129750 25608 net.cpp:141] Setting up scale3_1
I0525 12:57:04.129757 25608 net.cpp:148] Top shape: 1 128 56 56 (401408)
I0525 12:57:04.129765 25608 net.cpp:156] Memory required for data: 48971776
I0525 12:57:04.129775 25608 layer_factory.hpp:77] Creating layer relu3_1
I0525 12:57:04.129778 25608 net.cpp:91] Creating Layer relu3_1
I0525 12:57:04.129781 25608 net.cpp:425] relu3_1 <- conv3_1
I0525 12:57:04.129784 25608 net.cpp:386] relu3_1 -> conv3_1 (in-place)
I0525 12:57:04.129925 25608 net.cpp:141] Setting up relu3_1
I0525 12:57:04.129933 25608 net.cpp:148] Top shape: 1 128 56 56 (401408)
I0525 12:57:04.129935 25608 net.cpp:156] Memory required for data: 50577408
I0525 12:57:04.129938 25608 layer_factory.hpp:77] Creating layer pool3
I0525 12:57:04.129941 25608 layer_factory.cpp:90] cuDNN does not support multiple tops. Using Caffe's own pooling layer.
I0525 12:57:04.129947 25608 net.cpp:91] Creating Layer pool3
I0525 12:57:04.129951 25608 net.cpp:425] pool3 <- conv3_1
I0525 12:57:04.129954 25608 net.cpp:399] pool3 -> pool3
I0525 12:57:04.129968 25608 net.cpp:399] pool3 -> pool3_mask
I0525 12:57:04.130004 25608 net.cpp:141] Setting up pool3
I0525 12:57:04.130010 25608 net.cpp:148] Top shape: 1 128 28 28 (100352)
I0525 12:57:04.130013 25608 net.cpp:148] Top shape: 1 128 28 28 (100352)
I0525 12:57:04.130015 25608 net.cpp:156] Memory required for data: 51380224
I0525 12:57:04.130017 25608 layer_factory.hpp:77] Creating layer conv4_1
I0525 12:57:04.130026 25608 net.cpp:91] Creating Layer conv4_1
I0525 12:57:04.130029 25608 net.cpp:425] conv4_1 <- pool3
I0525 12:57:04.130034 25608 net.cpp:399] conv4_1 -> conv4_1
I0525 12:57:04.138514 25608 net.cpp:141] Setting up conv4_1
I0525 12:57:04.138528 25608 net.cpp:148] Top shape: 1 256 28 28 (200704)
I0525 12:57:04.138532 25608 net.cpp:156] Memory required for data: 52183040
I0525 12:57:04.138538 25608 layer_factory.hpp:77] Creating layer bn4_1
I0525 12:57:04.138545 25608 net.cpp:91] Creating Layer bn4_1
I0525 12:57:04.138548 25608 net.cpp:425] bn4_1 <- conv4_1
I0525 12:57:04.138555 25608 net.cpp:386] bn4_1 -> conv4_1 (in-place)
I0525 12:57:04.138701 25608 net.cpp:141] Setting up bn4_1
I0525 12:57:04.138708 25608 net.cpp:148] Top shape: 1 256 28 28 (200704)
I0525 12:57:04.138711 25608 net.cpp:156] Memory required for data: 52985856
I0525 12:57:04.138716 25608 layer_factory.hpp:77] Creating layer scale4_1
I0525 12:57:04.138722 25608 net.cpp:91] Creating Layer scale4_1
I0525 12:57:04.138725 25608 net.cpp:425] scale4_1 <- conv4_1
I0525 12:57:04.138728 25608 net.cpp:386] scale4_1 -> conv4_1 (in-place)
I0525 12:57:04.138759 25608 layer_factory.hpp:77] Creating layer scale4_1
I0525 12:57:04.138846 25608 net.cpp:141] Setting up scale4_1
I0525 12:57:04.138852 25608 net.cpp:148] Top shape: 1 256 28 28 (200704)
I0525 12:57:04.138855 25608 net.cpp:156] Memory required for data: 53788672
I0525 12:57:04.138859 25608 layer_factory.hpp:77] Creating layer relu4_1
I0525 12:57:04.138864 25608 net.cpp:91] Creating Layer relu4_1
I0525 12:57:04.138866 25608 net.cpp:425] relu4_1 <- conv4_1
I0525 12:57:04.138870 25608 net.cpp:386] relu4_1 -> conv4_1 (in-place)
I0525 12:57:04.139118 25608 net.cpp:141] Setting up relu4_1
I0525 12:57:04.139129 25608 net.cpp:148] Top shape: 1 256 28 28 (200704)
I0525 12:57:04.139132 25608 net.cpp:156] Memory required for data: 54591488
I0525 12:57:04.139134 25608 layer_factory.hpp:77] Creating layer pool4
I0525 12:57:04.139137 25608 layer_factory.cpp:90] cuDNN does not support multiple tops. Using Caffe's own pooling layer.
I0525 12:57:04.139142 25608 net.cpp:91] Creating Layer pool4
I0525 12:57:04.139144 25608 net.cpp:425] pool4 <- conv4_1
I0525 12:57:04.139149 25608 net.cpp:399] pool4 -> pool4
I0525 12:57:04.139154 25608 net.cpp:399] pool4 -> pool4_mask
I0525 12:57:04.139189 25608 net.cpp:141] Setting up pool4
I0525 12:57:04.139196 25608 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0525 12:57:04.139199 25608 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0525 12:57:04.139201 25608 net.cpp:156] Memory required for data: 54992896
I0525 12:57:04.139204 25608 layer_factory.hpp:77] Creating layer conv5_1
I0525 12:57:04.139210 25608 net.cpp:91] Creating Layer conv5_1
I0525 12:57:04.139214 25608 net.cpp:425] conv5_1 <- pool4
I0525 12:57:04.139217 25608 net.cpp:399] conv5_1 -> conv5_1
I0525 12:57:04.154999 25608 net.cpp:141] Setting up conv5_1
I0525 12:57:04.155016 25608 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0525 12:57:04.155019 25608 net.cpp:156] Memory required for data: 55193600
I0525 12:57:04.155025 25608 layer_factory.hpp:77] Creating layer bn5_1
I0525 12:57:04.155031 25608 net.cpp:91] Creating Layer bn5_1
I0525 12:57:04.155035 25608 net.cpp:425] bn5_1 <- conv5_1
I0525 12:57:04.155040 25608 net.cpp:386] bn5_1 -> conv5_1 (in-place)
I0525 12:57:04.155187 25608 net.cpp:141] Setting up bn5_1
I0525 12:57:04.155194 25608 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0525 12:57:04.155196 25608 net.cpp:156] Memory required for data: 55394304
I0525 12:57:04.155202 25608 layer_factory.hpp:77] Creating layer scale5_1
I0525 12:57:04.155208 25608 net.cpp:91] Creating Layer scale5_1
I0525 12:57:04.155210 25608 net.cpp:425] scale5_1 <- conv5_1
I0525 12:57:04.155226 25608 net.cpp:386] scale5_1 -> conv5_1 (in-place)
I0525 12:57:04.155261 25608 layer_factory.hpp:77] Creating layer scale5_1
I0525 12:57:04.155351 25608 net.cpp:141] Setting up scale5_1
I0525 12:57:04.155359 25608 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0525 12:57:04.155361 25608 net.cpp:156] Memory required for data: 55595008
I0525 12:57:04.155365 25608 layer_factory.hpp:77] Creating layer relu5_1
I0525 12:57:04.155371 25608 net.cpp:91] Creating Layer relu5_1
I0525 12:57:04.155375 25608 net.cpp:425] relu5_1 <- conv5_1
I0525 12:57:04.155377 25608 net.cpp:386] relu5_1 -> conv5_1 (in-place)
I0525 12:57:04.155521 25608 net.cpp:141] Setting up relu5_1
I0525 12:57:04.155529 25608 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0525 12:57:04.155532 25608 net.cpp:156] Memory required for data: 55795712
I0525 12:57:04.155534 25608 layer_factory.hpp:77] Creating layer pool5
I0525 12:57:04.155537 25608 layer_factory.cpp:90] cuDNN does not support multiple tops. Using Caffe's own pooling layer.
I0525 12:57:04.155542 25608 net.cpp:91] Creating Layer pool5
I0525 12:57:04.155545 25608 net.cpp:425] pool5 <- conv5_1
I0525 12:57:04.155550 25608 net.cpp:399] pool5 -> pool5
I0525 12:57:04.155555 25608 net.cpp:399] pool5 -> pool5_mask
I0525 12:57:04.155587 25608 net.cpp:141] Setting up pool5
I0525 12:57:04.155593 25608 net.cpp:148] Top shape: 1 256 7 7 (12544)
I0525 12:57:04.155597 25608 net.cpp:148] Top shape: 1 256 7 7 (12544)
I0525 12:57:04.155599 25608 net.cpp:156] Memory required for data: 55896064
I0525 12:57:04.155601 25608 layer_factory.hpp:77] Creating layer upsample5
I0525 12:57:04.155607 25608 net.cpp:91] Creating Layer upsample5
I0525 12:57:04.155611 25608 net.cpp:425] upsample5 <- pool5
I0525 12:57:04.155613 25608 net.cpp:425] upsample5 <- pool5_mask
I0525 12:57:04.155616 25608 net.cpp:399] upsample5 -> pool5_D
I0525 12:57:04.155637 25608 net.cpp:141] Setting up upsample5
I0525 12:57:04.155643 25608 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0525 12:57:04.155645 25608 net.cpp:156] Memory required for data: 56096768
I0525 12:57:04.155647 25608 layer_factory.hpp:77] Creating layer conv5_1_D
I0525 12:57:04.155656 25608 net.cpp:91] Creating Layer conv5_1_D
I0525 12:57:04.155658 25608 net.cpp:425] conv5_1_D <- pool5_D
I0525 12:57:04.155664 25608 net.cpp:399] conv5_1_D -> conv5_1_D
I0525 12:57:04.171511 25608 net.cpp:141] Setting up conv5_1_D
I0525 12:57:04.171532 25608 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0525 12:57:04.171535 25608 net.cpp:156] Memory required for data: 56297472
I0525 12:57:04.171541 25608 layer_factory.hpp:77] Creating layer bn5_1_D
I0525 12:57:04.171552 25608 net.cpp:91] Creating Layer bn5_1_D
I0525 12:57:04.171556 25608 net.cpp:425] bn5_1_D <- conv5_1_D
I0525 12:57:04.171562 25608 net.cpp:386] bn5_1_D -> conv5_1_D (in-place)
I0525 12:57:04.171712 25608 net.cpp:141] Setting up bn5_1_D
I0525 12:57:04.171720 25608 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0525 12:57:04.171723 25608 net.cpp:156] Memory required for data: 56498176
I0525 12:57:04.171737 25608 layer_factory.hpp:77] Creating layer scale5_1_D
I0525 12:57:04.171744 25608 net.cpp:91] Creating Layer scale5_1_D
I0525 12:57:04.171747 25608 net.cpp:425] scale5_1_D <- conv5_1_D
I0525 12:57:04.171751 25608 net.cpp:386] scale5_1_D -> conv5_1_D (in-place)
I0525 12:57:04.171785 25608 layer_factory.hpp:77] Creating layer scale5_1_D
I0525 12:57:04.171871 25608 net.cpp:141] Setting up scale5_1_D
I0525 12:57:04.171877 25608 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0525 12:57:04.171880 25608 net.cpp:156] Memory required for data: 56698880
I0525 12:57:04.171885 25608 layer_factory.hpp:77] Creating layer relu5_1_D
I0525 12:57:04.171888 25608 net.cpp:91] Creating Layer relu5_1_D
I0525 12:57:04.171891 25608 net.cpp:425] relu5_1_D <- conv5_1_D
I0525 12:57:04.171897 25608 net.cpp:386] relu5_1_D -> conv5_1_D (in-place)
I0525 12:57:04.172039 25608 net.cpp:141] Setting up relu5_1_D
I0525 12:57:04.172045 25608 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0525 12:57:04.172049 25608 net.cpp:156] Memory required for data: 56899584
I0525 12:57:04.172063 25608 layer_factory.hpp:77] Creating layer upsample4
I0525 12:57:04.172073 25608 net.cpp:91] Creating Layer upsample4
I0525 12:57:04.172076 25608 net.cpp:425] upsample4 <- conv5_1_D
I0525 12:57:04.172080 25608 net.cpp:425] upsample4 <- pool4_mask
I0525 12:57:04.172085 25608 net.cpp:399] upsample4 -> pool4_D
I0525 12:57:04.172111 25608 net.cpp:141] Setting up upsample4
I0525 12:57:04.172117 25608 net.cpp:148] Top shape: 1 256 28 28 (200704)
I0525 12:57:04.172119 25608 net.cpp:156] Memory required for data: 57702400
I0525 12:57:04.172122 25608 layer_factory.hpp:77] Creating layer conv4_1_D
I0525 12:57:04.172130 25608 net.cpp:91] Creating Layer conv4_1_D
I0525 12:57:04.172133 25608 net.cpp:425] conv4_1_D <- pool4_D
I0525 12:57:04.172137 25608 net.cpp:399] conv4_1_D -> conv4_1_D
I0525 12:57:04.180574 25608 net.cpp:141] Setting up conv4_1_D
I0525 12:57:04.180591 25608 net.cpp:148] Top shape: 1 128 28 28 (100352)
I0525 12:57:04.180593 25608 net.cpp:156] Memory required for data: 58103808
I0525 12:57:04.180598 25608 layer_factory.hpp:77] Creating layer bn4_1_D
I0525 12:57:04.180605 25608 net.cpp:91] Creating Layer bn4_1_D
I0525 12:57:04.180609 25608 net.cpp:425] bn4_1_D <- conv4_1_D
I0525 12:57:04.180614 25608 net.cpp:386] bn4_1_D -> conv4_1_D (in-place)
I0525 12:57:04.180768 25608 net.cpp:141] Setting up bn4_1_D
I0525 12:57:04.180775 25608 net.cpp:148] Top shape: 1 128 28 28 (100352)
I0525 12:57:04.180778 25608 net.cpp:156] Memory required for data: 58505216
I0525 12:57:04.180783 25608 layer_factory.hpp:77] Creating layer scale4_1_D
I0525 12:57:04.180789 25608 net.cpp:91] Creating Layer scale4_1_D
I0525 12:57:04.180793 25608 net.cpp:425] scale4_1_D <- conv4_1_D
I0525 12:57:04.180796 25608 net.cpp:386] scale4_1_D -> conv4_1_D (in-place)
I0525 12:57:04.180827 25608 layer_factory.hpp:77] Creating layer scale4_1_D
I0525 12:57:04.180913 25608 net.cpp:141] Setting up scale4_1_D
I0525 12:57:04.180922 25608 net.cpp:148] Top shape: 1 128 28 28 (100352)
I0525 12:57:04.180923 25608 net.cpp:156] Memory required for data: 58906624
I0525 12:57:04.180927 25608 layer_factory.hpp:77] Creating layer relu4_1_D
I0525 12:57:04.180932 25608 net.cpp:91] Creating Layer relu4_1_D
I0525 12:57:04.180934 25608 net.cpp:425] relu4_1_D <- conv4_1_D
I0525 12:57:04.180938 25608 net.cpp:386] relu4_1_D -> conv4_1_D (in-place)
I0525 12:57:04.181182 25608 net.cpp:141] Setting up relu4_1_D
I0525 12:57:04.181193 25608 net.cpp:148] Top shape: 1 128 28 28 (100352)
I0525 12:57:04.181195 25608 net.cpp:156] Memory required for data: 59308032
I0525 12:57:04.181198 25608 layer_factory.hpp:77] Creating layer upsample3
I0525 12:57:04.181205 25608 net.cpp:91] Creating Layer upsample3
I0525 12:57:04.181208 25608 net.cpp:425] upsample3 <- conv4_1_D
I0525 12:57:04.181212 25608 net.cpp:425] upsample3 <- pool3_mask
I0525 12:57:04.181216 25608 net.cpp:399] upsample3 -> pool3_D
I0525 12:57:04.181241 25608 net.cpp:141] Setting up upsample3
I0525 12:57:04.181246 25608 net.cpp:148] Top shape: 1 128 56 56 (401408)
I0525 12:57:04.181247 25608 net.cpp:156] Memory required for data: 60913664
I0525 12:57:04.181249 25608 layer_factory.hpp:77] Creating layer conv3_1_D
I0525 12:57:04.181259 25608 net.cpp:91] Creating Layer conv3_1_D
I0525 12:57:04.181262 25608 net.cpp:425] conv3_1_D <- pool3_D
I0525 12:57:04.181267 25608 net.cpp:399] conv3_1_D -> conv3_1_D
I0525 12:57:04.184018 25608 net.cpp:141] Setting up conv3_1_D
I0525 12:57:04.184031 25608 net.cpp:148] Top shape: 1 64 56 56 (200704)
I0525 12:57:04.184033 25608 net.cpp:156] Memory required for data: 61716480
I0525 12:57:04.184039 25608 layer_factory.hpp:77] Creating layer bn3_1_D
I0525 12:57:04.184044 25608 net.cpp:91] Creating Layer bn3_1_D
I0525 12:57:04.184047 25608 net.cpp:425] bn3_1_D <- conv3_1_D
I0525 12:57:04.184052 25608 net.cpp:386] bn3_1_D -> conv3_1_D (in-place)
I0525 12:57:04.184216 25608 net.cpp:141] Setting up bn3_1_D
I0525 12:57:04.184222 25608 net.cpp:148] Top shape: 1 64 56 56 (200704)
I0525 12:57:04.184224 25608 net.cpp:156] Memory required for data: 62519296
I0525 12:57:04.184231 25608 layer_factory.hpp:77] Creating layer scale3_1_D
I0525 12:57:04.184248 25608 net.cpp:91] Creating Layer scale3_1_D
I0525 12:57:04.184252 25608 net.cpp:425] scale3_1_D <- conv3_1_D
I0525 12:57:04.184255 25608 net.cpp:386] scale3_1_D -> conv3_1_D (in-place)
I0525 12:57:04.184291 25608 layer_factory.hpp:77] Creating layer scale3_1_D
I0525 12:57:04.184391 25608 net.cpp:141] Setting up scale3_1_D
I0525 12:57:04.184399 25608 net.cpp:148] Top shape: 1 64 56 56 (200704)
I0525 12:57:04.184402 25608 net.cpp:156] Memory required for data: 63322112
I0525 12:57:04.184406 25608 layer_factory.hpp:77] Creating layer relu3_1_D
I0525 12:57:04.184411 25608 net.cpp:91] Creating Layer relu3_1_D
I0525 12:57:04.184413 25608 net.cpp:425] relu3_1_D <- conv3_1_D
I0525 12:57:04.184417 25608 net.cpp:386] relu3_1_D -> conv3_1_D (in-place)
I0525 12:57:04.184559 25608 net.cpp:141] Setting up relu3_1_D
I0525 12:57:04.184567 25608 net.cpp:148] Top shape: 1 64 56 56 (200704)
I0525 12:57:04.184571 25608 net.cpp:156] Memory required for data: 64124928
I0525 12:57:04.184572 25608 layer_factory.hpp:77] Creating layer upsample2
I0525 12:57:04.184578 25608 net.cpp:91] Creating Layer upsample2
I0525 12:57:04.184581 25608 net.cpp:425] upsample2 <- conv3_1_D
I0525 12:57:04.184584 25608 net.cpp:425] upsample2 <- pool2_mask
I0525 12:57:04.184592 25608 net.cpp:399] upsample2 -> pool2_D
I0525 12:57:04.184619 25608 net.cpp:141] Setting up upsample2
I0525 12:57:04.184625 25608 net.cpp:148] Top shape: 1 64 112 112 (802816)
I0525 12:57:04.184628 25608 net.cpp:156] Memory required for data: 67336192
I0525 12:57:04.184630 25608 layer_factory.hpp:77] Creating layer conv2_1_D
I0525 12:57:04.184639 25608 net.cpp:91] Creating Layer conv2_1_D
I0525 12:57:04.184641 25608 net.cpp:425] conv2_1_D <- pool2_D
I0525 12:57:04.184646 25608 net.cpp:399] conv2_1_D -> conv2_1_D
I0525 12:57:04.185868 25608 net.cpp:141] Setting up conv2_1_D
I0525 12:57:04.185879 25608 net.cpp:148] Top shape: 1 32 112 112 (401408)
I0525 12:57:04.185881 25608 net.cpp:156] Memory required for data: 68941824
I0525 12:57:04.185886 25608 layer_factory.hpp:77] Creating layer bn2_1_D
I0525 12:57:04.185892 25608 net.cpp:91] Creating Layer bn2_1_D
I0525 12:57:04.185895 25608 net.cpp:425] bn2_1_D <- conv2_1_D
I0525 12:57:04.185901 25608 net.cpp:386] bn2_1_D -> conv2_1_D (in-place)
I0525 12:57:04.186063 25608 net.cpp:141] Setting up bn2_1_D
I0525 12:57:04.186070 25608 net.cpp:148] Top shape: 1 32 112 112 (401408)
I0525 12:57:04.186072 25608 net.cpp:156] Memory required for data: 70547456
I0525 12:57:04.186077 25608 layer_factory.hpp:77] Creating layer scale2_1_D
I0525 12:57:04.186084 25608 net.cpp:91] Creating Layer scale2_1_D
I0525 12:57:04.186087 25608 net.cpp:425] scale2_1_D <- conv2_1_D
I0525 12:57:04.186090 25608 net.cpp:386] scale2_1_D -> conv2_1_D (in-place)
I0525 12:57:04.186125 25608 layer_factory.hpp:77] Creating layer scale2_1_D
I0525 12:57:04.186264 25608 net.cpp:141] Setting up scale2_1_D
I0525 12:57:04.186275 25608 net.cpp:148] Top shape: 1 32 112 112 (401408)
I0525 12:57:04.186277 25608 net.cpp:156] Memory required for data: 72153088
I0525 12:57:04.186282 25608 layer_factory.hpp:77] Creating layer relu2_1_D
I0525 12:57:04.186287 25608 net.cpp:91] Creating Layer relu2_1_D
I0525 12:57:04.186290 25608 net.cpp:425] relu2_1_D <- conv2_1_D
I0525 12:57:04.186295 25608 net.cpp:386] relu2_1_D -> conv2_1_D (in-place)
I0525 12:57:04.186434 25608 net.cpp:141] Setting up relu2_1_D
I0525 12:57:04.186442 25608 net.cpp:148] Top shape: 1 32 112 112 (401408)
I0525 12:57:04.186445 25608 net.cpp:156] Memory required for data: 73758720
I0525 12:57:04.186449 25608 layer_factory.hpp:77] Creating layer upsample1
I0525 12:57:04.186452 25608 net.cpp:91] Creating Layer upsample1
I0525 12:57:04.186455 25608 net.cpp:425] upsample1 <- conv2_1_D
I0525 12:57:04.186458 25608 net.cpp:425] upsample1 <- pool1_mask
I0525 12:57:04.186462 25608 net.cpp:399] upsample1 -> pool1_D
I0525 12:57:04.186486 25608 net.cpp:141] Setting up upsample1
I0525 12:57:04.186492 25608 net.cpp:148] Top shape: 1 32 224 224 (1605632)
I0525 12:57:04.186494 25608 net.cpp:156] Memory required for data: 80181248
I0525 12:57:04.186506 25608 layer_factory.hpp:77] Creating layer conv1_1_D
I0525 12:57:04.186513 25608 net.cpp:91] Creating Layer conv1_1_D
I0525 12:57:04.186517 25608 net.cpp:425] conv1_1_D <- pool1_D
I0525 12:57:04.186522 25608 net.cpp:399] conv1_1_D -> conv1_1_D
I0525 12:57:04.187553 25608 net.cpp:141] Setting up conv1_1_D
I0525 12:57:04.187566 25608 net.cpp:148] Top shape: 1 2 224 224 (100352)
I0525 12:57:04.187569 25608 net.cpp:156] Memory required for data: 80582656
I0525 12:57:04.187574 25608 layer_factory.hpp:77] Creating layer conv1_1_D_conv1_1_D_0_split
I0525 12:57:04.187580 25608 net.cpp:91] Creating Layer conv1_1_D_conv1_1_D_0_split
I0525 12:57:04.187582 25608 net.cpp:425] conv1_1_D_conv1_1_D_0_split <- conv1_1_D
I0525 12:57:04.187589 25608 net.cpp:399] conv1_1_D_conv1_1_D_0_split -> conv1_1_D_conv1_1_D_0_split_0
I0525 12:57:04.187594 25608 net.cpp:399] conv1_1_D_conv1_1_D_0_split -> conv1_1_D_conv1_1_D_0_split_1
I0525 12:57:04.187634 25608 net.cpp:141] Setting up conv1_1_D_conv1_1_D_0_split
I0525 12:57:04.187641 25608 net.cpp:148] Top shape: 1 2 224 224 (100352)
I0525 12:57:04.187644 25608 net.cpp:148] Top shape: 1 2 224 224 (100352)
I0525 12:57:04.187646 25608 net.cpp:156] Memory required for data: 81385472
I0525 12:57:04.187649 25608 layer_factory.hpp:77] Creating layer loss
I0525 12:57:04.187652 25608 net.cpp:91] Creating Layer loss
I0525 12:57:04.187655 25608 net.cpp:425] loss <- conv1_1_D_conv1_1_D_0_split_0
I0525 12:57:04.187659 25608 net.cpp:425] loss <- label_data_1_split_0
I0525 12:57:04.187662 25608 net.cpp:399] loss -> loss
I0525 12:57:04.187669 25608 layer_factory.hpp:77] Creating layer loss
I0525 12:57:04.188082 25608 net.cpp:141] Setting up loss
I0525 12:57:04.188096 25608 net.cpp:148] Top shape: (1)
I0525 12:57:04.188097 25608 net.cpp:151]     with loss weight 1
I0525 12:57:04.188114 25608 net.cpp:156] Memory required for data: 81385476
I0525 12:57:04.188117 25608 layer_factory.hpp:77] Creating layer accuracy
I0525 12:57:04.188124 25608 net.cpp:91] Creating Layer accuracy
I0525 12:57:04.188127 25608 net.cpp:425] accuracy <- conv1_1_D_conv1_1_D_0_split_1
I0525 12:57:04.188130 25608 net.cpp:425] accuracy <- label_data_1_split_1
I0525 12:57:04.188135 25608 net.cpp:399] accuracy -> accuracy
I0525 12:57:04.188144 25608 net.cpp:141] Setting up accuracy
I0525 12:57:04.188150 25608 net.cpp:148] Top shape: (1)
I0525 12:57:04.188153 25608 net.cpp:156] Memory required for data: 81385480
I0525 12:57:04.188155 25608 net.cpp:219] accuracy does not need backward computation.
I0525 12:57:04.188158 25608 net.cpp:217] loss needs backward computation.
I0525 12:57:04.188160 25608 net.cpp:217] conv1_1_D_conv1_1_D_0_split needs backward computation.
I0525 12:57:04.188163 25608 net.cpp:217] conv1_1_D needs backward computation.
I0525 12:57:04.188165 25608 net.cpp:217] upsample1 needs backward computation.
I0525 12:57:04.188168 25608 net.cpp:217] relu2_1_D needs backward computation.
I0525 12:57:04.188169 25608 net.cpp:217] scale2_1_D needs backward computation.
I0525 12:57:04.188171 25608 net.cpp:217] bn2_1_D needs backward computation.
I0525 12:57:04.188174 25608 net.cpp:217] conv2_1_D needs backward computation.
I0525 12:57:04.188175 25608 net.cpp:217] upsample2 needs backward computation.
I0525 12:57:04.188177 25608 net.cpp:217] relu3_1_D needs backward computation.
I0525 12:57:04.188179 25608 net.cpp:217] scale3_1_D needs backward computation.
I0525 12:57:04.188184 25608 net.cpp:217] bn3_1_D needs backward computation.
I0525 12:57:04.188185 25608 net.cpp:217] conv3_1_D needs backward computation.
I0525 12:57:04.188187 25608 net.cpp:217] upsample3 needs backward computation.
I0525 12:57:04.188189 25608 net.cpp:217] relu4_1_D needs backward computation.
I0525 12:57:04.188191 25608 net.cpp:217] scale4_1_D needs backward computation.
I0525 12:57:04.188194 25608 net.cpp:217] bn4_1_D needs backward computation.
I0525 12:57:04.188195 25608 net.cpp:217] conv4_1_D needs backward computation.
I0525 12:57:04.188197 25608 net.cpp:217] upsample4 needs backward computation.
I0525 12:57:04.188210 25608 net.cpp:217] relu5_1_D needs backward computation.
I0525 12:57:04.188212 25608 net.cpp:217] scale5_1_D needs backward computation.
I0525 12:57:04.188215 25608 net.cpp:217] bn5_1_D needs backward computation.
I0525 12:57:04.188216 25608 net.cpp:217] conv5_1_D needs backward computation.
I0525 12:57:04.188218 25608 net.cpp:217] upsample5 needs backward computation.
I0525 12:57:04.188221 25608 net.cpp:217] pool5 needs backward computation.
I0525 12:57:04.188225 25608 net.cpp:217] relu5_1 needs backward computation.
I0525 12:57:04.188226 25608 net.cpp:217] scale5_1 needs backward computation.
I0525 12:57:04.188228 25608 net.cpp:217] bn5_1 needs backward computation.
I0525 12:57:04.188230 25608 net.cpp:217] conv5_1 needs backward computation.
I0525 12:57:04.188233 25608 net.cpp:217] pool4 needs backward computation.
I0525 12:57:04.188235 25608 net.cpp:217] relu4_1 needs backward computation.
I0525 12:57:04.188237 25608 net.cpp:217] scale4_1 needs backward computation.
I0525 12:57:04.188241 25608 net.cpp:217] bn4_1 needs backward computation.
I0525 12:57:04.188242 25608 net.cpp:217] conv4_1 needs backward computation.
I0525 12:57:04.188244 25608 net.cpp:217] pool3 needs backward computation.
I0525 12:57:04.188247 25608 net.cpp:217] relu3_1 needs backward computation.
I0525 12:57:04.188249 25608 net.cpp:217] scale3_1 needs backward computation.
I0525 12:57:04.188251 25608 net.cpp:217] bn3_1 needs backward computation.
I0525 12:57:04.188253 25608 net.cpp:217] conv3_1 needs backward computation.
I0525 12:57:04.188256 25608 net.cpp:217] pool2 needs backward computation.
I0525 12:57:04.188258 25608 net.cpp:217] relu2_1 needs backward computation.
I0525 12:57:04.188261 25608 net.cpp:217] scale2_1 needs backward computation.
I0525 12:57:04.188262 25608 net.cpp:217] bn2_1 needs backward computation.
I0525 12:57:04.188264 25608 net.cpp:217] conv2_1 needs backward computation.
I0525 12:57:04.188267 25608 net.cpp:217] pool1 needs backward computation.
I0525 12:57:04.188269 25608 net.cpp:217] relu1_1 needs backward computation.
I0525 12:57:04.188271 25608 net.cpp:217] scale1_1 needs backward computation.
I0525 12:57:04.188274 25608 net.cpp:217] bn1_1 needs backward computation.
I0525 12:57:04.188277 25608 net.cpp:217] conv1_1 needs backward computation.
I0525 12:57:04.188279 25608 net.cpp:219] label_data_1_split does not need backward computation.
I0525 12:57:04.188282 25608 net.cpp:219] data does not need backward computation.
I0525 12:57:04.188284 25608 net.cpp:261] This network produces output accuracy
I0525 12:57:04.188287 25608 net.cpp:261] This network produces output loss
I0525 12:57:04.188309 25608 net.cpp:274] Network initialization done.
I0525 12:57:04.189206 25608 solver.cpp:181] Creating test net (#0) specified by net file: models/segnet/train_val.prototxt
I0525 12:57:04.189263 25608 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0525 12:57:04.189493 25608 net.cpp:49] Initializing net from parameters: 
name: "segnet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "DenseImageData"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    crop_size: 224
    mean_value: 100
  }
  dense_image_data_param {
    source: "data/val.txt"
    batch_size: 4
    shuffle: true
    new_height: 224
    new_width: 310
    is_color: false
    root_folder: "data/raw/train/"
  }
}
layer {
  name: "conv1_1"
  type: "Convolution"
  bottom: "data"
  top: "conv1_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn1_1"
  type: "BatchNorm"
  bottom: "conv1_1"
  top: "conv1_1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale1_1"
  type: "Scale"
  bottom: "conv1_1"
  top: "conv1_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1_1"
  type: "ReLU"
  bottom: "conv1_1"
  top: "conv1_1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1_1"
  top: "pool1"
  top: "pool1_mask"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2_1"
  top: "pool2"
  top: "pool2_mask"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn3_1"
  type: "BatchNorm"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale3_1"
  type: "Scale"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3_1"
  top: "pool3"
  top: "pool3_mask"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4_1"
  top: "pool4"
  top: "pool4_mask"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv5_1"
  type: "Convolution"
  bottom: "pool4"
  top: "conv5_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn5_1"
  type: "BatchNorm"
  bottom: "conv5_1"
  top: "conv5_1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale5_1"
  type: "Scale"
  bottom: "conv5_1"
  top: "conv5_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5_1"
  type: "ReLU"
  bottom: "conv5_1"
  top: "conv5_1"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5_1"
  top: "pool5"
  top: "pool5_mask"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "upsample5"
  type: "Upsample"
  bottom: "pool5"
  bottom: "pool5_mask"
  top: "pool5_D"
  upsample_param {
    scale: 2
    upsample_h: 14
    upsample_w: 14
  }
}
layer {
  name: "conv5_1_D"
  type: "Convolution"
  bottom: "pool5_D"
  top: "conv5_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn5_1_D"
  type: "BatchNorm"
  bottom: "conv5_1_D"
  top: "conv5_1_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale5_1_D"
  type: "Scale"
  bottom: "conv5_1_D"
  top: "conv5_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5_1_D"
  type: "ReLU"
  bottom: "conv5_1_D"
  top: "conv5_1_D"
}
layer {
  name: "upsample4"
  type: "Upsample"
  bottom: "conv5_1_D"
  bottom: "pool4_mask"
  top: "pool4_D"
  upsample_param {
    scale: 2
    upsample_h: 28
    upsample_w: 28
  }
}
layer {
  name: "conv4_1_D"
  type: "Convolution"
  bottom: "pool4_D"
  top: "conv4_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn4_1_D"
  type: "BatchNorm"
  bottom: "conv4_1_D"
  top: "conv4_1_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale4_1_D"
  type: "Scale"
  bottom: "conv4_1_D"
  top: "conv4_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4_1_D"
  type: "ReLU"
  bottom: "conv4_1_D"
  top: "conv4_1_D"
}
layer {
  name: "upsample3"
  type: "Upsample"
  bottom: "conv4_1_D"
  bottom: "pool3_mask"
  top: "pool3_D"
  upsample_param {
    scale: 2
    upsample_h: 56
    upsample_w: 56
  }
}
layer {
  name: "conv3_1_D"
  type: "Convolution"
  bottom: "pool3_D"
  top: "conv3_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn3_1_D"
  type: "BatchNorm"
  bottom: "conv3_1_D"
  top: "conv3_1_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale3_1_D"
  type: "Scale"
  bottom: "conv3_1_D"
  top: "conv3_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_1_D"
  type: "ReLU"
  bottom: "conv3_1_D"
  top: "conv3_1_D"
}
layer {
  name: "upsample2"
  type: "Upsample"
  bottom: "conv3_1_D"
  bottom: "pool2_mask"
  top: "pool2_D"
  upsample_param {
    scale: 2
    upsample_h: 112
    upsample_w: 112
  }
}
layer {
  name: "conv2_1_D"
  type: "Convolution"
  bottom: "pool2_D"
  top: "conv2_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn2_1_D"
  type: "BatchNorm"
  bottom: "conv2_1_D"
  top: "conv2_1_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale2_1_D"
  type: "Scale"
  bottom: "conv2_1_D"
  top: "conv2_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2_1_D"
  type: "ReLU"
  bottom: "conv2_1_D"
  top: "conv2_1_D"
}
layer {
  name: "upsample1"
  type: "Upsample"
  bottom: "conv2_1_D"
  bottom: "pool1_mask"
  top: "pool1_D"
  upsample_param {
    scale: 2
    upsample_h: 224
    upsample_w: 224
  }
}
layer {
  name: "conv1_1_D"
  type: "Convolution"
  bottom: "pool1_D"
  top: "conv1_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 2
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "conv1_1_D"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "conv1_1_D"
  bottom: "label"
  top: "accuracy"
}
I0525 12:57:04.189643 25608 layer_factory.hpp:77] Creating layer data
I0525 12:57:04.189656 25608 net.cpp:91] Creating Layer data
I0525 12:57:04.189659 25608 net.cpp:399] data -> data
I0525 12:57:04.189666 25608 net.cpp:399] data -> label
I0525 12:57:04.189676 25608 dense_image_data_layer.cpp:38] Opening file data/val.txt
I0525 12:57:04.190006 25608 dense_image_data_layer.cpp:48] Shuffling data
I0525 12:57:04.190078 25608 dense_image_data_layer.cpp:53] A total of 705 examples.
I0525 12:57:04.194623 25608 dense_image_data_layer.cpp:92] output data size: 1,1,224,224
I0525 12:57:04.195363 25608 net.cpp:141] Setting up data
I0525 12:57:04.195374 25608 net.cpp:148] Top shape: 1 1 224 224 (50176)
I0525 12:57:04.195379 25608 net.cpp:148] Top shape: 1 1 224 224 (50176)
I0525 12:57:04.195380 25608 net.cpp:156] Memory required for data: 401408
I0525 12:57:04.195384 25608 layer_factory.hpp:77] Creating layer label_data_1_split
I0525 12:57:04.195390 25608 net.cpp:91] Creating Layer label_data_1_split
I0525 12:57:04.195394 25608 net.cpp:425] label_data_1_split <- label
I0525 12:57:04.195397 25608 net.cpp:399] label_data_1_split -> label_data_1_split_0
I0525 12:57:04.195404 25608 net.cpp:399] label_data_1_split -> label_data_1_split_1
I0525 12:57:04.195441 25608 net.cpp:141] Setting up label_data_1_split
I0525 12:57:04.195447 25608 net.cpp:148] Top shape: 1 1 224 224 (50176)
I0525 12:57:04.195451 25608 net.cpp:148] Top shape: 1 1 224 224 (50176)
I0525 12:57:04.195452 25608 net.cpp:156] Memory required for data: 802816
I0525 12:57:04.195456 25608 layer_factory.hpp:77] Creating layer conv1_1
I0525 12:57:04.195462 25608 net.cpp:91] Creating Layer conv1_1
I0525 12:57:04.195466 25608 net.cpp:425] conv1_1 <- data
I0525 12:57:04.195469 25608 net.cpp:399] conv1_1 -> conv1_1
I0525 12:57:04.196714 25608 net.cpp:141] Setting up conv1_1
I0525 12:57:04.196727 25608 net.cpp:148] Top shape: 1 32 224 224 (1605632)
I0525 12:57:04.196729 25608 net.cpp:156] Memory required for data: 7225344
I0525 12:57:04.196735 25608 layer_factory.hpp:77] Creating layer bn1_1
I0525 12:57:04.196741 25608 net.cpp:91] Creating Layer bn1_1
I0525 12:57:04.196744 25608 net.cpp:425] bn1_1 <- conv1_1
I0525 12:57:04.196748 25608 net.cpp:386] bn1_1 -> conv1_1 (in-place)
I0525 12:57:04.197258 25608 net.cpp:141] Setting up bn1_1
I0525 12:57:04.197269 25608 net.cpp:148] Top shape: 1 32 224 224 (1605632)
I0525 12:57:04.197273 25608 net.cpp:156] Memory required for data: 13647872
I0525 12:57:04.197281 25608 layer_factory.hpp:77] Creating layer scale1_1
I0525 12:57:04.197288 25608 net.cpp:91] Creating Layer scale1_1
I0525 12:57:04.197293 25608 net.cpp:425] scale1_1 <- conv1_1
I0525 12:57:04.197296 25608 net.cpp:386] scale1_1 -> conv1_1 (in-place)
I0525 12:57:04.197331 25608 layer_factory.hpp:77] Creating layer scale1_1
I0525 12:57:04.197537 25608 net.cpp:141] Setting up scale1_1
I0525 12:57:04.197557 25608 net.cpp:148] Top shape: 1 32 224 224 (1605632)
I0525 12:57:04.197561 25608 net.cpp:156] Memory required for data: 20070400
I0525 12:57:04.197567 25608 layer_factory.hpp:77] Creating layer relu1_1
I0525 12:57:04.197572 25608 net.cpp:91] Creating Layer relu1_1
I0525 12:57:04.197574 25608 net.cpp:425] relu1_1 <- conv1_1
I0525 12:57:04.197578 25608 net.cpp:386] relu1_1 -> conv1_1 (in-place)
I0525 12:57:04.197723 25608 net.cpp:141] Setting up relu1_1
I0525 12:57:04.197731 25608 net.cpp:148] Top shape: 1 32 224 224 (1605632)
I0525 12:57:04.197734 25608 net.cpp:156] Memory required for data: 26492928
I0525 12:57:04.197736 25608 layer_factory.hpp:77] Creating layer pool1
I0525 12:57:04.197739 25608 layer_factory.cpp:90] cuDNN does not support multiple tops. Using Caffe's own pooling layer.
I0525 12:57:04.197744 25608 net.cpp:91] Creating Layer pool1
I0525 12:57:04.197747 25608 net.cpp:425] pool1 <- conv1_1
I0525 12:57:04.197751 25608 net.cpp:399] pool1 -> pool1
I0525 12:57:04.197756 25608 net.cpp:399] pool1 -> pool1_mask
I0525 12:57:04.197790 25608 net.cpp:141] Setting up pool1
I0525 12:57:04.197796 25608 net.cpp:148] Top shape: 1 32 112 112 (401408)
I0525 12:57:04.197799 25608 net.cpp:148] Top shape: 1 32 112 112 (401408)
I0525 12:57:04.197801 25608 net.cpp:156] Memory required for data: 29704192
I0525 12:57:04.197803 25608 layer_factory.hpp:77] Creating layer conv2_1
I0525 12:57:04.197810 25608 net.cpp:91] Creating Layer conv2_1
I0525 12:57:04.197813 25608 net.cpp:425] conv2_1 <- pool1
I0525 12:57:04.197816 25608 net.cpp:399] conv2_1 -> conv2_1
I0525 12:57:04.199097 25608 net.cpp:141] Setting up conv2_1
I0525 12:57:04.199110 25608 net.cpp:148] Top shape: 1 64 112 112 (802816)
I0525 12:57:04.199112 25608 net.cpp:156] Memory required for data: 32915456
I0525 12:57:04.199116 25608 layer_factory.hpp:77] Creating layer bn2_1
I0525 12:57:04.199123 25608 net.cpp:91] Creating Layer bn2_1
I0525 12:57:04.199126 25608 net.cpp:425] bn2_1 <- conv2_1
I0525 12:57:04.199129 25608 net.cpp:386] bn2_1 -> conv2_1 (in-place)
I0525 12:57:04.199292 25608 net.cpp:141] Setting up bn2_1
I0525 12:57:04.199300 25608 net.cpp:148] Top shape: 1 64 112 112 (802816)
I0525 12:57:04.199302 25608 net.cpp:156] Memory required for data: 36126720
I0525 12:57:04.199309 25608 layer_factory.hpp:77] Creating layer scale2_1
I0525 12:57:04.199314 25608 net.cpp:91] Creating Layer scale2_1
I0525 12:57:04.199317 25608 net.cpp:425] scale2_1 <- conv2_1
I0525 12:57:04.199321 25608 net.cpp:386] scale2_1 -> conv2_1 (in-place)
I0525 12:57:04.199364 25608 layer_factory.hpp:77] Creating layer scale2_1
I0525 12:57:04.199478 25608 net.cpp:141] Setting up scale2_1
I0525 12:57:04.199486 25608 net.cpp:148] Top shape: 1 64 112 112 (802816)
I0525 12:57:04.199489 25608 net.cpp:156] Memory required for data: 39337984
I0525 12:57:04.199493 25608 layer_factory.hpp:77] Creating layer relu2_1
I0525 12:57:04.199497 25608 net.cpp:91] Creating Layer relu2_1
I0525 12:57:04.199499 25608 net.cpp:425] relu2_1 <- conv2_1
I0525 12:57:04.199503 25608 net.cpp:386] relu2_1 -> conv2_1 (in-place)
I0525 12:57:04.199643 25608 net.cpp:141] Setting up relu2_1
I0525 12:57:04.199651 25608 net.cpp:148] Top shape: 1 64 112 112 (802816)
I0525 12:57:04.199653 25608 net.cpp:156] Memory required for data: 42549248
I0525 12:57:04.199656 25608 layer_factory.hpp:77] Creating layer pool2
I0525 12:57:04.199658 25608 layer_factory.cpp:90] cuDNN does not support multiple tops. Using Caffe's own pooling layer.
I0525 12:57:04.199662 25608 net.cpp:91] Creating Layer pool2
I0525 12:57:04.199664 25608 net.cpp:425] pool2 <- conv2_1
I0525 12:57:04.199668 25608 net.cpp:399] pool2 -> pool2
I0525 12:57:04.199673 25608 net.cpp:399] pool2 -> pool2_mask
I0525 12:57:04.199709 25608 net.cpp:141] Setting up pool2
I0525 12:57:04.199715 25608 net.cpp:148] Top shape: 1 64 56 56 (200704)
I0525 12:57:04.199718 25608 net.cpp:148] Top shape: 1 64 56 56 (200704)
I0525 12:57:04.199720 25608 net.cpp:156] Memory required for data: 44154880
I0525 12:57:04.199723 25608 layer_factory.hpp:77] Creating layer conv3_1
I0525 12:57:04.199738 25608 net.cpp:91] Creating Layer conv3_1
I0525 12:57:04.199743 25608 net.cpp:425] conv3_1 <- pool2
I0525 12:57:04.199746 25608 net.cpp:399] conv3_1 -> conv3_1
I0525 12:57:04.202697 25608 net.cpp:141] Setting up conv3_1
I0525 12:57:04.202708 25608 net.cpp:148] Top shape: 1 128 56 56 (401408)
I0525 12:57:04.202711 25608 net.cpp:156] Memory required for data: 45760512
I0525 12:57:04.202715 25608 layer_factory.hpp:77] Creating layer bn3_1
I0525 12:57:04.202721 25608 net.cpp:91] Creating Layer bn3_1
I0525 12:57:04.202723 25608 net.cpp:425] bn3_1 <- conv3_1
I0525 12:57:04.202728 25608 net.cpp:386] bn3_1 -> conv3_1 (in-place)
I0525 12:57:04.202888 25608 net.cpp:141] Setting up bn3_1
I0525 12:57:04.202894 25608 net.cpp:148] Top shape: 1 128 56 56 (401408)
I0525 12:57:04.202898 25608 net.cpp:156] Memory required for data: 47366144
I0525 12:57:04.202903 25608 layer_factory.hpp:77] Creating layer scale3_1
I0525 12:57:04.202908 25608 net.cpp:91] Creating Layer scale3_1
I0525 12:57:04.202910 25608 net.cpp:425] scale3_1 <- conv3_1
I0525 12:57:04.202914 25608 net.cpp:386] scale3_1 -> conv3_1 (in-place)
I0525 12:57:04.202946 25608 layer_factory.hpp:77] Creating layer scale3_1
I0525 12:57:04.203043 25608 net.cpp:141] Setting up scale3_1
I0525 12:57:04.203050 25608 net.cpp:148] Top shape: 1 128 56 56 (401408)
I0525 12:57:04.203053 25608 net.cpp:156] Memory required for data: 48971776
I0525 12:57:04.203060 25608 layer_factory.hpp:77] Creating layer relu3_1
I0525 12:57:04.203065 25608 net.cpp:91] Creating Layer relu3_1
I0525 12:57:04.203068 25608 net.cpp:425] relu3_1 <- conv3_1
I0525 12:57:04.203070 25608 net.cpp:386] relu3_1 -> conv3_1 (in-place)
I0525 12:57:04.203315 25608 net.cpp:141] Setting up relu3_1
I0525 12:57:04.203330 25608 net.cpp:148] Top shape: 1 128 56 56 (401408)
I0525 12:57:04.203333 25608 net.cpp:156] Memory required for data: 50577408
I0525 12:57:04.203336 25608 layer_factory.hpp:77] Creating layer pool3
I0525 12:57:04.203339 25608 layer_factory.cpp:90] cuDNN does not support multiple tops. Using Caffe's own pooling layer.
I0525 12:57:04.203344 25608 net.cpp:91] Creating Layer pool3
I0525 12:57:04.203347 25608 net.cpp:425] pool3 <- conv3_1
I0525 12:57:04.203351 25608 net.cpp:399] pool3 -> pool3
I0525 12:57:04.203357 25608 net.cpp:399] pool3 -> pool3_mask
I0525 12:57:04.203395 25608 net.cpp:141] Setting up pool3
I0525 12:57:04.203402 25608 net.cpp:148] Top shape: 1 128 28 28 (100352)
I0525 12:57:04.203405 25608 net.cpp:148] Top shape: 1 128 28 28 (100352)
I0525 12:57:04.203408 25608 net.cpp:156] Memory required for data: 51380224
I0525 12:57:04.203409 25608 layer_factory.hpp:77] Creating layer conv4_1
I0525 12:57:04.203416 25608 net.cpp:91] Creating Layer conv4_1
I0525 12:57:04.203418 25608 net.cpp:425] conv4_1 <- pool3
I0525 12:57:04.203423 25608 net.cpp:399] conv4_1 -> conv4_1
I0525 12:57:04.211778 25608 net.cpp:141] Setting up conv4_1
I0525 12:57:04.211791 25608 net.cpp:148] Top shape: 1 256 28 28 (200704)
I0525 12:57:04.211794 25608 net.cpp:156] Memory required for data: 52183040
I0525 12:57:04.211799 25608 layer_factory.hpp:77] Creating layer bn4_1
I0525 12:57:04.211804 25608 net.cpp:91] Creating Layer bn4_1
I0525 12:57:04.211807 25608 net.cpp:425] bn4_1 <- conv4_1
I0525 12:57:04.211812 25608 net.cpp:386] bn4_1 -> conv4_1 (in-place)
I0525 12:57:04.211971 25608 net.cpp:141] Setting up bn4_1
I0525 12:57:04.211979 25608 net.cpp:148] Top shape: 1 256 28 28 (200704)
I0525 12:57:04.211982 25608 net.cpp:156] Memory required for data: 52985856
I0525 12:57:04.211987 25608 layer_factory.hpp:77] Creating layer scale4_1
I0525 12:57:04.211992 25608 net.cpp:91] Creating Layer scale4_1
I0525 12:57:04.211995 25608 net.cpp:425] scale4_1 <- conv4_1
I0525 12:57:04.211998 25608 net.cpp:386] scale4_1 -> conv4_1 (in-place)
I0525 12:57:04.212033 25608 layer_factory.hpp:77] Creating layer scale4_1
I0525 12:57:04.212124 25608 net.cpp:141] Setting up scale4_1
I0525 12:57:04.212131 25608 net.cpp:148] Top shape: 1 256 28 28 (200704)
I0525 12:57:04.212133 25608 net.cpp:156] Memory required for data: 53788672
I0525 12:57:04.212151 25608 layer_factory.hpp:77] Creating layer relu4_1
I0525 12:57:04.212154 25608 net.cpp:91] Creating Layer relu4_1
I0525 12:57:04.212157 25608 net.cpp:425] relu4_1 <- conv4_1
I0525 12:57:04.212160 25608 net.cpp:386] relu4_1 -> conv4_1 (in-place)
I0525 12:57:04.212303 25608 net.cpp:141] Setting up relu4_1
I0525 12:57:04.212312 25608 net.cpp:148] Top shape: 1 256 28 28 (200704)
I0525 12:57:04.212314 25608 net.cpp:156] Memory required for data: 54591488
I0525 12:57:04.212317 25608 layer_factory.hpp:77] Creating layer pool4
I0525 12:57:04.212321 25608 layer_factory.cpp:90] cuDNN does not support multiple tops. Using Caffe's own pooling layer.
I0525 12:57:04.212324 25608 net.cpp:91] Creating Layer pool4
I0525 12:57:04.212327 25608 net.cpp:425] pool4 <- conv4_1
I0525 12:57:04.212330 25608 net.cpp:399] pool4 -> pool4
I0525 12:57:04.212337 25608 net.cpp:399] pool4 -> pool4_mask
I0525 12:57:04.212373 25608 net.cpp:141] Setting up pool4
I0525 12:57:04.212378 25608 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0525 12:57:04.212381 25608 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0525 12:57:04.212383 25608 net.cpp:156] Memory required for data: 54992896
I0525 12:57:04.212386 25608 layer_factory.hpp:77] Creating layer conv5_1
I0525 12:57:04.212393 25608 net.cpp:91] Creating Layer conv5_1
I0525 12:57:04.212395 25608 net.cpp:425] conv5_1 <- pool4
I0525 12:57:04.212399 25608 net.cpp:399] conv5_1 -> conv5_1
I0525 12:57:04.228346 25608 net.cpp:141] Setting up conv5_1
I0525 12:57:04.228368 25608 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0525 12:57:04.228371 25608 net.cpp:156] Memory required for data: 55193600
I0525 12:57:04.228376 25608 layer_factory.hpp:77] Creating layer bn5_1
I0525 12:57:04.228386 25608 net.cpp:91] Creating Layer bn5_1
I0525 12:57:04.228389 25608 net.cpp:425] bn5_1 <- conv5_1
I0525 12:57:04.228394 25608 net.cpp:386] bn5_1 -> conv5_1 (in-place)
I0525 12:57:04.228559 25608 net.cpp:141] Setting up bn5_1
I0525 12:57:04.228566 25608 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0525 12:57:04.228569 25608 net.cpp:156] Memory required for data: 55394304
I0525 12:57:04.228574 25608 layer_factory.hpp:77] Creating layer scale5_1
I0525 12:57:04.228581 25608 net.cpp:91] Creating Layer scale5_1
I0525 12:57:04.228585 25608 net.cpp:425] scale5_1 <- conv5_1
I0525 12:57:04.228587 25608 net.cpp:386] scale5_1 -> conv5_1 (in-place)
I0525 12:57:04.228622 25608 layer_factory.hpp:77] Creating layer scale5_1
I0525 12:57:04.228710 25608 net.cpp:141] Setting up scale5_1
I0525 12:57:04.228718 25608 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0525 12:57:04.228719 25608 net.cpp:156] Memory required for data: 55595008
I0525 12:57:04.228724 25608 layer_factory.hpp:77] Creating layer relu5_1
I0525 12:57:04.228729 25608 net.cpp:91] Creating Layer relu5_1
I0525 12:57:04.228731 25608 net.cpp:425] relu5_1 <- conv5_1
I0525 12:57:04.228734 25608 net.cpp:386] relu5_1 -> conv5_1 (in-place)
I0525 12:57:04.228884 25608 net.cpp:141] Setting up relu5_1
I0525 12:57:04.228893 25608 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0525 12:57:04.228895 25608 net.cpp:156] Memory required for data: 55795712
I0525 12:57:04.228898 25608 layer_factory.hpp:77] Creating layer pool5
I0525 12:57:04.228900 25608 layer_factory.cpp:90] cuDNN does not support multiple tops. Using Caffe's own pooling layer.
I0525 12:57:04.228905 25608 net.cpp:91] Creating Layer pool5
I0525 12:57:04.228907 25608 net.cpp:425] pool5 <- conv5_1
I0525 12:57:04.228911 25608 net.cpp:399] pool5 -> pool5
I0525 12:57:04.228917 25608 net.cpp:399] pool5 -> pool5_mask
I0525 12:57:04.228955 25608 net.cpp:141] Setting up pool5
I0525 12:57:04.228961 25608 net.cpp:148] Top shape: 1 256 7 7 (12544)
I0525 12:57:04.228965 25608 net.cpp:148] Top shape: 1 256 7 7 (12544)
I0525 12:57:04.228966 25608 net.cpp:156] Memory required for data: 55896064
I0525 12:57:04.228968 25608 layer_factory.hpp:77] Creating layer upsample5
I0525 12:57:04.228974 25608 net.cpp:91] Creating Layer upsample5
I0525 12:57:04.228976 25608 net.cpp:425] upsample5 <- pool5
I0525 12:57:04.228979 25608 net.cpp:425] upsample5 <- pool5_mask
I0525 12:57:04.228996 25608 net.cpp:399] upsample5 -> pool5_D
I0525 12:57:04.229022 25608 net.cpp:141] Setting up upsample5
I0525 12:57:04.229028 25608 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0525 12:57:04.229030 25608 net.cpp:156] Memory required for data: 56096768
I0525 12:57:04.229032 25608 layer_factory.hpp:77] Creating layer conv5_1_D
I0525 12:57:04.229039 25608 net.cpp:91] Creating Layer conv5_1_D
I0525 12:57:04.229043 25608 net.cpp:425] conv5_1_D <- pool5_D
I0525 12:57:04.229046 25608 net.cpp:399] conv5_1_D -> conv5_1_D
I0525 12:57:04.244905 25608 net.cpp:141] Setting up conv5_1_D
I0525 12:57:04.244920 25608 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0525 12:57:04.244923 25608 net.cpp:156] Memory required for data: 56297472
I0525 12:57:04.244928 25608 layer_factory.hpp:77] Creating layer bn5_1_D
I0525 12:57:04.244935 25608 net.cpp:91] Creating Layer bn5_1_D
I0525 12:57:04.244937 25608 net.cpp:425] bn5_1_D <- conv5_1_D
I0525 12:57:04.244942 25608 net.cpp:386] bn5_1_D -> conv5_1_D (in-place)
I0525 12:57:04.245108 25608 net.cpp:141] Setting up bn5_1_D
I0525 12:57:04.245115 25608 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0525 12:57:04.245117 25608 net.cpp:156] Memory required for data: 56498176
I0525 12:57:04.245129 25608 layer_factory.hpp:77] Creating layer scale5_1_D
I0525 12:57:04.245136 25608 net.cpp:91] Creating Layer scale5_1_D
I0525 12:57:04.245138 25608 net.cpp:425] scale5_1_D <- conv5_1_D
I0525 12:57:04.245141 25608 net.cpp:386] scale5_1_D -> conv5_1_D (in-place)
I0525 12:57:04.245177 25608 layer_factory.hpp:77] Creating layer scale5_1_D
I0525 12:57:04.245266 25608 net.cpp:141] Setting up scale5_1_D
I0525 12:57:04.245273 25608 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0525 12:57:04.245275 25608 net.cpp:156] Memory required for data: 56698880
I0525 12:57:04.245280 25608 layer_factory.hpp:77] Creating layer relu5_1_D
I0525 12:57:04.245283 25608 net.cpp:91] Creating Layer relu5_1_D
I0525 12:57:04.245286 25608 net.cpp:425] relu5_1_D <- conv5_1_D
I0525 12:57:04.245290 25608 net.cpp:386] relu5_1_D -> conv5_1_D (in-place)
I0525 12:57:04.245539 25608 net.cpp:141] Setting up relu5_1_D
I0525 12:57:04.245550 25608 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0525 12:57:04.245553 25608 net.cpp:156] Memory required for data: 56899584
I0525 12:57:04.245556 25608 layer_factory.hpp:77] Creating layer upsample4
I0525 12:57:04.245564 25608 net.cpp:91] Creating Layer upsample4
I0525 12:57:04.245568 25608 net.cpp:425] upsample4 <- conv5_1_D
I0525 12:57:04.245571 25608 net.cpp:425] upsample4 <- pool4_mask
I0525 12:57:04.245575 25608 net.cpp:399] upsample4 -> pool4_D
I0525 12:57:04.245604 25608 net.cpp:141] Setting up upsample4
I0525 12:57:04.245610 25608 net.cpp:148] Top shape: 1 256 28 28 (200704)
I0525 12:57:04.245612 25608 net.cpp:156] Memory required for data: 57702400
I0525 12:57:04.245615 25608 layer_factory.hpp:77] Creating layer conv4_1_D
I0525 12:57:04.245622 25608 net.cpp:91] Creating Layer conv4_1_D
I0525 12:57:04.245625 25608 net.cpp:425] conv4_1_D <- pool4_D
I0525 12:57:04.245628 25608 net.cpp:399] conv4_1_D -> conv4_1_D
I0525 12:57:04.254127 25608 net.cpp:141] Setting up conv4_1_D
I0525 12:57:04.254142 25608 net.cpp:148] Top shape: 1 128 28 28 (100352)
I0525 12:57:04.254145 25608 net.cpp:156] Memory required for data: 58103808
I0525 12:57:04.254151 25608 layer_factory.hpp:77] Creating layer bn4_1_D
I0525 12:57:04.254158 25608 net.cpp:91] Creating Layer bn4_1_D
I0525 12:57:04.254161 25608 net.cpp:425] bn4_1_D <- conv4_1_D
I0525 12:57:04.254168 25608 net.cpp:386] bn4_1_D -> conv4_1_D (in-place)
I0525 12:57:04.254344 25608 net.cpp:141] Setting up bn4_1_D
I0525 12:57:04.254353 25608 net.cpp:148] Top shape: 1 128 28 28 (100352)
I0525 12:57:04.254355 25608 net.cpp:156] Memory required for data: 58505216
I0525 12:57:04.254361 25608 layer_factory.hpp:77] Creating layer scale4_1_D
I0525 12:57:04.254367 25608 net.cpp:91] Creating Layer scale4_1_D
I0525 12:57:04.254369 25608 net.cpp:425] scale4_1_D <- conv4_1_D
I0525 12:57:04.254374 25608 net.cpp:386] scale4_1_D -> conv4_1_D (in-place)
I0525 12:57:04.254410 25608 layer_factory.hpp:77] Creating layer scale4_1_D
I0525 12:57:04.254521 25608 net.cpp:141] Setting up scale4_1_D
I0525 12:57:04.254531 25608 net.cpp:148] Top shape: 1 128 28 28 (100352)
I0525 12:57:04.254534 25608 net.cpp:156] Memory required for data: 58906624
I0525 12:57:04.254537 25608 layer_factory.hpp:77] Creating layer relu4_1_D
I0525 12:57:04.254542 25608 net.cpp:91] Creating Layer relu4_1_D
I0525 12:57:04.254544 25608 net.cpp:425] relu4_1_D <- conv4_1_D
I0525 12:57:04.254549 25608 net.cpp:386] relu4_1_D -> conv4_1_D (in-place)
I0525 12:57:04.254700 25608 net.cpp:141] Setting up relu4_1_D
I0525 12:57:04.254710 25608 net.cpp:148] Top shape: 1 128 28 28 (100352)
I0525 12:57:04.254714 25608 net.cpp:156] Memory required for data: 59308032
I0525 12:57:04.254715 25608 layer_factory.hpp:77] Creating layer upsample3
I0525 12:57:04.254721 25608 net.cpp:91] Creating Layer upsample3
I0525 12:57:04.254724 25608 net.cpp:425] upsample3 <- conv4_1_D
I0525 12:57:04.254727 25608 net.cpp:425] upsample3 <- pool3_mask
I0525 12:57:04.254732 25608 net.cpp:399] upsample3 -> pool3_D
I0525 12:57:04.254760 25608 net.cpp:141] Setting up upsample3
I0525 12:57:04.254763 25608 net.cpp:148] Top shape: 1 128 56 56 (401408)
I0525 12:57:04.254766 25608 net.cpp:156] Memory required for data: 60913664
I0525 12:57:04.254768 25608 layer_factory.hpp:77] Creating layer conv3_1_D
I0525 12:57:04.254776 25608 net.cpp:91] Creating Layer conv3_1_D
I0525 12:57:04.254779 25608 net.cpp:425] conv3_1_D <- pool3_D
I0525 12:57:04.254783 25608 net.cpp:399] conv3_1_D -> conv3_1_D
I0525 12:57:04.257547 25608 net.cpp:141] Setting up conv3_1_D
I0525 12:57:04.257560 25608 net.cpp:148] Top shape: 1 64 56 56 (200704)
I0525 12:57:04.257563 25608 net.cpp:156] Memory required for data: 61716480
I0525 12:57:04.257568 25608 layer_factory.hpp:77] Creating layer bn3_1_D
I0525 12:57:04.257573 25608 net.cpp:91] Creating Layer bn3_1_D
I0525 12:57:04.257576 25608 net.cpp:425] bn3_1_D <- conv3_1_D
I0525 12:57:04.257581 25608 net.cpp:386] bn3_1_D -> conv3_1_D (in-place)
I0525 12:57:04.257783 25608 net.cpp:141] Setting up bn3_1_D
I0525 12:57:04.257791 25608 net.cpp:148] Top shape: 1 64 56 56 (200704)
I0525 12:57:04.257794 25608 net.cpp:156] Memory required for data: 62519296
I0525 12:57:04.257799 25608 layer_factory.hpp:77] Creating layer scale3_1_D
I0525 12:57:04.257805 25608 net.cpp:91] Creating Layer scale3_1_D
I0525 12:57:04.257807 25608 net.cpp:425] scale3_1_D <- conv3_1_D
I0525 12:57:04.257812 25608 net.cpp:386] scale3_1_D -> conv3_1_D (in-place)
I0525 12:57:04.257850 25608 layer_factory.hpp:77] Creating layer scale3_1_D
I0525 12:57:04.257971 25608 net.cpp:141] Setting up scale3_1_D
I0525 12:57:04.257978 25608 net.cpp:148] Top shape: 1 64 56 56 (200704)
I0525 12:57:04.257980 25608 net.cpp:156] Memory required for data: 63322112
I0525 12:57:04.257985 25608 layer_factory.hpp:77] Creating layer relu3_1_D
I0525 12:57:04.257990 25608 net.cpp:91] Creating Layer relu3_1_D
I0525 12:57:04.257992 25608 net.cpp:425] relu3_1_D <- conv3_1_D
I0525 12:57:04.257997 25608 net.cpp:386] relu3_1_D -> conv3_1_D (in-place)
I0525 12:57:04.258150 25608 net.cpp:141] Setting up relu3_1_D
I0525 12:57:04.258160 25608 net.cpp:148] Top shape: 1 64 56 56 (200704)
I0525 12:57:04.258163 25608 net.cpp:156] Memory required for data: 64124928
I0525 12:57:04.258165 25608 layer_factory.hpp:77] Creating layer upsample2
I0525 12:57:04.258170 25608 net.cpp:91] Creating Layer upsample2
I0525 12:57:04.258172 25608 net.cpp:425] upsample2 <- conv3_1_D
I0525 12:57:04.258177 25608 net.cpp:425] upsample2 <- pool2_mask
I0525 12:57:04.258180 25608 net.cpp:399] upsample2 -> pool2_D
I0525 12:57:04.258208 25608 net.cpp:141] Setting up upsample2
I0525 12:57:04.258213 25608 net.cpp:148] Top shape: 1 64 112 112 (802816)
I0525 12:57:04.258215 25608 net.cpp:156] Memory required for data: 67336192
I0525 12:57:04.258218 25608 layer_factory.hpp:77] Creating layer conv2_1_D
I0525 12:57:04.258224 25608 net.cpp:91] Creating Layer conv2_1_D
I0525 12:57:04.258226 25608 net.cpp:425] conv2_1_D <- pool2_D
I0525 12:57:04.258231 25608 net.cpp:399] conv2_1_D -> conv2_1_D
I0525 12:57:04.259512 25608 net.cpp:141] Setting up conv2_1_D
I0525 12:57:04.259524 25608 net.cpp:148] Top shape: 1 32 112 112 (401408)
I0525 12:57:04.259527 25608 net.cpp:156] Memory required for data: 68941824
I0525 12:57:04.259531 25608 layer_factory.hpp:77] Creating layer bn2_1_D
I0525 12:57:04.259538 25608 net.cpp:91] Creating Layer bn2_1_D
I0525 12:57:04.259541 25608 net.cpp:425] bn2_1_D <- conv2_1_D
I0525 12:57:04.259544 25608 net.cpp:386] bn2_1_D -> conv2_1_D (in-place)
I0525 12:57:04.259737 25608 net.cpp:141] Setting up bn2_1_D
I0525 12:57:04.259744 25608 net.cpp:148] Top shape: 1 32 112 112 (401408)
I0525 12:57:04.259747 25608 net.cpp:156] Memory required for data: 70547456
I0525 12:57:04.259752 25608 layer_factory.hpp:77] Creating layer scale2_1_D
I0525 12:57:04.259757 25608 net.cpp:91] Creating Layer scale2_1_D
I0525 12:57:04.259759 25608 net.cpp:425] scale2_1_D <- conv2_1_D
I0525 12:57:04.259763 25608 net.cpp:386] scale2_1_D -> conv2_1_D (in-place)
I0525 12:57:04.259801 25608 layer_factory.hpp:77] Creating layer scale2_1_D
I0525 12:57:04.259927 25608 net.cpp:141] Setting up scale2_1_D
I0525 12:57:04.259934 25608 net.cpp:148] Top shape: 1 32 112 112 (401408)
I0525 12:57:04.259937 25608 net.cpp:156] Memory required for data: 72153088
I0525 12:57:04.259941 25608 layer_factory.hpp:77] Creating layer relu2_1_D
I0525 12:57:04.259945 25608 net.cpp:91] Creating Layer relu2_1_D
I0525 12:57:04.259948 25608 net.cpp:425] relu2_1_D <- conv2_1_D
I0525 12:57:04.259953 25608 net.cpp:386] relu2_1_D -> conv2_1_D (in-place)
I0525 12:57:04.260213 25608 net.cpp:141] Setting up relu2_1_D
I0525 12:57:04.260224 25608 net.cpp:148] Top shape: 1 32 112 112 (401408)
I0525 12:57:04.260227 25608 net.cpp:156] Memory required for data: 73758720
I0525 12:57:04.260229 25608 layer_factory.hpp:77] Creating layer upsample1
I0525 12:57:04.260234 25608 net.cpp:91] Creating Layer upsample1
I0525 12:57:04.260237 25608 net.cpp:425] upsample1 <- conv2_1_D
I0525 12:57:04.260241 25608 net.cpp:425] upsample1 <- pool1_mask
I0525 12:57:04.260246 25608 net.cpp:399] upsample1 -> pool1_D
I0525 12:57:04.260275 25608 net.cpp:141] Setting up upsample1
I0525 12:57:04.260279 25608 net.cpp:148] Top shape: 1 32 224 224 (1605632)
I0525 12:57:04.260282 25608 net.cpp:156] Memory required for data: 80181248
I0525 12:57:04.260284 25608 layer_factory.hpp:77] Creating layer conv1_1_D
I0525 12:57:04.260293 25608 net.cpp:91] Creating Layer conv1_1_D
I0525 12:57:04.260295 25608 net.cpp:425] conv1_1_D <- pool1_D
I0525 12:57:04.260299 25608 net.cpp:399] conv1_1_D -> conv1_1_D
I0525 12:57:04.262496 25608 net.cpp:141] Setting up conv1_1_D
I0525 12:57:04.262508 25608 net.cpp:148] Top shape: 1 2 224 224 (100352)
I0525 12:57:04.262511 25608 net.cpp:156] Memory required for data: 80582656
I0525 12:57:04.262518 25608 layer_factory.hpp:77] Creating layer conv1_1_D_conv1_1_D_0_split
I0525 12:57:04.262523 25608 net.cpp:91] Creating Layer conv1_1_D_conv1_1_D_0_split
I0525 12:57:04.262527 25608 net.cpp:425] conv1_1_D_conv1_1_D_0_split <- conv1_1_D
I0525 12:57:04.262532 25608 net.cpp:399] conv1_1_D_conv1_1_D_0_split -> conv1_1_D_conv1_1_D_0_split_0
I0525 12:57:04.262538 25608 net.cpp:399] conv1_1_D_conv1_1_D_0_split -> conv1_1_D_conv1_1_D_0_split_1
I0525 12:57:04.262585 25608 net.cpp:141] Setting up conv1_1_D_conv1_1_D_0_split
I0525 12:57:04.262590 25608 net.cpp:148] Top shape: 1 2 224 224 (100352)
I0525 12:57:04.262593 25608 net.cpp:148] Top shape: 1 2 224 224 (100352)
I0525 12:57:04.262595 25608 net.cpp:156] Memory required for data: 81385472
I0525 12:57:04.262598 25608 layer_factory.hpp:77] Creating layer loss
I0525 12:57:04.262603 25608 net.cpp:91] Creating Layer loss
I0525 12:57:04.262604 25608 net.cpp:425] loss <- conv1_1_D_conv1_1_D_0_split_0
I0525 12:57:04.262608 25608 net.cpp:425] loss <- label_data_1_split_0
I0525 12:57:04.262611 25608 net.cpp:399] loss -> loss
I0525 12:57:04.262617 25608 layer_factory.hpp:77] Creating layer loss
I0525 12:57:04.262929 25608 net.cpp:141] Setting up loss
I0525 12:57:04.262938 25608 net.cpp:148] Top shape: (1)
I0525 12:57:04.262949 25608 net.cpp:151]     with loss weight 1
I0525 12:57:04.262959 25608 net.cpp:156] Memory required for data: 81385476
I0525 12:57:04.262962 25608 layer_factory.hpp:77] Creating layer accuracy
I0525 12:57:04.262967 25608 net.cpp:91] Creating Layer accuracy
I0525 12:57:04.262969 25608 net.cpp:425] accuracy <- conv1_1_D_conv1_1_D_0_split_1
I0525 12:57:04.262974 25608 net.cpp:425] accuracy <- label_data_1_split_1
I0525 12:57:04.262979 25608 net.cpp:399] accuracy -> accuracy
I0525 12:57:04.262987 25608 net.cpp:141] Setting up accuracy
I0525 12:57:04.262991 25608 net.cpp:148] Top shape: (1)
I0525 12:57:04.262994 25608 net.cpp:156] Memory required for data: 81385480
I0525 12:57:04.262995 25608 net.cpp:219] accuracy does not need backward computation.
I0525 12:57:04.262997 25608 net.cpp:217] loss needs backward computation.
I0525 12:57:04.263000 25608 net.cpp:217] conv1_1_D_conv1_1_D_0_split needs backward computation.
I0525 12:57:04.263002 25608 net.cpp:217] conv1_1_D needs backward computation.
I0525 12:57:04.263005 25608 net.cpp:217] upsample1 needs backward computation.
I0525 12:57:04.263007 25608 net.cpp:217] relu2_1_D needs backward computation.
I0525 12:57:04.263010 25608 net.cpp:217] scale2_1_D needs backward computation.
I0525 12:57:04.263011 25608 net.cpp:217] bn2_1_D needs backward computation.
I0525 12:57:04.263013 25608 net.cpp:217] conv2_1_D needs backward computation.
I0525 12:57:04.263015 25608 net.cpp:217] upsample2 needs backward computation.
I0525 12:57:04.263017 25608 net.cpp:217] relu3_1_D needs backward computation.
I0525 12:57:04.263020 25608 net.cpp:217] scale3_1_D needs backward computation.
I0525 12:57:04.263022 25608 net.cpp:217] bn3_1_D needs backward computation.
I0525 12:57:04.263023 25608 net.cpp:217] conv3_1_D needs backward computation.
I0525 12:57:04.263026 25608 net.cpp:217] upsample3 needs backward computation.
I0525 12:57:04.263028 25608 net.cpp:217] relu4_1_D needs backward computation.
I0525 12:57:04.263031 25608 net.cpp:217] scale4_1_D needs backward computation.
I0525 12:57:04.263032 25608 net.cpp:217] bn4_1_D needs backward computation.
I0525 12:57:04.263034 25608 net.cpp:217] conv4_1_D needs backward computation.
I0525 12:57:04.263036 25608 net.cpp:217] upsample4 needs backward computation.
I0525 12:57:04.263038 25608 net.cpp:217] relu5_1_D needs backward computation.
I0525 12:57:04.263041 25608 net.cpp:217] scale5_1_D needs backward computation.
I0525 12:57:04.263043 25608 net.cpp:217] bn5_1_D needs backward computation.
I0525 12:57:04.263046 25608 net.cpp:217] conv5_1_D needs backward computation.
I0525 12:57:04.263047 25608 net.cpp:217] upsample5 needs backward computation.
I0525 12:57:04.263051 25608 net.cpp:217] pool5 needs backward computation.
I0525 12:57:04.263053 25608 net.cpp:217] relu5_1 needs backward computation.
I0525 12:57:04.263056 25608 net.cpp:217] scale5_1 needs backward computation.
I0525 12:57:04.263058 25608 net.cpp:217] bn5_1 needs backward computation.
I0525 12:57:04.263061 25608 net.cpp:217] conv5_1 needs backward computation.
I0525 12:57:04.263063 25608 net.cpp:217] pool4 needs backward computation.
I0525 12:57:04.263065 25608 net.cpp:217] relu4_1 needs backward computation.
I0525 12:57:04.263067 25608 net.cpp:217] scale4_1 needs backward computation.
I0525 12:57:04.263070 25608 net.cpp:217] bn4_1 needs backward computation.
I0525 12:57:04.263072 25608 net.cpp:217] conv4_1 needs backward computation.
I0525 12:57:04.263074 25608 net.cpp:217] pool3 needs backward computation.
I0525 12:57:04.263077 25608 net.cpp:217] relu3_1 needs backward computation.
I0525 12:57:04.263079 25608 net.cpp:217] scale3_1 needs backward computation.
I0525 12:57:04.263082 25608 net.cpp:217] bn3_1 needs backward computation.
I0525 12:57:04.263083 25608 net.cpp:217] conv3_1 needs backward computation.
I0525 12:57:04.263087 25608 net.cpp:217] pool2 needs backward computation.
I0525 12:57:04.263088 25608 net.cpp:217] relu2_1 needs backward computation.
I0525 12:57:04.263090 25608 net.cpp:217] scale2_1 needs backward computation.
I0525 12:57:04.263092 25608 net.cpp:217] bn2_1 needs backward computation.
I0525 12:57:04.263100 25608 net.cpp:217] conv2_1 needs backward computation.
I0525 12:57:04.263103 25608 net.cpp:217] pool1 needs backward computation.
I0525 12:57:04.263105 25608 net.cpp:217] relu1_1 needs backward computation.
I0525 12:57:04.263108 25608 net.cpp:217] scale1_1 needs backward computation.
I0525 12:57:04.263109 25608 net.cpp:217] bn1_1 needs backward computation.
I0525 12:57:04.263111 25608 net.cpp:217] conv1_1 needs backward computation.
I0525 12:57:04.263114 25608 net.cpp:219] label_data_1_split does not need backward computation.
I0525 12:57:04.263118 25608 net.cpp:219] data does not need backward computation.
I0525 12:57:04.263119 25608 net.cpp:261] This network produces output accuracy
I0525 12:57:04.263123 25608 net.cpp:261] This network produces output loss
I0525 12:57:04.263145 25608 net.cpp:274] Network initialization done.
I0525 12:57:04.263285 25608 solver.cpp:60] Solver scaffolding done.
I0525 12:57:04.265005 25608 caffe.cpp:219] Starting Optimization
I0525 12:57:04.265013 25608 solver.cpp:279] Solving segnet
I0525 12:57:04.265015 25608 solver.cpp:280] Learning Rate Policy: step
I0525 12:57:04.266441 25608 solver.cpp:337] Iteration 0, Testing net (#0)
I0525 12:57:04.810967 25608 solver.cpp:404]     Test net output #0: accuracy = 0.604938
I0525 12:57:04.810992 25608 solver.cpp:404]     Test net output #1: loss = 0.669556 (* 1 = 0.669556 loss)
I0525 12:57:05.574422 25608 solver.cpp:228] Iteration 0, loss = 0.670124
I0525 12:57:05.574448 25608 solver.cpp:244]     Train net output #0: accuracy = 0.604563
I0525 12:57:05.574455 25608 solver.cpp:244]     Train net output #1: loss = 0.670124 (* 1 = 0.670124 loss)
I0525 12:57:05.574467 25608 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0525 12:57:20.589710 25608 solver.cpp:228] Iteration 20, loss = 0.133641
I0525 12:57:20.589735 25608 solver.cpp:244]     Train net output #0: accuracy = 0.983961
I0525 12:57:20.589742 25608 solver.cpp:244]     Train net output #1: loss = 0.133641 (* 1 = 0.133641 loss)
I0525 12:57:20.589747 25608 sgd_solver.cpp:106] Iteration 20, lr = 0.01
I0525 12:57:35.854604 25608 solver.cpp:228] Iteration 40, loss = 0.117938
I0525 12:57:35.854662 25608 solver.cpp:244]     Train net output #0: accuracy = 0.978268
I0525 12:57:35.854672 25608 solver.cpp:244]     Train net output #1: loss = 0.117938 (* 1 = 0.117938 loss)
I0525 12:57:35.854676 25608 sgd_solver.cpp:106] Iteration 40, lr = 0.01
I0525 12:57:51.122282 25608 solver.cpp:228] Iteration 60, loss = 0.109264
I0525 12:57:51.122306 25608 solver.cpp:244]     Train net output #0: accuracy = 0.979933
I0525 12:57:51.122313 25608 solver.cpp:244]     Train net output #1: loss = 0.109264 (* 1 = 0.109264 loss)
I0525 12:57:51.122318 25608 sgd_solver.cpp:106] Iteration 60, lr = 0.01
I0525 12:58:06.399090 25608 solver.cpp:228] Iteration 80, loss = 0.10572
I0525 12:58:06.399190 25608 solver.cpp:244]     Train net output #0: accuracy = 0.980004
I0525 12:58:06.399201 25608 solver.cpp:244]     Train net output #1: loss = 0.10572 (* 1 = 0.10572 loss)
I0525 12:58:06.399205 25608 sgd_solver.cpp:106] Iteration 80, lr = 0.01
I0525 12:58:21.237661 25608 solver.cpp:337] Iteration 100, Testing net (#0)
I0525 12:58:21.751147 25608 solver.cpp:404]     Test net output #0: accuracy = 0.98371
I0525 12:58:21.751184 25608 solver.cpp:404]     Test net output #1: loss = 0.0896728 (* 1 = 0.0896728 loss)
I0525 12:58:22.203910 25608 solver.cpp:228] Iteration 100, loss = 0.0873383
I0525 12:58:22.203933 25608 solver.cpp:244]     Train net output #0: accuracy = 0.984211
I0525 12:58:22.203941 25608 solver.cpp:244]     Train net output #1: loss = 0.0873383 (* 1 = 0.0873383 loss)
I0525 12:58:22.203946 25608 sgd_solver.cpp:106] Iteration 100, lr = 0.01
I0525 12:58:37.521270 25608 solver.cpp:228] Iteration 120, loss = 0.0903041
I0525 12:58:37.521368 25608 solver.cpp:244]     Train net output #0: accuracy = 0.983346
I0525 12:58:37.521376 25608 solver.cpp:244]     Train net output #1: loss = 0.0903041 (* 1 = 0.0903041 loss)
I0525 12:58:37.521381 25608 sgd_solver.cpp:106] Iteration 120, lr = 0.01
I0525 12:58:53.011772 25608 solver.cpp:228] Iteration 140, loss = 0.0811502
I0525 12:58:53.011809 25608 solver.cpp:244]     Train net output #0: accuracy = 0.985258
I0525 12:58:53.011817 25608 solver.cpp:244]     Train net output #1: loss = 0.0811502 (* 1 = 0.0811502 loss)
I0525 12:58:53.011822 25608 sgd_solver.cpp:106] Iteration 140, lr = 0.01
I0525 12:59:08.638586 25608 solver.cpp:228] Iteration 160, loss = 0.0948375
I0525 12:59:08.638715 25608 solver.cpp:244]     Train net output #0: accuracy = 0.981776
I0525 12:59:08.638725 25608 solver.cpp:244]     Train net output #1: loss = 0.0948375 (* 1 = 0.0948375 loss)
I0525 12:59:08.638730 25608 sgd_solver.cpp:106] Iteration 160, lr = 0.01
I0525 12:59:24.180718 25608 solver.cpp:228] Iteration 180, loss = 0.0741496
I0525 12:59:24.180754 25608 solver.cpp:244]     Train net output #0: accuracy = 0.986581
I0525 12:59:24.180763 25608 solver.cpp:244]     Train net output #1: loss = 0.0741496 (* 1 = 0.0741496 loss)
I0525 12:59:24.180766 25608 sgd_solver.cpp:106] Iteration 180, lr = 0.01
I0525 12:59:39.096758 25608 solver.cpp:337] Iteration 200, Testing net (#0)
I0525 12:59:39.610772 25608 solver.cpp:404]     Test net output #0: accuracy = 0.981224
I0525 12:59:39.610807 25608 solver.cpp:404]     Test net output #1: loss = 0.0951829 (* 1 = 0.0951829 loss)
I0525 12:59:40.062022 25608 solver.cpp:228] Iteration 200, loss = 0.0824436
I0525 12:59:40.062047 25608 solver.cpp:244]     Train net output #0: accuracy = 0.984649
I0525 12:59:40.062053 25608 solver.cpp:244]     Train net output #1: loss = 0.0824436 (* 1 = 0.0824436 loss)
I0525 12:59:40.062058 25608 sgd_solver.cpp:106] Iteration 200, lr = 0.01
I0525 12:59:55.398736 25608 solver.cpp:228] Iteration 220, loss = 0.086515
I0525 12:59:55.398759 25608 solver.cpp:244]     Train net output #0: accuracy = 0.983487
I0525 12:59:55.398766 25608 solver.cpp:244]     Train net output #1: loss = 0.086515 (* 1 = 0.086515 loss)
I0525 12:59:55.398772 25608 sgd_solver.cpp:106] Iteration 220, lr = 0.01
I0525 13:00:10.733439 25608 solver.cpp:228] Iteration 240, loss = 0.0702947
I0525 13:00:10.733526 25608 solver.cpp:244]     Train net output #0: accuracy = 0.987343
I0525 13:00:10.733536 25608 solver.cpp:244]     Train net output #1: loss = 0.0702947 (* 1 = 0.0702947 loss)
I0525 13:00:10.733539 25608 sgd_solver.cpp:106] Iteration 240, lr = 0.01
I0525 13:00:26.052831 25608 solver.cpp:228] Iteration 260, loss = 0.0906434
I0525 13:00:26.052855 25608 solver.cpp:244]     Train net output #0: accuracy = 0.982137
I0525 13:00:26.052860 25608 solver.cpp:244]     Train net output #1: loss = 0.0906434 (* 1 = 0.0906434 loss)
I0525 13:00:26.052865 25608 sgd_solver.cpp:106] Iteration 260, lr = 0.01
I0525 13:00:41.376904 25608 solver.cpp:228] Iteration 280, loss = 0.0952192
I0525 13:00:41.376996 25608 solver.cpp:244]     Train net output #0: accuracy = 0.980891
I0525 13:00:41.377004 25608 solver.cpp:244]     Train net output #1: loss = 0.0952192 (* 1 = 0.0952192 loss)
I0525 13:00:41.377009 25608 sgd_solver.cpp:106] Iteration 280, lr = 0.01
I0525 13:00:56.252013 25608 solver.cpp:337] Iteration 300, Testing net (#0)
I0525 13:00:56.765887 25608 solver.cpp:404]     Test net output #0: accuracy = 0.980087
I0525 13:00:56.765921 25608 solver.cpp:404]     Test net output #1: loss = 0.0975134 (* 1 = 0.0975134 loss)
I0525 13:00:57.216639 25608 solver.cpp:228] Iteration 300, loss = 0.0816676
I0525 13:00:57.216662 25608 solver.cpp:244]     Train net output #0: accuracy = 0.984051
I0525 13:00:57.216670 25608 solver.cpp:244]     Train net output #1: loss = 0.0816676 (* 1 = 0.0816676 loss)
I0525 13:00:57.216675 25608 sgd_solver.cpp:106] Iteration 300, lr = 0.01
I0525 13:01:12.556150 25608 solver.cpp:228] Iteration 320, loss = 0.0878487
I0525 13:01:12.556257 25608 solver.cpp:244]     Train net output #0: accuracy = 0.982228
I0525 13:01:12.556267 25608 solver.cpp:244]     Train net output #1: loss = 0.0878487 (* 1 = 0.0878487 loss)
I0525 13:01:12.556272 25608 sgd_solver.cpp:106] Iteration 320, lr = 0.01
I0525 13:01:27.976553 25608 solver.cpp:228] Iteration 340, loss = 0.074876
I0525 13:01:27.976577 25608 solver.cpp:244]     Train net output #0: accuracy = 0.985502
I0525 13:01:27.976583 25608 solver.cpp:244]     Train net output #1: loss = 0.074876 (* 1 = 0.074876 loss)
I0525 13:01:27.976588 25608 sgd_solver.cpp:106] Iteration 340, lr = 0.01
I0525 13:01:43.403014 25608 solver.cpp:228] Iteration 360, loss = 0.0856661
I0525 13:01:43.403151 25608 solver.cpp:244]     Train net output #0: accuracy = 0.982783
I0525 13:01:43.403164 25608 solver.cpp:244]     Train net output #1: loss = 0.0856661 (* 1 = 0.0856661 loss)
I0525 13:01:43.403169 25608 sgd_solver.cpp:106] Iteration 360, lr = 0.01
I0525 13:01:58.854017 25608 solver.cpp:228] Iteration 380, loss = 0.076942
I0525 13:01:58.854041 25608 solver.cpp:244]     Train net output #0: accuracy = 0.984827
I0525 13:01:58.854048 25608 solver.cpp:244]     Train net output #1: loss = 0.076942 (* 1 = 0.076942 loss)
I0525 13:01:58.854053 25608 sgd_solver.cpp:106] Iteration 380, lr = 0.01
I0525 13:02:13.962190 25608 solver.cpp:337] Iteration 400, Testing net (#0)
I0525 13:02:14.498757 25608 solver.cpp:404]     Test net output #0: accuracy = 0.979509
I0525 13:02:14.498783 25608 solver.cpp:404]     Test net output #1: loss = 0.0963223 (* 1 = 0.0963223 loss)
I0525 13:02:14.957222 25608 solver.cpp:228] Iteration 400, loss = 0.101204
I0525 13:02:14.957247 25608 solver.cpp:244]     Train net output #0: accuracy = 0.97816
I0525 13:02:14.957254 25608 solver.cpp:244]     Train net output #1: loss = 0.101204 (* 1 = 0.101204 loss)
I0525 13:02:14.957259 25608 sgd_solver.cpp:106] Iteration 400, lr = 0.01
I0525 13:02:30.646407 25608 solver.cpp:228] Iteration 420, loss = 0.0899742
I0525 13:02:30.646446 25608 solver.cpp:244]     Train net output #0: accuracy = 0.98086
I0525 13:02:30.646455 25608 solver.cpp:244]     Train net output #1: loss = 0.0899742 (* 1 = 0.0899742 loss)
I0525 13:02:30.646459 25608 sgd_solver.cpp:106] Iteration 420, lr = 0.01
I0525 13:02:46.208580 25608 solver.cpp:228] Iteration 440, loss = 0.0723324
I0525 13:02:46.208688 25608 solver.cpp:244]     Train net output #0: accuracy = 0.985442
I0525 13:02:46.208698 25608 solver.cpp:244]     Train net output #1: loss = 0.0723324 (* 1 = 0.0723324 loss)
I0525 13:02:46.208703 25608 sgd_solver.cpp:106] Iteration 440, lr = 0.01
I0525 13:03:01.807757 25608 solver.cpp:228] Iteration 460, loss = 0.0883975
I0525 13:03:01.807796 25608 solver.cpp:244]     Train net output #0: accuracy = 0.980767
I0525 13:03:01.807802 25608 solver.cpp:244]     Train net output #1: loss = 0.0883975 (* 1 = 0.0883975 loss)
I0525 13:03:01.807808 25608 sgd_solver.cpp:106] Iteration 460, lr = 0.01
I0525 13:03:17.313403 25608 solver.cpp:228] Iteration 480, loss = 0.0720804
I0525 13:03:17.313494 25608 solver.cpp:244]     Train net output #0: accuracy = 0.985209
I0525 13:03:17.313503 25608 solver.cpp:244]     Train net output #1: loss = 0.0720804 (* 1 = 0.0720804 loss)
I0525 13:03:17.313508 25608 sgd_solver.cpp:106] Iteration 480, lr = 0.01
I0525 13:03:32.398615 25608 solver.cpp:454] Snapshotting to binary proto file data/models/segnet_iter_500.caffemodel
I0525 13:03:32.424392 25608 sgd_solver.cpp:273] Snapshotting solver state to binary proto file data/models/segnet_iter_500.solverstate
I0525 13:03:32.434206 25608 solver.cpp:337] Iteration 500, Testing net (#0)
I0525 13:03:32.957932 25608 solver.cpp:404]     Test net output #0: accuracy = 0.983647
I0525 13:03:32.957967 25608 solver.cpp:404]     Test net output #1: loss = 0.0757741 (* 1 = 0.0757741 loss)
I0525 13:03:33.411190 25608 solver.cpp:228] Iteration 500, loss = 0.0826758
I0525 13:03:33.411213 25608 solver.cpp:244]     Train net output #0: accuracy = 0.981919
I0525 13:03:33.411221 25608 solver.cpp:244]     Train net output #1: loss = 0.0826758 (* 1 = 0.0826758 loss)
I0525 13:03:33.411226 25608 sgd_solver.cpp:106] Iteration 500, lr = 0.01
I0525 13:03:48.893424 25608 solver.cpp:228] Iteration 520, loss = 0.0890739
I0525 13:03:48.893549 25608 solver.cpp:244]     Train net output #0: accuracy = 0.979823
I0525 13:03:48.893559 25608 solver.cpp:244]     Train net output #1: loss = 0.0890739 (* 1 = 0.0890739 loss)
I0525 13:03:48.893564 25608 sgd_solver.cpp:106] Iteration 520, lr = 0.01
I0525 13:04:04.222770 25608 solver.cpp:228] Iteration 540, loss = 0.0591938
I0525 13:04:04.222795 25608 solver.cpp:244]     Train net output #0: accuracy = 0.988611
I0525 13:04:04.222801 25608 solver.cpp:244]     Train net output #1: loss = 0.0591938 (* 1 = 0.0591938 loss)
I0525 13:04:04.222806 25608 sgd_solver.cpp:106] Iteration 540, lr = 0.01
I0525 13:04:19.647828 25608 solver.cpp:228] Iteration 560, loss = 0.0750732
I0525 13:04:19.647928 25608 solver.cpp:244]     Train net output #0: accuracy = 0.983403
I0525 13:04:19.647938 25608 solver.cpp:244]     Train net output #1: loss = 0.0750732 (* 1 = 0.0750732 loss)
I0525 13:04:19.647944 25608 sgd_solver.cpp:106] Iteration 560, lr = 0.01
I0525 13:04:35.117981 25608 solver.cpp:228] Iteration 580, loss = 0.0557035
I0525 13:04:35.118005 25608 solver.cpp:244]     Train net output #0: accuracy = 0.988851
I0525 13:04:35.118023 25608 solver.cpp:244]     Train net output #1: loss = 0.0557035 (* 1 = 0.0557035 loss)
I0525 13:04:35.118027 25608 sgd_solver.cpp:106] Iteration 580, lr = 0.01
I0525 13:04:49.978478 25608 solver.cpp:337] Iteration 600, Testing net (#0)
I0525 13:04:50.492646 25608 solver.cpp:404]     Test net output #0: accuracy = 0.986717
I0525 13:04:50.492681 25608 solver.cpp:404]     Test net output #1: loss = 0.0618091 (* 1 = 0.0618091 loss)
I0525 13:04:50.943274 25608 solver.cpp:228] Iteration 600, loss = 0.0825966
I0525 13:04:50.943296 25608 solver.cpp:244]     Train net output #0: accuracy = 0.980456
I0525 13:04:50.943303 25608 solver.cpp:244]     Train net output #1: loss = 0.0825966 (* 1 = 0.0825966 loss)
I0525 13:04:50.943307 25608 sgd_solver.cpp:106] Iteration 600, lr = 0.01
I0525 13:05:06.335572 25608 solver.cpp:228] Iteration 620, loss = 0.0667324
I0525 13:05:06.335597 25608 solver.cpp:244]     Train net output #0: accuracy = 0.984891
I0525 13:05:06.335603 25608 solver.cpp:244]     Train net output #1: loss = 0.0667324 (* 1 = 0.0667324 loss)
I0525 13:05:06.335608 25608 sgd_solver.cpp:106] Iteration 620, lr = 0.01
I0525 13:05:21.688874 25608 solver.cpp:228] Iteration 640, loss = 0.072578
I0525 13:05:21.688969 25608 solver.cpp:244]     Train net output #0: accuracy = 0.983437
I0525 13:05:21.688979 25608 solver.cpp:244]     Train net output #1: loss = 0.072578 (* 1 = 0.072578 loss)
I0525 13:05:21.688984 25608 sgd_solver.cpp:106] Iteration 640, lr = 0.01
I0525 13:05:36.991562 25608 solver.cpp:228] Iteration 660, loss = 0.0689849
I0525 13:05:36.991585 25608 solver.cpp:244]     Train net output #0: accuracy = 0.983698
I0525 13:05:36.991603 25608 solver.cpp:244]     Train net output #1: loss = 0.0689849 (* 1 = 0.0689849 loss)
I0525 13:05:36.991607 25608 sgd_solver.cpp:106] Iteration 660, lr = 0.01
I0525 13:05:52.321976 25608 solver.cpp:228] Iteration 680, loss = 0.0832415
I0525 13:05:52.322075 25608 solver.cpp:244]     Train net output #0: accuracy = 0.978984
I0525 13:05:52.322084 25608 solver.cpp:244]     Train net output #1: loss = 0.0832415 (* 1 = 0.0832415 loss)
I0525 13:05:52.322088 25608 sgd_solver.cpp:106] Iteration 680, lr = 0.01
I0525 13:06:07.196173 25608 solver.cpp:337] Iteration 700, Testing net (#0)
I0525 13:06:07.710417 25608 solver.cpp:404]     Test net output #0: accuracy = 0.983493
I0525 13:06:07.710450 25608 solver.cpp:404]     Test net output #1: loss = 0.0686444 (* 1 = 0.0686444 loss)
I0525 13:06:08.158838 25608 solver.cpp:228] Iteration 700, loss = 0.0731025
I0525 13:06:08.158861 25608 solver.cpp:244]     Train net output #0: accuracy = 0.981966
I0525 13:06:08.158869 25608 solver.cpp:244]     Train net output #1: loss = 0.0731025 (* 1 = 0.0731025 loss)
I0525 13:06:08.158874 25608 sgd_solver.cpp:106] Iteration 700, lr = 0.01
I0525 13:06:23.472136 25608 solver.cpp:228] Iteration 720, loss = 0.0707711
I0525 13:06:23.472225 25608 solver.cpp:244]     Train net output #0: accuracy = 0.983708
I0525 13:06:23.472235 25608 solver.cpp:244]     Train net output #1: loss = 0.0707711 (* 1 = 0.0707711 loss)
I0525 13:06:23.472240 25608 sgd_solver.cpp:106] Iteration 720, lr = 0.01
I0525 13:06:38.792960 25608 solver.cpp:228] Iteration 740, loss = 0.0588888
I0525 13:06:38.792985 25608 solver.cpp:244]     Train net output #0: accuracy = 0.986841
I0525 13:06:38.792992 25608 solver.cpp:244]     Train net output #1: loss = 0.0588888 (* 1 = 0.0588888 loss)
I0525 13:06:38.792996 25608 sgd_solver.cpp:106] Iteration 740, lr = 0.01
I0525 13:06:54.122458 25608 solver.cpp:228] Iteration 760, loss = 0.0656982
I0525 13:06:54.122584 25608 solver.cpp:244]     Train net output #0: accuracy = 0.984827
I0525 13:06:54.122594 25608 solver.cpp:244]     Train net output #1: loss = 0.0656982 (* 1 = 0.0656982 loss)
I0525 13:06:54.122599 25608 sgd_solver.cpp:106] Iteration 760, lr = 0.01
I0525 13:07:09.458261 25608 solver.cpp:228] Iteration 780, loss = 0.0611759
I0525 13:07:09.458286 25608 solver.cpp:244]     Train net output #0: accuracy = 0.984852
I0525 13:07:09.458292 25608 solver.cpp:244]     Train net output #1: loss = 0.0611759 (* 1 = 0.0611759 loss)
I0525 13:07:09.458297 25608 sgd_solver.cpp:106] Iteration 780, lr = 0.01
I0525 13:07:24.388125 25608 solver.cpp:337] Iteration 800, Testing net (#0)
I0525 13:07:24.903388 25608 solver.cpp:404]     Test net output #0: accuracy = 0.983241
I0525 13:07:24.903411 25608 solver.cpp:404]     Test net output #1: loss = 0.0669834 (* 1 = 0.0669834 loss)
I0525 13:07:25.355357 25608 solver.cpp:228] Iteration 800, loss = 0.0904383
I0525 13:07:25.355392 25608 solver.cpp:244]     Train net output #0: accuracy = 0.9768
I0525 13:07:25.355399 25608 solver.cpp:244]     Train net output #1: loss = 0.0904383 (* 1 = 0.0904383 loss)
I0525 13:07:25.355404 25608 sgd_solver.cpp:106] Iteration 800, lr = 0.01
I0525 13:07:40.702893 25608 solver.cpp:228] Iteration 820, loss = 0.0609804
I0525 13:07:40.702918 25608 solver.cpp:244]     Train net output #0: accuracy = 0.985154
I0525 13:07:40.702925 25608 solver.cpp:244]     Train net output #1: loss = 0.0609804 (* 1 = 0.0609804 loss)
I0525 13:07:40.702930 25608 sgd_solver.cpp:106] Iteration 820, lr = 0.01
I0525 13:07:56.040752 25608 solver.cpp:228] Iteration 840, loss = 0.0600429
I0525 13:07:56.040843 25608 solver.cpp:244]     Train net output #0: accuracy = 0.985661
I0525 13:07:56.040853 25608 solver.cpp:244]     Train net output #1: loss = 0.0600429 (* 1 = 0.0600429 loss)
I0525 13:07:56.040858 25608 sgd_solver.cpp:106] Iteration 840, lr = 0.01
I0525 13:08:11.387544 25608 solver.cpp:228] Iteration 860, loss = 0.0628967
I0525 13:08:11.387568 25608 solver.cpp:244]     Train net output #0: accuracy = 0.984387
I0525 13:08:11.387575 25608 solver.cpp:244]     Train net output #1: loss = 0.0628967 (* 1 = 0.0628967 loss)
I0525 13:08:11.387580 25608 sgd_solver.cpp:106] Iteration 860, lr = 0.01
I0525 13:08:26.716855 25608 solver.cpp:228] Iteration 880, loss = 0.0816186
I0525 13:08:26.716948 25608 solver.cpp:244]     Train net output #0: accuracy = 0.977882
I0525 13:08:26.716964 25608 solver.cpp:244]     Train net output #1: loss = 0.0816186 (* 1 = 0.0816186 loss)
I0525 13:08:26.716969 25608 sgd_solver.cpp:106] Iteration 880, lr = 0.01
I0525 13:08:41.593685 25608 solver.cpp:337] Iteration 900, Testing net (#0)
I0525 13:08:42.107606 25608 solver.cpp:404]     Test net output #0: accuracy = 0.98468
I0525 13:08:42.107630 25608 solver.cpp:404]     Test net output #1: loss = 0.0610218 (* 1 = 0.0610218 loss)
I0525 13:08:42.557974 25608 solver.cpp:228] Iteration 900, loss = 0.0584878
I0525 13:08:42.558008 25608 solver.cpp:244]     Train net output #0: accuracy = 0.985969
I0525 13:08:42.558015 25608 solver.cpp:244]     Train net output #1: loss = 0.0584878 (* 1 = 0.0584878 loss)
I0525 13:08:42.558020 25608 sgd_solver.cpp:106] Iteration 900, lr = 0.01
I0525 13:08:57.883010 25608 solver.cpp:228] Iteration 920, loss = 0.0511487
I0525 13:08:57.883117 25608 solver.cpp:244]     Train net output #0: accuracy = 0.987339
I0525 13:08:57.883126 25608 solver.cpp:244]     Train net output #1: loss = 0.0511487 (* 1 = 0.0511487 loss)
I0525 13:08:57.883131 25608 sgd_solver.cpp:106] Iteration 920, lr = 0.01
I0525 13:09:13.205778 25608 solver.cpp:228] Iteration 940, loss = 0.0564051
I0525 13:09:13.205801 25608 solver.cpp:244]     Train net output #0: accuracy = 0.985856
I0525 13:09:13.205808 25608 solver.cpp:244]     Train net output #1: loss = 0.0564051 (* 1 = 0.0564051 loss)
I0525 13:09:13.205813 25608 sgd_solver.cpp:106] Iteration 940, lr = 0.01
I0525 13:09:28.648968 25608 solver.cpp:228] Iteration 960, loss = 0.0630138
I0525 13:09:28.649082 25608 solver.cpp:244]     Train net output #0: accuracy = 0.984354
I0525 13:09:28.649092 25608 solver.cpp:244]     Train net output #1: loss = 0.0630138 (* 1 = 0.0630138 loss)
I0525 13:09:28.649097 25608 sgd_solver.cpp:106] Iteration 960, lr = 0.01
I0525 13:09:44.386559 25608 solver.cpp:228] Iteration 980, loss = 0.0654779
I0525 13:09:44.386582 25608 solver.cpp:244]     Train net output #0: accuracy = 0.982127
I0525 13:09:44.386590 25608 solver.cpp:244]     Train net output #1: loss = 0.0654779 (* 1 = 0.0654779 loss)
I0525 13:09:44.386595 25608 sgd_solver.cpp:106] Iteration 980, lr = 0.01
I0525 13:09:59.274993 25608 solver.cpp:454] Snapshotting to binary proto file data/models/segnet_iter_1000.caffemodel
I0525 13:09:59.294548 25608 sgd_solver.cpp:273] Snapshotting solver state to binary proto file data/models/segnet_iter_1000.solverstate
I0525 13:09:59.303885 25608 solver.cpp:337] Iteration 1000, Testing net (#0)
I0525 13:09:59.819813 25608 solver.cpp:404]     Test net output #0: accuracy = 0.982671
I0525 13:09:59.819836 25608 solver.cpp:404]     Test net output #1: loss = 0.0626415 (* 1 = 0.0626415 loss)
I0525 13:10:00.270912 25608 solver.cpp:228] Iteration 1000, loss = 0.0539063
I0525 13:10:00.270936 25608 solver.cpp:244]     Train net output #0: accuracy = 0.986735
I0525 13:10:00.270943 25608 solver.cpp:244]     Train net output #1: loss = 0.0539063 (* 1 = 0.0539063 loss)
I0525 13:10:00.270948 25608 sgd_solver.cpp:106] Iteration 1000, lr = 0.01
I0525 13:10:15.752892 25608 solver.cpp:228] Iteration 1020, loss = 0.0631737
I0525 13:10:15.752926 25608 solver.cpp:244]     Train net output #0: accuracy = 0.982567
I0525 13:10:15.752933 25608 solver.cpp:244]     Train net output #1: loss = 0.0631737 (* 1 = 0.0631737 loss)
I0525 13:10:15.752938 25608 sgd_solver.cpp:106] Iteration 1020, lr = 0.01
I0525 13:10:31.349889 25608 solver.cpp:228] Iteration 1040, loss = 0.060285
I0525 13:10:31.349987 25608 solver.cpp:244]     Train net output #0: accuracy = 0.983767
I0525 13:10:31.349997 25608 solver.cpp:244]     Train net output #1: loss = 0.060285 (* 1 = 0.060285 loss)
I0525 13:10:31.350002 25608 sgd_solver.cpp:106] Iteration 1040, lr = 0.01
I0525 13:10:47.025147 25608 solver.cpp:228] Iteration 1060, loss = 0.0680739
I0525 13:10:47.025183 25608 solver.cpp:244]     Train net output #0: accuracy = 0.980569
I0525 13:10:47.025192 25608 solver.cpp:244]     Train net output #1: loss = 0.0680739 (* 1 = 0.0680739 loss)
I0525 13:10:47.025195 25608 sgd_solver.cpp:106] Iteration 1060, lr = 0.01
I0525 13:11:02.730120 25608 solver.cpp:228] Iteration 1080, loss = 0.0565341
I0525 13:11:02.730216 25608 solver.cpp:244]     Train net output #0: accuracy = 0.984679
I0525 13:11:02.730226 25608 solver.cpp:244]     Train net output #1: loss = 0.0565341 (* 1 = 0.0565341 loss)
I0525 13:11:02.730231 25608 sgd_solver.cpp:106] Iteration 1080, lr = 0.01
I0525 13:11:17.703016 25608 solver.cpp:337] Iteration 1100, Testing net (#0)
I0525 13:11:17.968813 25608 blocking_queue.cpp:50] Data layer prefetch queue empty
I0525 13:11:18.250419 25608 solver.cpp:404]     Test net output #0: accuracy = 0.986691
I0525 13:11:18.250447 25608 solver.cpp:404]     Test net output #1: loss = 0.0508041 (* 1 = 0.0508041 loss)
I0525 13:11:18.711874 25608 solver.cpp:228] Iteration 1100, loss = 0.0527411
I0525 13:11:18.711899 25608 solver.cpp:244]     Train net output #0: accuracy = 0.985883
I0525 13:11:18.711906 25608 solver.cpp:244]     Train net output #1: loss = 0.0527411 (* 1 = 0.0527411 loss)
I0525 13:11:18.711910 25608 sgd_solver.cpp:106] Iteration 1100, lr = 0.01
I0525 13:11:34.344741 25608 solver.cpp:228] Iteration 1120, loss = 0.0645924
I0525 13:11:34.344840 25608 solver.cpp:244]     Train net output #0: accuracy = 0.980586
I0525 13:11:34.344849 25608 solver.cpp:244]     Train net output #1: loss = 0.0645924 (* 1 = 0.0645924 loss)
I0525 13:11:34.344854 25608 sgd_solver.cpp:106] Iteration 1120, lr = 0.01
I0525 13:11:49.819378 25608 solver.cpp:228] Iteration 1140, loss = 0.0549264
I0525 13:11:49.819401 25608 solver.cpp:244]     Train net output #0: accuracy = 0.984941
I0525 13:11:49.819408 25608 solver.cpp:244]     Train net output #1: loss = 0.0549264 (* 1 = 0.0549264 loss)
I0525 13:11:49.819413 25608 sgd_solver.cpp:106] Iteration 1140, lr = 0.01
I0525 13:12:05.213419 25608 solver.cpp:228] Iteration 1160, loss = 0.0618919
I0525 13:12:05.213516 25608 solver.cpp:244]     Train net output #0: accuracy = 0.981909
I0525 13:12:05.213526 25608 solver.cpp:244]     Train net output #1: loss = 0.0618919 (* 1 = 0.0618919 loss)
I0525 13:12:05.213531 25608 sgd_solver.cpp:106] Iteration 1160, lr = 0.01
I0525 13:12:20.567303 25608 solver.cpp:228] Iteration 1180, loss = 0.06476
I0525 13:12:20.567332 25608 solver.cpp:244]     Train net output #0: accuracy = 0.981158
I0525 13:12:20.567340 25608 solver.cpp:244]     Train net output #1: loss = 0.06476 (* 1 = 0.06476 loss)
I0525 13:12:20.567345 25608 sgd_solver.cpp:106] Iteration 1180, lr = 0.01
I0525 13:12:35.775099 25608 solver.cpp:337] Iteration 1200, Testing net (#0)
I0525 13:12:36.299468 25608 solver.cpp:404]     Test net output #0: accuracy = 0.98263
I0525 13:12:36.299492 25608 solver.cpp:404]     Test net output #1: loss = 0.0601835 (* 1 = 0.0601835 loss)
I0525 13:12:36.756024 25608 solver.cpp:228] Iteration 1200, loss = 0.0598305
I0525 13:12:36.756059 25608 solver.cpp:244]     Train net output #0: accuracy = 0.984205
I0525 13:12:36.756067 25608 solver.cpp:244]     Train net output #1: loss = 0.0598305 (* 1 = 0.0598305 loss)
I0525 13:12:36.756072 25608 sgd_solver.cpp:106] Iteration 1200, lr = 0.01
I0525 13:12:52.469444 25608 solver.cpp:228] Iteration 1220, loss = 0.0587468
I0525 13:12:52.469467 25608 solver.cpp:244]     Train net output #0: accuracy = 0.982687
I0525 13:12:52.469475 25608 solver.cpp:244]     Train net output #1: loss = 0.0587468 (* 1 = 0.0587468 loss)
I0525 13:12:52.469480 25608 sgd_solver.cpp:106] Iteration 1220, lr = 0.01
I0525 13:13:07.983870 25608 solver.cpp:228] Iteration 1240, loss = 0.0594177
I0525 13:13:07.983963 25608 solver.cpp:244]     Train net output #0: accuracy = 0.982678
I0525 13:13:07.983973 25608 solver.cpp:244]     Train net output #1: loss = 0.0594177 (* 1 = 0.0594177 loss)
I0525 13:13:07.983978 25608 sgd_solver.cpp:106] Iteration 1240, lr = 0.01
I0525 13:13:23.465324 25608 solver.cpp:228] Iteration 1260, loss = 0.0480491
I0525 13:13:23.465353 25608 solver.cpp:244]     Train net output #0: accuracy = 0.986453
I0525 13:13:23.465360 25608 solver.cpp:244]     Train net output #1: loss = 0.0480491 (* 1 = 0.0480491 loss)
I0525 13:13:23.465364 25608 sgd_solver.cpp:106] Iteration 1260, lr = 0.01
I0525 13:13:39.024035 25608 solver.cpp:228] Iteration 1280, loss = 0.063284
I0525 13:13:39.024122 25608 solver.cpp:244]     Train net output #0: accuracy = 0.981405
I0525 13:13:39.024132 25608 solver.cpp:244]     Train net output #1: loss = 0.063284 (* 1 = 0.063284 loss)
I0525 13:13:39.024137 25608 sgd_solver.cpp:106] Iteration 1280, lr = 0.01
I0525 13:13:54.550264 25608 solver.cpp:337] Iteration 1300, Testing net (#0)
I0525 13:13:55.065381 25608 solver.cpp:404]     Test net output #0: accuracy = 0.985978
I0525 13:13:55.065417 25608 solver.cpp:404]     Test net output #1: loss = 0.0500117 (* 1 = 0.0500117 loss)
I0525 13:13:55.514724 25608 solver.cpp:228] Iteration 1300, loss = 0.0496323
I0525 13:13:55.514750 25608 solver.cpp:244]     Train net output #0: accuracy = 0.98642
I0525 13:13:55.514756 25608 solver.cpp:244]     Train net output #1: loss = 0.0496323 (* 1 = 0.0496323 loss)
I0525 13:13:55.514760 25608 sgd_solver.cpp:106] Iteration 1300, lr = 0.01
I0525 13:14:10.916712 25608 solver.cpp:228] Iteration 1320, loss = 0.046188
I0525 13:14:10.916833 25608 solver.cpp:244]     Train net output #0: accuracy = 0.98759
I0525 13:14:10.916843 25608 solver.cpp:244]     Train net output #1: loss = 0.046188 (* 1 = 0.046188 loss)
I0525 13:14:10.916847 25608 sgd_solver.cpp:106] Iteration 1320, lr = 0.01
I0525 13:14:26.282680 25608 solver.cpp:228] Iteration 1340, loss = 0.0550578
I0525 13:14:26.282704 25608 solver.cpp:244]     Train net output #0: accuracy = 0.984767
I0525 13:14:26.282711 25608 solver.cpp:244]     Train net output #1: loss = 0.0550578 (* 1 = 0.0550578 loss)
I0525 13:14:26.282716 25608 sgd_solver.cpp:106] Iteration 1340, lr = 0.01
I0525 13:14:41.589393 25608 solver.cpp:228] Iteration 1360, loss = 0.0562448
I0525 13:14:41.589493 25608 solver.cpp:244]     Train net output #0: accuracy = 0.983958
I0525 13:14:41.589503 25608 solver.cpp:244]     Train net output #1: loss = 0.0562448 (* 1 = 0.0562448 loss)
I0525 13:14:41.589507 25608 sgd_solver.cpp:106] Iteration 1360, lr = 0.01
I0525 13:14:56.894006 25608 solver.cpp:228] Iteration 1380, loss = 0.0575857
I0525 13:14:56.894031 25608 solver.cpp:244]     Train net output #0: accuracy = 0.982329
I0525 13:14:56.894037 25608 solver.cpp:244]     Train net output #1: loss = 0.0575857 (* 1 = 0.0575857 loss)
I0525 13:14:56.894042 25608 sgd_solver.cpp:106] Iteration 1380, lr = 0.01
I0525 13:15:11.750951 25608 solver.cpp:337] Iteration 1400, Testing net (#0)
I0525 13:15:12.264943 25608 solver.cpp:404]     Test net output #0: accuracy = 0.984211
I0525 13:15:12.264967 25608 solver.cpp:404]     Test net output #1: loss = 0.0526589 (* 1 = 0.0526589 loss)
I0525 13:15:12.714494 25608 solver.cpp:228] Iteration 1400, loss = 0.0651155
I0525 13:15:12.714530 25608 solver.cpp:244]     Train net output #0: accuracy = 0.980095
I0525 13:15:12.714537 25608 solver.cpp:244]     Train net output #1: loss = 0.0651155 (* 1 = 0.0651155 loss)
I0525 13:15:12.714542 25608 sgd_solver.cpp:106] Iteration 1400, lr = 0.01
I0525 13:15:28.032250 25608 solver.cpp:228] Iteration 1420, loss = 0.0481141
I0525 13:15:28.032274 25608 solver.cpp:244]     Train net output #0: accuracy = 0.986819
I0525 13:15:28.032282 25608 solver.cpp:244]     Train net output #1: loss = 0.0481141 (* 1 = 0.0481141 loss)
I0525 13:15:28.032286 25608 sgd_solver.cpp:106] Iteration 1420, lr = 0.01
I0525 13:15:43.338883 25608 solver.cpp:228] Iteration 1440, loss = 0.0581692
I0525 13:15:43.338973 25608 solver.cpp:244]     Train net output #0: accuracy = 0.982238
I0525 13:15:43.338982 25608 solver.cpp:244]     Train net output #1: loss = 0.0581692 (* 1 = 0.0581692 loss)
I0525 13:15:43.338987 25608 sgd_solver.cpp:106] Iteration 1440, lr = 0.01
I0525 13:15:58.651427 25608 solver.cpp:228] Iteration 1460, loss = 0.046821
I0525 13:15:58.651450 25608 solver.cpp:244]     Train net output #0: accuracy = 0.986355
I0525 13:15:58.651458 25608 solver.cpp:244]     Train net output #1: loss = 0.046821 (* 1 = 0.046821 loss)
I0525 13:15:58.651463 25608 sgd_solver.cpp:106] Iteration 1460, lr = 0.01
I0525 13:16:13.958719 25608 solver.cpp:228] Iteration 1480, loss = 0.0535148
I0525 13:16:13.958811 25608 solver.cpp:244]     Train net output #0: accuracy = 0.983354
I0525 13:16:13.958819 25608 solver.cpp:244]     Train net output #1: loss = 0.0535148 (* 1 = 0.0535148 loss)
I0525 13:16:13.958824 25608 sgd_solver.cpp:106] Iteration 1480, lr = 0.01
I0525 13:16:28.818028 25608 solver.cpp:454] Snapshotting to binary proto file data/models/segnet_iter_1500.caffemodel
I0525 13:16:28.836015 25608 sgd_solver.cpp:273] Snapshotting solver state to binary proto file data/models/segnet_iter_1500.solverstate
I0525 13:16:28.845202 25608 solver.cpp:337] Iteration 1500, Testing net (#0)
I0525 13:16:29.362336 25608 solver.cpp:404]     Test net output #0: accuracy = 0.983848
I0525 13:16:29.362371 25608 solver.cpp:404]     Test net output #1: loss = 0.0554002 (* 1 = 0.0554002 loss)
I0525 13:16:29.813078 25608 solver.cpp:228] Iteration 1500, loss = 0.062804
I0525 13:16:29.813102 25608 solver.cpp:244]     Train net output #0: accuracy = 0.98089
I0525 13:16:29.813109 25608 solver.cpp:244]     Train net output #1: loss = 0.062804 (* 1 = 0.062804 loss)
I0525 13:16:29.813113 25608 sgd_solver.cpp:106] Iteration 1500, lr = 0.01
I0525 13:16:45.119258 25608 solver.cpp:228] Iteration 1520, loss = 0.0446269
I0525 13:16:45.119380 25608 solver.cpp:244]     Train net output #0: accuracy = 0.987199
I0525 13:16:45.119390 25608 solver.cpp:244]     Train net output #1: loss = 0.0446269 (* 1 = 0.0446269 loss)
I0525 13:16:45.119395 25608 sgd_solver.cpp:106] Iteration 1520, lr = 0.01
I0525 13:17:00.428710 25608 solver.cpp:228] Iteration 1540, loss = 0.0577354
I0525 13:17:00.428733 25608 solver.cpp:244]     Train net output #0: accuracy = 0.981699
I0525 13:17:00.428740 25608 solver.cpp:244]     Train net output #1: loss = 0.0577354 (* 1 = 0.0577354 loss)
I0525 13:17:00.428745 25608 sgd_solver.cpp:106] Iteration 1540, lr = 0.01
I0525 13:17:15.748874 25608 solver.cpp:228] Iteration 1560, loss = 0.0384977
I0525 13:17:15.748965 25608 solver.cpp:244]     Train net output #0: accuracy = 0.989315
I0525 13:17:15.748975 25608 solver.cpp:244]     Train net output #1: loss = 0.0384977 (* 1 = 0.0384977 loss)
I0525 13:17:15.748980 25608 sgd_solver.cpp:106] Iteration 1560, lr = 0.01
I0525 13:17:31.134903 25608 solver.cpp:228] Iteration 1580, loss = 0.0589148
I0525 13:17:31.134928 25608 solver.cpp:244]     Train net output #0: accuracy = 0.98116
I0525 13:17:31.134937 25608 solver.cpp:244]     Train net output #1: loss = 0.0589148 (* 1 = 0.0589148 loss)
I0525 13:17:31.134940 25608 sgd_solver.cpp:106] Iteration 1580, lr = 0.01
I0525 13:17:46.000288 25608 solver.cpp:337] Iteration 1600, Testing net (#0)
I0525 13:17:46.523046 25608 solver.cpp:404]     Test net output #0: accuracy = 0.979679
I0525 13:17:46.523071 25608 solver.cpp:404]     Test net output #1: loss = 0.0611867 (* 1 = 0.0611867 loss)
I0525 13:17:46.974887 25608 solver.cpp:228] Iteration 1600, loss = 0.0547548
I0525 13:17:46.974922 25608 solver.cpp:244]     Train net output #0: accuracy = 0.983316
I0525 13:17:46.974928 25608 solver.cpp:244]     Train net output #1: loss = 0.0547548 (* 1 = 0.0547548 loss)
I0525 13:17:46.974933 25608 sgd_solver.cpp:106] Iteration 1600, lr = 0.01
I0525 13:18:02.277127 25608 solver.cpp:228] Iteration 1620, loss = 0.0679826
I0525 13:18:02.277151 25608 solver.cpp:244]     Train net output #0: accuracy = 0.978715
I0525 13:18:02.277158 25608 solver.cpp:244]     Train net output #1: loss = 0.0679826 (* 1 = 0.0679826 loss)
I0525 13:18:02.277163 25608 sgd_solver.cpp:106] Iteration 1620, lr = 0.01
I0525 13:18:17.589378 25608 solver.cpp:228] Iteration 1640, loss = 0.0649355
I0525 13:18:17.589483 25608 solver.cpp:244]     Train net output #0: accuracy = 0.978575
I0525 13:18:17.589491 25608 solver.cpp:244]     Train net output #1: loss = 0.0649355 (* 1 = 0.0649355 loss)
I0525 13:18:17.589496 25608 sgd_solver.cpp:106] Iteration 1640, lr = 0.01
I0525 13:18:32.879295 25608 solver.cpp:228] Iteration 1660, loss = 0.0581263
I0525 13:18:32.879333 25608 solver.cpp:244]     Train net output #0: accuracy = 0.980335
I0525 13:18:32.879341 25608 solver.cpp:244]     Train net output #1: loss = 0.0581263 (* 1 = 0.0581263 loss)
I0525 13:18:32.879346 25608 sgd_solver.cpp:106] Iteration 1660, lr = 0.01
I0525 13:18:48.165434 25608 solver.cpp:228] Iteration 1680, loss = 0.0630048
I0525 13:18:48.165542 25608 solver.cpp:244]     Train net output #0: accuracy = 0.978773
I0525 13:18:48.165551 25608 solver.cpp:244]     Train net output #1: loss = 0.0630048 (* 1 = 0.0630048 loss)
I0525 13:18:48.165556 25608 sgd_solver.cpp:106] Iteration 1680, lr = 0.01
I0525 13:19:03.005180 25608 solver.cpp:337] Iteration 1700, Testing net (#0)
I0525 13:19:03.518462 25608 solver.cpp:404]     Test net output #0: accuracy = 0.982851
I0525 13:19:03.518486 25608 solver.cpp:404]     Test net output #1: loss = 0.0522677 (* 1 = 0.0522677 loss)
I0525 13:19:03.969640 25608 solver.cpp:228] Iteration 1700, loss = 0.0495739
I0525 13:19:03.969665 25608 solver.cpp:244]     Train net output #0: accuracy = 0.985817
I0525 13:19:03.969671 25608 solver.cpp:244]     Train net output #1: loss = 0.0495739 (* 1 = 0.0495739 loss)
I0525 13:19:03.969676 25608 sgd_solver.cpp:106] Iteration 1700, lr = 0.01
I0525 13:19:19.268034 25608 solver.cpp:228] Iteration 1720, loss = 0.0463697
I0525 13:19:19.268157 25608 solver.cpp:244]     Train net output #0: accuracy = 0.985552
I0525 13:19:19.268167 25608 solver.cpp:244]     Train net output #1: loss = 0.0463697 (* 1 = 0.0463697 loss)
I0525 13:19:19.268172 25608 sgd_solver.cpp:106] Iteration 1720, lr = 0.01
I0525 13:19:34.990768 25608 solver.cpp:228] Iteration 1740, loss = 0.0429604
I0525 13:19:34.990803 25608 solver.cpp:244]     Train net output #0: accuracy = 0.986803
I0525 13:19:34.990810 25608 solver.cpp:244]     Train net output #1: loss = 0.0429604 (* 1 = 0.0429604 loss)
I0525 13:19:34.990815 25608 sgd_solver.cpp:106] Iteration 1740, lr = 0.01
I0525 13:19:50.316395 25608 solver.cpp:228] Iteration 1760, loss = 0.0499813
I0525 13:19:50.316495 25608 solver.cpp:244]     Train net output #0: accuracy = 0.983606
I0525 13:19:50.316504 25608 solver.cpp:244]     Train net output #1: loss = 0.0499813 (* 1 = 0.0499813 loss)
I0525 13:19:50.316509 25608 sgd_solver.cpp:106] Iteration 1760, lr = 0.01
I0525 13:20:05.645931 25608 solver.cpp:228] Iteration 1780, loss = 0.0535583
I0525 13:20:05.645953 25608 solver.cpp:244]     Train net output #0: accuracy = 0.982552
I0525 13:20:05.645961 25608 solver.cpp:244]     Train net output #1: loss = 0.0535583 (* 1 = 0.0535583 loss)
I0525 13:20:05.645965 25608 sgd_solver.cpp:106] Iteration 1780, lr = 0.01
I0525 13:20:20.521358 25608 solver.cpp:337] Iteration 1800, Testing net (#0)
I0525 13:20:21.036010 25608 solver.cpp:404]     Test net output #0: accuracy = 0.983269
I0525 13:20:21.036033 25608 solver.cpp:404]     Test net output #1: loss = 0.051641 (* 1 = 0.051641 loss)
I0525 13:20:21.486915 25608 solver.cpp:228] Iteration 1800, loss = 0.0482528
I0525 13:20:21.486937 25608 solver.cpp:244]     Train net output #0: accuracy = 0.984939
I0525 13:20:21.486944 25608 solver.cpp:244]     Train net output #1: loss = 0.0482528 (* 1 = 0.0482528 loss)
I0525 13:20:21.486948 25608 sgd_solver.cpp:106] Iteration 1800, lr = 0.01
I0525 13:20:36.818116 25608 solver.cpp:228] Iteration 1820, loss = 0.0498171
I0525 13:20:36.818141 25608 solver.cpp:244]     Train net output #0: accuracy = 0.984791
I0525 13:20:36.818148 25608 solver.cpp:244]     Train net output #1: loss = 0.0498171 (* 1 = 0.0498171 loss)
I0525 13:20:36.818153 25608 sgd_solver.cpp:106] Iteration 1820, lr = 0.01
I0525 13:20:52.132349 25608 solver.cpp:228] Iteration 1840, loss = 0.0442894
I0525 13:20:52.132446 25608 solver.cpp:244]     Train net output #0: accuracy = 0.986335
I0525 13:20:52.132454 25608 solver.cpp:244]     Train net output #1: loss = 0.0442894 (* 1 = 0.0442894 loss)
I0525 13:20:52.132459 25608 sgd_solver.cpp:106] Iteration 1840, lr = 0.01
I0525 13:21:07.470788 25608 solver.cpp:228] Iteration 1860, loss = 0.0524191
I0525 13:21:07.470810 25608 solver.cpp:244]     Train net output #0: accuracy = 0.982942
I0525 13:21:07.470818 25608 solver.cpp:244]     Train net output #1: loss = 0.0524191 (* 1 = 0.0524191 loss)
I0525 13:21:07.470823 25608 sgd_solver.cpp:106] Iteration 1860, lr = 0.01
I0525 13:21:22.843046 25608 solver.cpp:228] Iteration 1880, loss = 0.0495351
I0525 13:21:22.843118 25608 solver.cpp:244]     Train net output #0: accuracy = 0.984202
I0525 13:21:22.843128 25608 solver.cpp:244]     Train net output #1: loss = 0.0495351 (* 1 = 0.0495351 loss)
I0525 13:21:22.843133 25608 sgd_solver.cpp:106] Iteration 1880, lr = 0.01
I0525 13:21:37.814748 25608 solver.cpp:337] Iteration 1900, Testing net (#0)
I0525 13:21:38.330763 25608 solver.cpp:404]     Test net output #0: accuracy = 0.979716
I0525 13:21:38.330787 25608 solver.cpp:404]     Test net output #1: loss = 0.0585916 (* 1 = 0.0585916 loss)
I0525 13:21:38.781775 25608 solver.cpp:228] Iteration 1900, loss = 0.0598078
I0525 13:21:38.781800 25608 solver.cpp:244]     Train net output #0: accuracy = 0.981268
I0525 13:21:38.781807 25608 solver.cpp:244]     Train net output #1: loss = 0.0598078 (* 1 = 0.0598078 loss)
I0525 13:21:38.781812 25608 sgd_solver.cpp:106] Iteration 1900, lr = 0.01
I0525 13:21:54.139026 25608 solver.cpp:228] Iteration 1920, loss = 0.0468826
I0525 13:21:54.139138 25608 solver.cpp:244]     Train net output #0: accuracy = 0.984992
I0525 13:21:54.139148 25608 solver.cpp:244]     Train net output #1: loss = 0.0468826 (* 1 = 0.0468826 loss)
I0525 13:21:54.139153 25608 sgd_solver.cpp:106] Iteration 1920, lr = 0.01
I0525 13:22:09.499132 25608 solver.cpp:228] Iteration 1940, loss = 0.0629951
I0525 13:22:09.499156 25608 solver.cpp:244]     Train net output #0: accuracy = 0.980569
I0525 13:22:09.499163 25608 solver.cpp:244]     Train net output #1: loss = 0.0629951 (* 1 = 0.0629951 loss)
I0525 13:22:09.499168 25608 sgd_solver.cpp:106] Iteration 1940, lr = 0.01
I0525 13:22:24.853137 25608 solver.cpp:228] Iteration 1960, loss = 0.0453047
I0525 13:22:24.853247 25608 solver.cpp:244]     Train net output #0: accuracy = 0.985898
I0525 13:22:24.853257 25608 solver.cpp:244]     Train net output #1: loss = 0.0453047 (* 1 = 0.0453047 loss)
I0525 13:22:24.853261 25608 sgd_solver.cpp:106] Iteration 1960, lr = 0.01
I0525 13:22:40.192443 25608 solver.cpp:228] Iteration 1980, loss = 0.0449473
I0525 13:22:40.192467 25608 solver.cpp:244]     Train net output #0: accuracy = 0.986892
I0525 13:22:40.192473 25608 solver.cpp:244]     Train net output #1: loss = 0.0449473 (* 1 = 0.0449473 loss)
I0525 13:22:40.192478 25608 sgd_solver.cpp:106] Iteration 1980, lr = 0.01
I0525 13:22:55.092294 25608 solver.cpp:454] Snapshotting to binary proto file data/models/segnet_iter_2000.caffemodel
I0525 13:22:55.110198 25608 sgd_solver.cpp:273] Snapshotting solver state to binary proto file data/models/segnet_iter_2000.solverstate
I0525 13:22:55.131436 25608 solver.cpp:337] Iteration 2000, Testing net (#0)
I0525 13:22:55.696197 25608 solver.cpp:404]     Test net output #0: accuracy = 0.983098
I0525 13:22:55.696224 25608 solver.cpp:404]     Test net output #1: loss = 0.0560028 (* 1 = 0.0560028 loss)
I0525 13:22:56.159677 25608 solver.cpp:228] Iteration 2000, loss = 0.0464199
I0525 13:22:56.159705 25608 solver.cpp:244]     Train net output #0: accuracy = 0.983814
I0525 13:22:56.159713 25608 solver.cpp:244]     Train net output #1: loss = 0.0464199 (* 1 = 0.0464199 loss)
I0525 13:22:56.159718 25608 sgd_solver.cpp:106] Iteration 2000, lr = 0.01
I0525 13:23:11.509994 25608 solver.cpp:228] Iteration 2020, loss = 0.0519433
I0525 13:23:11.510016 25608 solver.cpp:244]     Train net output #0: accuracy = 0.98302
I0525 13:23:11.510023 25608 solver.cpp:244]     Train net output #1: loss = 0.0519433 (* 1 = 0.0519433 loss)
I0525 13:23:11.510028 25608 sgd_solver.cpp:106] Iteration 2020, lr = 0.01
I0525 13:23:26.827263 25608 solver.cpp:228] Iteration 2040, loss = 0.0553515
I0525 13:23:26.827368 25608 solver.cpp:244]     Train net output #0: accuracy = 0.982122
I0525 13:23:26.827376 25608 solver.cpp:244]     Train net output #1: loss = 0.0553515 (* 1 = 0.0553515 loss)
I0525 13:23:26.827381 25608 sgd_solver.cpp:106] Iteration 2040, lr = 0.01
I0525 13:23:42.145839 25608 solver.cpp:228] Iteration 2060, loss = 0.056217
I0525 13:23:42.145864 25608 solver.cpp:244]     Train net output #0: accuracy = 0.980286
I0525 13:23:42.145870 25608 solver.cpp:244]     Train net output #1: loss = 0.056217 (* 1 = 0.056217 loss)
I0525 13:23:42.145875 25608 sgd_solver.cpp:106] Iteration 2060, lr = 0.01
I0525 13:23:57.464025 25608 solver.cpp:228] Iteration 2080, loss = 0.0490001
I0525 13:23:57.464123 25608 solver.cpp:244]     Train net output #0: accuracy = 0.98424
I0525 13:23:57.464133 25608 solver.cpp:244]     Train net output #1: loss = 0.0490001 (* 1 = 0.0490001 loss)
I0525 13:23:57.464138 25608 sgd_solver.cpp:106] Iteration 2080, lr = 0.01
I0525 13:24:12.358546 25608 solver.cpp:337] Iteration 2100, Testing net (#0)
I0525 13:24:12.872851 25608 solver.cpp:404]     Test net output #0: accuracy = 0.983026
I0525 13:24:12.872879 25608 solver.cpp:404]     Test net output #1: loss = 0.0517515 (* 1 = 0.0517515 loss)
I0525 13:24:13.326460 25608 solver.cpp:228] Iteration 2100, loss = 0.0499757
I0525 13:24:13.326484 25608 solver.cpp:244]     Train net output #0: accuracy = 0.981668
I0525 13:24:13.326493 25608 solver.cpp:244]     Train net output #1: loss = 0.0499757 (* 1 = 0.0499757 loss)
I0525 13:24:13.326496 25608 sgd_solver.cpp:106] Iteration 2100, lr = 0.01
I0525 13:24:28.675940 25608 solver.cpp:228] Iteration 2120, loss = 0.0451072
I0525 13:24:28.676074 25608 solver.cpp:244]     Train net output #0: accuracy = 0.985511
I0525 13:24:28.676084 25608 solver.cpp:244]     Train net output #1: loss = 0.0451072 (* 1 = 0.0451072 loss)
I0525 13:24:28.676090 25608 sgd_solver.cpp:106] Iteration 2120, lr = 0.01
I0525 13:24:44.008910 25608 solver.cpp:228] Iteration 2140, loss = 0.0508975
I0525 13:24:44.008946 25608 solver.cpp:244]     Train net output #0: accuracy = 0.981935
I0525 13:24:44.008954 25608 solver.cpp:244]     Train net output #1: loss = 0.0508975 (* 1 = 0.0508975 loss)
I0525 13:24:44.008958 25608 sgd_solver.cpp:106] Iteration 2140, lr = 0.01
I0525 13:24:59.336720 25608 solver.cpp:228] Iteration 2160, loss = 0.0457081
I0525 13:24:59.336827 25608 solver.cpp:244]     Train net output #0: accuracy = 0.985409
I0525 13:24:59.336838 25608 solver.cpp:244]     Train net output #1: loss = 0.0457081 (* 1 = 0.0457081 loss)
I0525 13:24:59.336841 25608 sgd_solver.cpp:106] Iteration 2160, lr = 0.01
I0525 13:25:14.680182 25608 solver.cpp:228] Iteration 2180, loss = 0.0495434
I0525 13:25:14.680209 25608 solver.cpp:244]     Train net output #0: accuracy = 0.982281
I0525 13:25:14.680217 25608 solver.cpp:244]     Train net output #1: loss = 0.0495434 (* 1 = 0.0495434 loss)
I0525 13:25:14.680222 25608 sgd_solver.cpp:106] Iteration 2180, lr = 0.01
I0525 13:25:29.564388 25608 solver.cpp:337] Iteration 2200, Testing net (#0)
I0525 13:25:30.079252 25608 solver.cpp:404]     Test net output #0: accuracy = 0.985163
I0525 13:25:30.079288 25608 solver.cpp:404]     Test net output #1: loss = 0.0470475 (* 1 = 0.0470475 loss)
I0525 13:25:30.530045 25608 solver.cpp:228] Iteration 2200, loss = 0.0470194
I0525 13:25:30.530067 25608 solver.cpp:244]     Train net output #0: accuracy = 0.984947
I0525 13:25:30.530074 25608 solver.cpp:244]     Train net output #1: loss = 0.0470194 (* 1 = 0.0470194 loss)
I0525 13:25:30.530078 25608 sgd_solver.cpp:106] Iteration 2200, lr = 0.01
I0525 13:25:45.844447 25608 solver.cpp:228] Iteration 2220, loss = 0.037297
I0525 13:25:45.844471 25608 solver.cpp:244]     Train net output #0: accuracy = 0.988616
I0525 13:25:45.844478 25608 solver.cpp:244]     Train net output #1: loss = 0.037297 (* 1 = 0.037297 loss)
I0525 13:25:45.844482 25608 sgd_solver.cpp:106] Iteration 2220, lr = 0.01
I0525 13:26:01.168576 25608 solver.cpp:228] Iteration 2240, loss = 0.0401219
I0525 13:26:01.168668 25608 solver.cpp:244]     Train net output #0: accuracy = 0.986487
I0525 13:26:01.168678 25608 solver.cpp:244]     Train net output #1: loss = 0.0401219 (* 1 = 0.0401219 loss)
I0525 13:26:01.168684 25608 sgd_solver.cpp:106] Iteration 2240, lr = 0.01
I0525 13:26:16.503581 25608 solver.cpp:228] Iteration 2260, loss = 0.0449786
I0525 13:26:16.503607 25608 solver.cpp:244]     Train net output #0: accuracy = 0.986182
I0525 13:26:16.503613 25608 solver.cpp:244]     Train net output #1: loss = 0.0449786 (* 1 = 0.0449786 loss)
I0525 13:26:16.503618 25608 sgd_solver.cpp:106] Iteration 2260, lr = 0.01
I0525 13:26:31.920266 25608 solver.cpp:228] Iteration 2280, loss = 0.0522285
I0525 13:26:31.920364 25608 solver.cpp:244]     Train net output #0: accuracy = 0.98191
I0525 13:26:31.920374 25608 solver.cpp:244]     Train net output #1: loss = 0.0522285 (* 1 = 0.0522285 loss)
I0525 13:26:31.920379 25608 sgd_solver.cpp:106] Iteration 2280, lr = 0.01
I0525 13:26:46.804633 25608 solver.cpp:337] Iteration 2300, Testing net (#0)
I0525 13:26:47.320358 25608 solver.cpp:404]     Test net output #0: accuracy = 0.979772
I0525 13:26:47.320381 25608 solver.cpp:404]     Test net output #1: loss = 0.0548021 (* 1 = 0.0548021 loss)
I0525 13:26:47.772930 25608 solver.cpp:228] Iteration 2300, loss = 0.0566254
I0525 13:26:47.772966 25608 solver.cpp:244]     Train net output #0: accuracy = 0.979658
I0525 13:26:47.772972 25608 solver.cpp:244]     Train net output #1: loss = 0.0566254 (* 1 = 0.0566254 loss)
I0525 13:26:47.772979 25608 sgd_solver.cpp:106] Iteration 2300, lr = 0.01
I0525 13:27:03.110113 25608 solver.cpp:228] Iteration 2320, loss = 0.0618641
I0525 13:27:03.110237 25608 solver.cpp:244]     Train net output #0: accuracy = 0.978226
I0525 13:27:03.110249 25608 solver.cpp:244]     Train net output #1: loss = 0.0618641 (* 1 = 0.0618641 loss)
I0525 13:27:03.110254 25608 sgd_solver.cpp:106] Iteration 2320, lr = 0.01
I0525 13:27:18.456372 25608 solver.cpp:228] Iteration 2340, loss = 0.0418409
I0525 13:27:18.456395 25608 solver.cpp:244]     Train net output #0: accuracy = 0.985481
I0525 13:27:18.456403 25608 solver.cpp:244]     Train net output #1: loss = 0.0418409 (* 1 = 0.0418409 loss)
I0525 13:27:18.456408 25608 sgd_solver.cpp:106] Iteration 2340, lr = 0.01
I0525 13:27:33.775121 25608 solver.cpp:228] Iteration 2360, loss = 0.0497194
I0525 13:27:33.775213 25608 solver.cpp:244]     Train net output #0: accuracy = 0.981552
I0525 13:27:33.775223 25608 solver.cpp:244]     Train net output #1: loss = 0.0497194 (* 1 = 0.0497194 loss)
I0525 13:27:33.775228 25608 sgd_solver.cpp:106] Iteration 2360, lr = 0.01
I0525 13:27:49.084179 25608 solver.cpp:228] Iteration 2380, loss = 0.0479568
I0525 13:27:49.084205 25608 solver.cpp:244]     Train net output #0: accuracy = 0.983912
I0525 13:27:49.084213 25608 solver.cpp:244]     Train net output #1: loss = 0.0479568 (* 1 = 0.0479568 loss)
I0525 13:27:49.084218 25608 sgd_solver.cpp:106] Iteration 2380, lr = 0.01
I0525 13:28:03.956410 25608 solver.cpp:337] Iteration 2400, Testing net (#0)
I0525 13:28:04.471091 25608 solver.cpp:404]     Test net output #0: accuracy = 0.985592
I0525 13:28:04.471125 25608 solver.cpp:404]     Test net output #1: loss = 0.0404201 (* 1 = 0.0404201 loss)
I0525 13:28:04.921423 25608 solver.cpp:228] Iteration 2400, loss = 0.0461165
I0525 13:28:04.921448 25608 solver.cpp:244]     Train net output #0: accuracy = 0.984421
I0525 13:28:04.921455 25608 solver.cpp:244]     Train net output #1: loss = 0.0461165 (* 1 = 0.0461165 loss)
I0525 13:28:04.921460 25608 sgd_solver.cpp:106] Iteration 2400, lr = 0.01
I0525 13:28:20.249436 25608 solver.cpp:228] Iteration 2420, loss = 0.0415905
I0525 13:28:20.249460 25608 solver.cpp:244]     Train net output #0: accuracy = 0.985729
I0525 13:28:20.249467 25608 solver.cpp:244]     Train net output #1: loss = 0.0415905 (* 1 = 0.0415905 loss)
I0525 13:28:20.249472 25608 sgd_solver.cpp:106] Iteration 2420, lr = 0.01
I0525 13:28:35.572732 25608 solver.cpp:228] Iteration 2440, loss = 0.0389131
I0525 13:28:35.572834 25608 solver.cpp:244]     Train net output #0: accuracy = 0.987182
I0525 13:28:35.572844 25608 solver.cpp:244]     Train net output #1: loss = 0.0389131 (* 1 = 0.0389131 loss)
I0525 13:28:35.572849 25608 sgd_solver.cpp:106] Iteration 2440, lr = 0.01
I0525 13:28:50.884877 25608 solver.cpp:228] Iteration 2460, loss = 0.0479171
I0525 13:28:50.884903 25608 solver.cpp:244]     Train net output #0: accuracy = 0.98347
I0525 13:28:50.884910 25608 solver.cpp:244]     Train net output #1: loss = 0.0479171 (* 1 = 0.0479171 loss)
I0525 13:28:50.884915 25608 sgd_solver.cpp:106] Iteration 2460, lr = 0.01
I0525 13:29:06.222296 25608 solver.cpp:228] Iteration 2480, loss = 0.0451986
I0525 13:29:06.222391 25608 solver.cpp:244]     Train net output #0: accuracy = 0.984419
I0525 13:29:06.222401 25608 solver.cpp:244]     Train net output #1: loss = 0.0451986 (* 1 = 0.0451986 loss)
I0525 13:29:06.222406 25608 sgd_solver.cpp:106] Iteration 2480, lr = 0.01
I0525 13:29:21.114040 25608 solver.cpp:454] Snapshotting to binary proto file data/models/segnet_iter_2500.caffemodel
I0525 13:29:21.133405 25608 sgd_solver.cpp:273] Snapshotting solver state to binary proto file data/models/segnet_iter_2500.solverstate
I0525 13:29:21.142825 25608 solver.cpp:337] Iteration 2500, Testing net (#0)
I0525 13:29:21.659189 25608 solver.cpp:404]     Test net output #0: accuracy = 0.984295
I0525 13:29:21.659224 25608 solver.cpp:404]     Test net output #1: loss = 0.0441338 (* 1 = 0.0441338 loss)
I0525 13:29:22.110913 25608 solver.cpp:228] Iteration 2500, loss = 0.0411182
I0525 13:29:22.110937 25608 solver.cpp:244]     Train net output #0: accuracy = 0.985932
I0525 13:29:22.110944 25608 solver.cpp:244]     Train net output #1: loss = 0.0411182 (* 1 = 0.0411182 loss)
I0525 13:29:22.110949 25608 sgd_solver.cpp:106] Iteration 2500, lr = 0.01
I0525 13:29:37.431432 25608 solver.cpp:228] Iteration 2520, loss = 0.0535705
I0525 13:29:37.431561 25608 solver.cpp:244]     Train net output #0: accuracy = 0.980785
I0525 13:29:37.431572 25608 solver.cpp:244]     Train net output #1: loss = 0.0535705 (* 1 = 0.0535705 loss)
I0525 13:29:37.431577 25608 sgd_solver.cpp:106] Iteration 2520, lr = 0.01
I0525 13:29:52.753072 25608 solver.cpp:228] Iteration 2540, loss = 0.0408964
I0525 13:29:52.753106 25608 solver.cpp:244]     Train net output #0: accuracy = 0.986804
I0525 13:29:52.753113 25608 solver.cpp:244]     Train net output #1: loss = 0.0408964 (* 1 = 0.0408964 loss)
I0525 13:29:52.753118 25608 sgd_solver.cpp:106] Iteration 2540, lr = 0.01
I0525 13:30:08.089241 25608 solver.cpp:228] Iteration 2560, loss = 0.0430891
I0525 13:30:08.089342 25608 solver.cpp:244]     Train net output #0: accuracy = 0.984348
I0525 13:30:08.089352 25608 solver.cpp:244]     Train net output #1: loss = 0.0430891 (* 1 = 0.0430891 loss)
I0525 13:30:08.089357 25608 sgd_solver.cpp:106] Iteration 2560, lr = 0.01
I0525 13:30:23.539124 25608 solver.cpp:228] Iteration 2580, loss = 0.0475681
I0525 13:30:23.539150 25608 solver.cpp:244]     Train net output #0: accuracy = 0.984245
I0525 13:30:23.539157 25608 solver.cpp:244]     Train net output #1: loss = 0.0475681 (* 1 = 0.0475681 loss)
I0525 13:30:23.539162 25608 sgd_solver.cpp:106] Iteration 2580, lr = 0.01
I0525 13:30:38.437723 25608 solver.cpp:337] Iteration 2600, Testing net (#0)
I0525 13:30:38.953954 25608 solver.cpp:404]     Test net output #0: accuracy = 0.986153
I0525 13:30:38.953979 25608 solver.cpp:404]     Test net output #1: loss = 0.0426467 (* 1 = 0.0426467 loss)
I0525 13:30:39.408185 25608 solver.cpp:228] Iteration 2600, loss = 0.0479993
I0525 13:30:39.408208 25608 solver.cpp:244]     Train net output #0: accuracy = 0.983939
I0525 13:30:39.408215 25608 solver.cpp:244]     Train net output #1: loss = 0.0479993 (* 1 = 0.0479993 loss)
I0525 13:30:39.408221 25608 sgd_solver.cpp:106] Iteration 2600, lr = 0.01
I0525 13:30:54.746832 25608 solver.cpp:228] Iteration 2620, loss = 0.0483759
I0525 13:30:54.746857 25608 solver.cpp:244]     Train net output #0: accuracy = 0.983321
I0525 13:30:54.746865 25608 solver.cpp:244]     Train net output #1: loss = 0.0483759 (* 1 = 0.0483759 loss)
I0525 13:30:54.746870 25608 sgd_solver.cpp:106] Iteration 2620, lr = 0.01
I0525 13:31:10.110467 25608 solver.cpp:228] Iteration 2640, loss = 0.0418541
I0525 13:31:10.110613 25608 solver.cpp:244]     Train net output #0: accuracy = 0.985451
I0525 13:31:10.110622 25608 solver.cpp:244]     Train net output #1: loss = 0.0418541 (* 1 = 0.0418541 loss)
I0525 13:31:10.110627 25608 sgd_solver.cpp:106] Iteration 2640, lr = 0.01
I0525 13:31:25.533318 25608 solver.cpp:228] Iteration 2660, loss = 0.0519051
I0525 13:31:25.533354 25608 solver.cpp:244]     Train net output #0: accuracy = 0.981287
I0525 13:31:25.533360 25608 solver.cpp:244]     Train net output #1: loss = 0.0519051 (* 1 = 0.0519051 loss)
I0525 13:31:25.533365 25608 sgd_solver.cpp:106] Iteration 2660, lr = 0.01
I0525 13:31:41.102099 25608 solver.cpp:228] Iteration 2680, loss = 0.0518965
I0525 13:31:41.102198 25608 solver.cpp:244]     Train net output #0: accuracy = 0.980871
I0525 13:31:41.102207 25608 solver.cpp:244]     Train net output #1: loss = 0.0518965 (* 1 = 0.0518965 loss)
I0525 13:31:41.102213 25608 sgd_solver.cpp:106] Iteration 2680, lr = 0.01
I0525 13:31:56.248164 25608 solver.cpp:337] Iteration 2700, Testing net (#0)
I0525 13:31:56.772697 25608 solver.cpp:404]     Test net output #0: accuracy = 0.985693
I0525 13:31:56.772734 25608 solver.cpp:404]     Test net output #1: loss = 0.0391649 (* 1 = 0.0391649 loss)
I0525 13:31:57.225523 25608 solver.cpp:228] Iteration 2700, loss = 0.0411391
I0525 13:31:57.225548 25608 solver.cpp:244]     Train net output #0: accuracy = 0.986145
I0525 13:31:57.225567 25608 solver.cpp:244]     Train net output #1: loss = 0.0411391 (* 1 = 0.0411391 loss)
I0525 13:31:57.225571 25608 sgd_solver.cpp:106] Iteration 2700, lr = 0.01
I0525 13:32:12.772794 25608 solver.cpp:228] Iteration 2720, loss = 0.0465273
I0525 13:32:12.772918 25608 solver.cpp:244]     Train net output #0: accuracy = 0.982763
I0525 13:32:12.772929 25608 solver.cpp:244]     Train net output #1: loss = 0.0465273 (* 1 = 0.0465273 loss)
I0525 13:32:12.772934 25608 sgd_solver.cpp:106] Iteration 2720, lr = 0.01
I0525 13:32:28.218410 25608 solver.cpp:228] Iteration 2740, loss = 0.040369
I0525 13:32:28.218446 25608 solver.cpp:244]     Train net output #0: accuracy = 0.984666
I0525 13:32:28.218452 25608 solver.cpp:244]     Train net output #1: loss = 0.040369 (* 1 = 0.040369 loss)
I0525 13:32:28.218457 25608 sgd_solver.cpp:106] Iteration 2740, lr = 0.01
I0525 13:32:43.803172 25608 solver.cpp:228] Iteration 2760, loss = 0.0369904
I0525 13:32:43.803266 25608 solver.cpp:244]     Train net output #0: accuracy = 0.98714
I0525 13:32:43.803274 25608 solver.cpp:244]     Train net output #1: loss = 0.0369904 (* 1 = 0.0369904 loss)
I0525 13:32:43.803279 25608 sgd_solver.cpp:106] Iteration 2760, lr = 0.01
I0525 13:32:59.109856 25608 solver.cpp:228] Iteration 2780, loss = 0.0485535
I0525 13:32:59.109880 25608 solver.cpp:244]     Train net output #0: accuracy = 0.981806
I0525 13:32:59.109887 25608 solver.cpp:244]     Train net output #1: loss = 0.0485535 (* 1 = 0.0485535 loss)
I0525 13:32:59.109892 25608 sgd_solver.cpp:106] Iteration 2780, lr = 0.01
I0525 13:33:13.966722 25608 solver.cpp:337] Iteration 2800, Testing net (#0)
I0525 13:33:14.480245 25608 solver.cpp:404]     Test net output #0: accuracy = 0.981357
I0525 13:33:14.480279 25608 solver.cpp:404]     Test net output #1: loss = 0.0503113 (* 1 = 0.0503113 loss)
I0525 13:33:14.930699 25608 solver.cpp:228] Iteration 2800, loss = 0.0451764
I0525 13:33:14.930723 25608 solver.cpp:244]     Train net output #0: accuracy = 0.984075
I0525 13:33:14.930742 25608 solver.cpp:244]     Train net output #1: loss = 0.0451764 (* 1 = 0.0451764 loss)
I0525 13:33:14.930747 25608 sgd_solver.cpp:106] Iteration 2800, lr = 0.01
I0525 13:33:30.226289 25608 solver.cpp:228] Iteration 2820, loss = 0.0432168
I0525 13:33:30.226310 25608 solver.cpp:244]     Train net output #0: accuracy = 0.985173
I0525 13:33:30.226317 25608 solver.cpp:244]     Train net output #1: loss = 0.0432168 (* 1 = 0.0432168 loss)
I0525 13:33:30.226322 25608 sgd_solver.cpp:106] Iteration 2820, lr = 0.01
I0525 13:33:45.526733 25608 solver.cpp:228] Iteration 2840, loss = 0.0467719
I0525 13:33:45.526830 25608 solver.cpp:244]     Train net output #0: accuracy = 0.982145
I0525 13:33:45.526839 25608 solver.cpp:244]     Train net output #1: loss = 0.0467719 (* 1 = 0.0467719 loss)
I0525 13:33:45.526844 25608 sgd_solver.cpp:106] Iteration 2840, lr = 0.01
I0525 13:34:01.156589 25608 solver.cpp:228] Iteration 2860, loss = 0.0419688
I0525 13:34:01.156612 25608 solver.cpp:244]     Train net output #0: accuracy = 0.984609
I0525 13:34:01.156620 25608 solver.cpp:244]     Train net output #1: loss = 0.0419688 (* 1 = 0.0419688 loss)
I0525 13:34:01.156625 25608 sgd_solver.cpp:106] Iteration 2860, lr = 0.01
I0525 13:34:16.776989 25608 solver.cpp:228] Iteration 2880, loss = 0.0354041
I0525 13:34:16.777091 25608 solver.cpp:244]     Train net output #0: accuracy = 0.98844
I0525 13:34:16.777101 25608 solver.cpp:244]     Train net output #1: loss = 0.0354041 (* 1 = 0.0354041 loss)
I0525 13:34:16.777106 25608 sgd_solver.cpp:106] Iteration 2880, lr = 0.01
I0525 13:34:31.866780 25608 solver.cpp:337] Iteration 2900, Testing net (#0)
I0525 13:34:32.380359 25608 solver.cpp:404]     Test net output #0: accuracy = 0.981513
I0525 13:34:32.380388 25608 solver.cpp:404]     Test net output #1: loss = 0.0531721 (* 1 = 0.0531721 loss)
I0525 13:34:32.830037 25608 solver.cpp:228] Iteration 2900, loss = 0.0489291
I0525 13:34:32.830063 25608 solver.cpp:244]     Train net output #0: accuracy = 0.980698
I0525 13:34:32.830070 25608 solver.cpp:244]     Train net output #1: loss = 0.0489291 (* 1 = 0.0489291 loss)
I0525 13:34:32.830075 25608 sgd_solver.cpp:106] Iteration 2900, lr = 0.01
I0525 13:34:48.394110 25608 solver.cpp:228] Iteration 2920, loss = 0.0400611
I0525 13:34:48.394227 25608 solver.cpp:244]     Train net output #0: accuracy = 0.983724
I0525 13:34:48.394237 25608 solver.cpp:244]     Train net output #1: loss = 0.0400611 (* 1 = 0.0400611 loss)
I0525 13:34:48.394243 25608 sgd_solver.cpp:106] Iteration 2920, lr = 0.01
I0525 13:35:04.013996 25608 solver.cpp:228] Iteration 2940, loss = 0.0424764
I0525 13:35:04.014032 25608 solver.cpp:244]     Train net output #0: accuracy = 0.984986
I0525 13:35:04.014039 25608 solver.cpp:244]     Train net output #1: loss = 0.0424764 (* 1 = 0.0424764 loss)
I0525 13:35:04.014045 25608 sgd_solver.cpp:106] Iteration 2940, lr = 0.01
I0525 13:35:19.631192 25608 solver.cpp:228] Iteration 2960, loss = 0.0379509
I0525 13:35:19.631286 25608 solver.cpp:244]     Train net output #0: accuracy = 0.987998
I0525 13:35:19.631295 25608 solver.cpp:244]     Train net output #1: loss = 0.0379509 (* 1 = 0.0379509 loss)
I0525 13:35:19.631299 25608 sgd_solver.cpp:106] Iteration 2960, lr = 0.01
I0525 13:35:35.103057 25608 solver.cpp:228] Iteration 2980, loss = 0.0452964
I0525 13:35:35.103081 25608 solver.cpp:244]     Train net output #0: accuracy = 0.984251
I0525 13:35:35.103087 25608 solver.cpp:244]     Train net output #1: loss = 0.0452964 (* 1 = 0.0452964 loss)
I0525 13:35:35.103092 25608 sgd_solver.cpp:106] Iteration 2980, lr = 0.01
I0525 13:35:49.966102 25608 solver.cpp:454] Snapshotting to binary proto file data/models/segnet_iter_3000.caffemodel
I0525 13:35:49.984371 25608 sgd_solver.cpp:273] Snapshotting solver state to binary proto file data/models/segnet_iter_3000.solverstate
I0525 13:35:49.999274 25608 solver.cpp:337] Iteration 3000, Testing net (#0)
I0525 13:35:50.573493 25608 solver.cpp:404]     Test net output #0: accuracy = 0.981713
I0525 13:35:50.573521 25608 solver.cpp:404]     Test net output #1: loss = 0.0468652 (* 1 = 0.0468652 loss)
I0525 13:35:51.114439 25608 solver.cpp:228] Iteration 3000, loss = 0.0357047
I0525 13:35:51.114462 25608 solver.cpp:244]     Train net output #0: accuracy = 0.987356
I0525 13:35:51.114470 25608 solver.cpp:244]     Train net output #1: loss = 0.0357047 (* 1 = 0.0357047 loss)
I0525 13:35:51.114475 25608 sgd_solver.cpp:106] Iteration 3000, lr = 0.001
I0525 13:36:08.484985 25608 solver.cpp:228] Iteration 3020, loss = 0.05066
I0525 13:36:08.485009 25608 solver.cpp:244]     Train net output #0: accuracy = 0.981055
I0525 13:36:08.485028 25608 solver.cpp:244]     Train net output #1: loss = 0.05066 (* 1 = 0.05066 loss)
I0525 13:36:08.485033 25608 sgd_solver.cpp:106] Iteration 3020, lr = 0.001
I0525 13:36:25.727522 25608 solver.cpp:228] Iteration 3040, loss = 0.0471718
I0525 13:36:25.727625 25608 solver.cpp:244]     Train net output #0: accuracy = 0.98215
I0525 13:36:25.727634 25608 solver.cpp:244]     Train net output #1: loss = 0.0471718 (* 1 = 0.0471718 loss)
I0525 13:36:25.727639 25608 sgd_solver.cpp:106] Iteration 3040, lr = 0.001
I0525 13:36:43.079563 25608 solver.cpp:228] Iteration 3060, loss = 0.0446761
I0525 13:36:43.079589 25608 solver.cpp:244]     Train net output #0: accuracy = 0.982744
I0525 13:36:43.079596 25608 solver.cpp:244]     Train net output #1: loss = 0.0446761 (* 1 = 0.0446761 loss)
I0525 13:36:43.079601 25608 sgd_solver.cpp:106] Iteration 3060, lr = 0.001
I0525 13:36:59.567250 25608 solver.cpp:228] Iteration 3080, loss = 0.0397889
I0525 13:36:59.567353 25608 solver.cpp:244]     Train net output #0: accuracy = 0.985764
I0525 13:36:59.567363 25608 solver.cpp:244]     Train net output #1: loss = 0.0397889 (* 1 = 0.0397889 loss)
I0525 13:36:59.567368 25608 sgd_solver.cpp:106] Iteration 3080, lr = 0.001
I0525 13:37:14.778879 25608 solver.cpp:337] Iteration 3100, Testing net (#0)
I0525 13:37:15.294739 25608 solver.cpp:404]     Test net output #0: accuracy = 0.984496
I0525 13:37:15.294764 25608 solver.cpp:404]     Test net output #1: loss = 0.0447398 (* 1 = 0.0447398 loss)
I0525 13:37:15.746747 25608 solver.cpp:228] Iteration 3100, loss = 0.0387926
I0525 13:37:15.746779 25608 solver.cpp:244]     Train net output #0: accuracy = 0.986471
I0525 13:37:15.746786 25608 solver.cpp:244]     Train net output #1: loss = 0.0387926 (* 1 = 0.0387926 loss)
I0525 13:37:15.746791 25608 sgd_solver.cpp:106] Iteration 3100, lr = 0.001
I0525 13:37:31.425742 25608 solver.cpp:228] Iteration 3120, loss = 0.0331228
I0525 13:37:31.425870 25608 solver.cpp:244]     Train net output #0: accuracy = 0.988085
I0525 13:37:31.425881 25608 solver.cpp:244]     Train net output #1: loss = 0.0331228 (* 1 = 0.0331228 loss)
I0525 13:37:31.425885 25608 sgd_solver.cpp:106] Iteration 3120, lr = 0.001
I0525 13:37:46.865187 25608 solver.cpp:228] Iteration 3140, loss = 0.0384349
I0525 13:37:46.865216 25608 solver.cpp:244]     Train net output #0: accuracy = 0.986074
I0525 13:37:46.865222 25608 solver.cpp:244]     Train net output #1: loss = 0.0384349 (* 1 = 0.0384349 loss)
I0525 13:37:46.865227 25608 sgd_solver.cpp:106] Iteration 3140, lr = 0.001
I0525 13:38:02.262480 25608 solver.cpp:228] Iteration 3160, loss = 0.0427585
I0525 13:38:02.262576 25608 solver.cpp:244]     Train net output #0: accuracy = 0.982288
I0525 13:38:02.262586 25608 solver.cpp:244]     Train net output #1: loss = 0.0427585 (* 1 = 0.0427585 loss)
I0525 13:38:02.262591 25608 sgd_solver.cpp:106] Iteration 3160, lr = 0.001
I0525 13:38:17.831980 25608 solver.cpp:228] Iteration 3180, loss = 0.0453422
I0525 13:38:17.832005 25608 solver.cpp:244]     Train net output #0: accuracy = 0.982291
I0525 13:38:17.832012 25608 solver.cpp:244]     Train net output #1: loss = 0.0453422 (* 1 = 0.0453422 loss)
I0525 13:38:17.832017 25608 sgd_solver.cpp:106] Iteration 3180, lr = 0.001
I0525 13:38:32.841084 25608 solver.cpp:337] Iteration 3200, Testing net (#0)
I0525 13:38:33.367014 25608 solver.cpp:404]     Test net output #0: accuracy = 0.98113
I0525 13:38:33.367050 25608 solver.cpp:404]     Test net output #1: loss = 0.0497501 (* 1 = 0.0497501 loss)
I0525 13:38:33.820255 25608 solver.cpp:228] Iteration 3200, loss = 0.0413918
I0525 13:38:33.820281 25608 solver.cpp:244]     Train net output #0: accuracy = 0.98682
I0525 13:38:33.820288 25608 solver.cpp:244]     Train net output #1: loss = 0.0413918 (* 1 = 0.0413918 loss)
I0525 13:38:33.820292 25608 sgd_solver.cpp:106] Iteration 3200, lr = 0.001
I0525 13:38:49.305011 25608 solver.cpp:228] Iteration 3220, loss = 0.047618
I0525 13:38:49.305034 25608 solver.cpp:244]     Train net output #0: accuracy = 0.98281
I0525 13:38:49.305053 25608 solver.cpp:244]     Train net output #1: loss = 0.047618 (* 1 = 0.047618 loss)
I0525 13:38:49.305058 25608 sgd_solver.cpp:106] Iteration 3220, lr = 0.001
I0525 13:39:04.746991 25608 solver.cpp:228] Iteration 3240, loss = 0.0422304
I0525 13:39:04.747063 25608 solver.cpp:244]     Train net output #0: accuracy = 0.98503
I0525 13:39:04.747078 25608 solver.cpp:244]     Train net output #1: loss = 0.0422304 (* 1 = 0.0422304 loss)
I0525 13:39:04.747083 25608 sgd_solver.cpp:106] Iteration 3240, lr = 0.001
I0525 13:39:20.466174 25608 solver.cpp:228] Iteration 3260, loss = 0.0450571
I0525 13:39:20.466197 25608 solver.cpp:244]     Train net output #0: accuracy = 0.983123
I0525 13:39:20.466204 25608 solver.cpp:244]     Train net output #1: loss = 0.0450571 (* 1 = 0.0450571 loss)
I0525 13:39:20.466208 25608 sgd_solver.cpp:106] Iteration 3260, lr = 0.001
I0525 13:39:36.018049 25608 solver.cpp:228] Iteration 3280, loss = 0.0479927
I0525 13:39:36.018152 25608 solver.cpp:244]     Train net output #0: accuracy = 0.981703
I0525 13:39:36.018167 25608 solver.cpp:244]     Train net output #1: loss = 0.0479927 (* 1 = 0.0479927 loss)
I0525 13:39:36.018172 25608 sgd_solver.cpp:106] Iteration 3280, lr = 0.001
I0525 13:39:51.025115 25608 solver.cpp:337] Iteration 3300, Testing net (#0)
I0525 13:39:51.541553 25608 solver.cpp:404]     Test net output #0: accuracy = 0.982521
I0525 13:39:51.541576 25608 solver.cpp:404]     Test net output #1: loss = 0.0483385 (* 1 = 0.0483385 loss)
I0525 13:39:51.994506 25608 solver.cpp:228] Iteration 3300, loss = 0.042993
I0525 13:39:51.994530 25608 solver.cpp:244]     Train net output #0: accuracy = 0.983662
I0525 13:39:51.994539 25608 solver.cpp:244]     Train net output #1: loss = 0.042993 (* 1 = 0.042993 loss)
I0525 13:39:51.994542 25608 sgd_solver.cpp:106] Iteration 3300, lr = 0.001
I0525 13:40:07.583495 25608 solver.cpp:228] Iteration 3320, loss = 0.0420255
I0525 13:40:07.583622 25608 solver.cpp:244]     Train net output #0: accuracy = 0.985207
I0525 13:40:07.583632 25608 solver.cpp:244]     Train net output #1: loss = 0.0420255 (* 1 = 0.0420255 loss)
I0525 13:40:07.583638 25608 sgd_solver.cpp:106] Iteration 3320, lr = 0.001
I0525 13:40:23.104528 25608 solver.cpp:228] Iteration 3340, loss = 0.0418817
I0525 13:40:23.104553 25608 solver.cpp:244]     Train net output #0: accuracy = 0.983359
I0525 13:40:23.104562 25608 solver.cpp:244]     Train net output #1: loss = 0.0418817 (* 1 = 0.0418817 loss)
I0525 13:40:23.104565 25608 sgd_solver.cpp:106] Iteration 3340, lr = 0.001
I0525 13:40:38.483294 25608 solver.cpp:228] Iteration 3360, loss = 0.0452949
I0525 13:40:38.483398 25608 solver.cpp:244]     Train net output #0: accuracy = 0.981995
I0525 13:40:38.483408 25608 solver.cpp:244]     Train net output #1: loss = 0.0452949 (* 1 = 0.0452949 loss)
I0525 13:40:38.483413 25608 sgd_solver.cpp:106] Iteration 3360, lr = 0.001
I0525 13:40:53.787515 25608 solver.cpp:228] Iteration 3380, loss = 0.0462479
I0525 13:40:53.787539 25608 solver.cpp:244]     Train net output #0: accuracy = 0.981653
I0525 13:40:53.787547 25608 solver.cpp:244]     Train net output #1: loss = 0.0462479 (* 1 = 0.0462479 loss)
I0525 13:40:53.787551 25608 sgd_solver.cpp:106] Iteration 3380, lr = 0.001
I0525 13:41:08.655660 25608 solver.cpp:337] Iteration 3400, Testing net (#0)
I0525 13:41:09.168725 25608 solver.cpp:404]     Test net output #0: accuracy = 0.981242
I0525 13:41:09.168751 25608 solver.cpp:404]     Test net output #1: loss = 0.0505967 (* 1 = 0.0505967 loss)
I0525 13:41:09.620621 25608 solver.cpp:228] Iteration 3400, loss = 0.0402657
I0525 13:41:09.620645 25608 solver.cpp:244]     Train net output #0: accuracy = 0.984802
I0525 13:41:09.620651 25608 solver.cpp:244]     Train net output #1: loss = 0.0402657 (* 1 = 0.0402657 loss)
I0525 13:41:09.620656 25608 sgd_solver.cpp:106] Iteration 3400, lr = 0.001
I0525 13:41:24.930234 25608 solver.cpp:228] Iteration 3420, loss = 0.0433346
I0525 13:41:24.930258 25608 solver.cpp:244]     Train net output #0: accuracy = 0.984056
I0525 13:41:24.930264 25608 solver.cpp:244]     Train net output #1: loss = 0.0433346 (* 1 = 0.0433346 loss)
I0525 13:41:24.930269 25608 sgd_solver.cpp:106] Iteration 3420, lr = 0.001
I0525 13:41:40.340478 25608 solver.cpp:228] Iteration 3440, loss = 0.0395829
I0525 13:41:40.340579 25608 solver.cpp:244]     Train net output #0: accuracy = 0.9867
I0525 13:41:40.340587 25608 solver.cpp:244]     Train net output #1: loss = 0.0395829 (* 1 = 0.0395829 loss)
I0525 13:41:40.340592 25608 sgd_solver.cpp:106] Iteration 3440, lr = 0.001
I0525 13:41:55.645094 25608 solver.cpp:228] Iteration 3460, loss = 0.0497044
I0525 13:41:55.645118 25608 solver.cpp:244]     Train net output #0: accuracy = 0.980851
I0525 13:41:55.645136 25608 solver.cpp:244]     Train net output #1: loss = 0.0497044 (* 1 = 0.0497044 loss)
I0525 13:41:55.645141 25608 sgd_solver.cpp:106] Iteration 3460, lr = 0.001
I0525 13:42:11.084113 25608 solver.cpp:228] Iteration 3480, loss = 0.0521508
I0525 13:42:11.084205 25608 solver.cpp:244]     Train net output #0: accuracy = 0.979005
I0525 13:42:11.084214 25608 solver.cpp:244]     Train net output #1: loss = 0.0521508 (* 1 = 0.0521508 loss)
I0525 13:42:11.084219 25608 sgd_solver.cpp:106] Iteration 3480, lr = 0.001
I0525 13:42:26.082362 25608 solver.cpp:454] Snapshotting to binary proto file data/models/segnet_iter_3500.caffemodel
I0525 13:42:26.100500 25608 sgd_solver.cpp:273] Snapshotting solver state to binary proto file data/models/segnet_iter_3500.solverstate
I0525 13:42:26.109848 25608 solver.cpp:337] Iteration 3500, Testing net (#0)
I0525 13:42:26.625169 25608 solver.cpp:404]     Test net output #0: accuracy = 0.983267
I0525 13:42:26.625193 25608 solver.cpp:404]     Test net output #1: loss = 0.0442566 (* 1 = 0.0442566 loss)
I0525 13:42:27.076112 25608 solver.cpp:228] Iteration 3500, loss = 0.0427535
I0525 13:42:27.076146 25608 solver.cpp:244]     Train net output #0: accuracy = 0.984185
I0525 13:42:27.076153 25608 solver.cpp:244]     Train net output #1: loss = 0.0427535 (* 1 = 0.0427535 loss)
I0525 13:42:27.076158 25608 sgd_solver.cpp:106] Iteration 3500, lr = 0.001
I0525 13:42:42.433943 25608 solver.cpp:228] Iteration 3520, loss = 0.047581
I0525 13:42:42.434063 25608 solver.cpp:244]     Train net output #0: accuracy = 0.981011
I0525 13:42:42.434073 25608 solver.cpp:244]     Train net output #1: loss = 0.047581 (* 1 = 0.047581 loss)
I0525 13:42:42.434078 25608 sgd_solver.cpp:106] Iteration 3520, lr = 0.001
I0525 13:42:57.739629 25608 solver.cpp:228] Iteration 3540, loss = 0.0496502
I0525 13:42:57.739653 25608 solver.cpp:244]     Train net output #0: accuracy = 0.980067
I0525 13:42:57.739660 25608 solver.cpp:244]     Train net output #1: loss = 0.0496502 (* 1 = 0.0496502 loss)
I0525 13:42:57.739665 25608 sgd_solver.cpp:106] Iteration 3540, lr = 0.001
I0525 13:43:13.058926 25608 solver.cpp:228] Iteration 3560, loss = 0.0472596
I0525 13:43:13.059044 25608 solver.cpp:244]     Train net output #0: accuracy = 0.979747
I0525 13:43:13.059054 25608 solver.cpp:244]     Train net output #1: loss = 0.0472596 (* 1 = 0.0472596 loss)
I0525 13:43:13.059061 25608 sgd_solver.cpp:106] Iteration 3560, lr = 0.001
I0525 13:43:28.363540 25608 solver.cpp:228] Iteration 3580, loss = 0.0470203
I0525 13:43:28.363565 25608 solver.cpp:244]     Train net output #0: accuracy = 0.982439
I0525 13:43:28.363572 25608 solver.cpp:244]     Train net output #1: loss = 0.0470203 (* 1 = 0.0470203 loss)
I0525 13:43:28.363577 25608 sgd_solver.cpp:106] Iteration 3580, lr = 0.001
I0525 13:43:43.221083 25608 solver.cpp:337] Iteration 3600, Testing net (#0)
I0525 13:43:43.734414 25608 solver.cpp:404]     Test net output #0: accuracy = 0.986609
I0525 13:43:43.734449 25608 solver.cpp:404]     Test net output #1: loss = 0.0372307 (* 1 = 0.0372307 loss)
I0525 13:43:44.185498 25608 solver.cpp:228] Iteration 3600, loss = 0.0446531
I0525 13:43:44.185521 25608 solver.cpp:244]     Train net output #0: accuracy = 0.984123
I0525 13:43:44.185528 25608 solver.cpp:244]     Train net output #1: loss = 0.0446531 (* 1 = 0.0446531 loss)
I0525 13:43:44.185533 25608 sgd_solver.cpp:106] Iteration 3600, lr = 0.001
I0525 13:43:59.497830 25608 solver.cpp:228] Iteration 3620, loss = 0.0353401
I0525 13:43:59.497853 25608 solver.cpp:244]     Train net output #0: accuracy = 0.989055
I0525 13:43:59.497861 25608 solver.cpp:244]     Train net output #1: loss = 0.0353401 (* 1 = 0.0353401 loss)
I0525 13:43:59.497865 25608 sgd_solver.cpp:106] Iteration 3620, lr = 0.001
I0525 13:44:14.815532 25608 solver.cpp:228] Iteration 3640, loss = 0.0415502
I0525 13:44:14.815628 25608 solver.cpp:244]     Train net output #0: accuracy = 0.986037
I0525 13:44:14.815636 25608 solver.cpp:244]     Train net output #1: loss = 0.0415502 (* 1 = 0.0415502 loss)
I0525 13:44:14.815641 25608 sgd_solver.cpp:106] Iteration 3640, lr = 0.001
I0525 13:44:30.123075 25608 solver.cpp:228] Iteration 3660, loss = 0.0396269
I0525 13:44:30.123100 25608 solver.cpp:244]     Train net output #0: accuracy = 0.985695
I0525 13:44:30.123107 25608 solver.cpp:244]     Train net output #1: loss = 0.0396269 (* 1 = 0.0396269 loss)
I0525 13:44:30.123111 25608 sgd_solver.cpp:106] Iteration 3660, lr = 0.001
I0525 13:44:45.455066 25608 solver.cpp:228] Iteration 3680, loss = 0.0441864
I0525 13:44:45.455160 25608 solver.cpp:244]     Train net output #0: accuracy = 0.982321
I0525 13:44:45.455169 25608 solver.cpp:244]     Train net output #1: loss = 0.0441864 (* 1 = 0.0441864 loss)
I0525 13:44:45.455173 25608 sgd_solver.cpp:106] Iteration 3680, lr = 0.001
I0525 13:45:00.539651 25608 solver.cpp:337] Iteration 3700, Testing net (#0)
I0525 13:45:01.053207 25608 solver.cpp:404]     Test net output #0: accuracy = 0.982983
I0525 13:45:01.053242 25608 solver.cpp:404]     Test net output #1: loss = 0.0462831 (* 1 = 0.0462831 loss)
I0525 13:45:01.502972 25608 solver.cpp:228] Iteration 3700, loss = 0.0462215
I0525 13:45:01.502995 25608 solver.cpp:244]     Train net output #0: accuracy = 0.98078
I0525 13:45:01.503001 25608 solver.cpp:244]     Train net output #1: loss = 0.0462215 (* 1 = 0.0462215 loss)
I0525 13:45:01.503006 25608 sgd_solver.cpp:106] Iteration 3700, lr = 0.001
I0525 13:45:16.805939 25608 solver.cpp:228] Iteration 3720, loss = 0.0418357
I0525 13:45:16.806059 25608 solver.cpp:244]     Train net output #0: accuracy = 0.985229
I0525 13:45:16.806071 25608 solver.cpp:244]     Train net output #1: loss = 0.0418357 (* 1 = 0.0418357 loss)
I0525 13:45:16.806074 25608 sgd_solver.cpp:106] Iteration 3720, lr = 0.001
I0525 13:45:32.106916 25608 solver.cpp:228] Iteration 3740, loss = 0.0466406
I0525 13:45:32.106940 25608 solver.cpp:244]     Train net output #0: accuracy = 0.98331
I0525 13:45:32.106946 25608 solver.cpp:244]     Train net output #1: loss = 0.0466406 (* 1 = 0.0466406 loss)
I0525 13:45:32.106950 25608 sgd_solver.cpp:106] Iteration 3740, lr = 0.001
I0525 13:45:47.683830 25608 solver.cpp:228] Iteration 3760, loss = 0.04387
I0525 13:45:47.683933 25608 solver.cpp:244]     Train net output #0: accuracy = 0.982229
I0525 13:45:47.683941 25608 solver.cpp:244]     Train net output #1: loss = 0.04387 (* 1 = 0.04387 loss)
I0525 13:45:47.683946 25608 sgd_solver.cpp:106] Iteration 3760, lr = 0.001
I0525 13:46:03.005090 25608 solver.cpp:228] Iteration 3780, loss = 0.0563106
I0525 13:46:03.005115 25608 solver.cpp:244]     Train net output #0: accuracy = 0.979867
I0525 13:46:03.005121 25608 solver.cpp:244]     Train net output #1: loss = 0.0563106 (* 1 = 0.0563106 loss)
I0525 13:46:03.005125 25608 sgd_solver.cpp:106] Iteration 3780, lr = 0.001
I0525 13:46:17.888581 25608 solver.cpp:337] Iteration 3800, Testing net (#0)
I0525 13:46:18.401603 25608 solver.cpp:404]     Test net output #0: accuracy = 0.982781
I0525 13:46:18.401626 25608 solver.cpp:404]     Test net output #1: loss = 0.044407 (* 1 = 0.044407 loss)
I0525 13:46:18.850723 25608 solver.cpp:228] Iteration 3800, loss = 0.0397318
I0525 13:46:18.850749 25608 solver.cpp:244]     Train net output #0: accuracy = 0.984816
I0525 13:46:18.850755 25608 solver.cpp:244]     Train net output #1: loss = 0.0397318 (* 1 = 0.0397318 loss)
I0525 13:46:18.850759 25608 sgd_solver.cpp:106] Iteration 3800, lr = 0.001
I0525 13:46:34.194699 25608 solver.cpp:228] Iteration 3820, loss = 0.0477131
I0525 13:46:34.194733 25608 solver.cpp:244]     Train net output #0: accuracy = 0.982999
I0525 13:46:34.194741 25608 solver.cpp:244]     Train net output #1: loss = 0.0477131 (* 1 = 0.0477131 loss)
I0525 13:46:34.194746 25608 sgd_solver.cpp:106] Iteration 3820, lr = 0.001
I0525 13:46:49.493542 25608 solver.cpp:228] Iteration 3840, loss = 0.0464947
I0525 13:46:49.493649 25608 solver.cpp:244]     Train net output #0: accuracy = 0.980067
I0525 13:46:49.493659 25608 solver.cpp:244]     Train net output #1: loss = 0.0464947 (* 1 = 0.0464947 loss)
I0525 13:46:49.493662 25608 sgd_solver.cpp:106] Iteration 3840, lr = 0.001
I0525 13:47:04.793237 25608 solver.cpp:228] Iteration 3860, loss = 0.0470267
I0525 13:47:04.793272 25608 solver.cpp:244]     Train net output #0: accuracy = 0.981165
I0525 13:47:04.793279 25608 solver.cpp:244]     Train net output #1: loss = 0.0470267 (* 1 = 0.0470267 loss)
I0525 13:47:04.793284 25608 sgd_solver.cpp:106] Iteration 3860, lr = 0.001
I0525 13:47:20.094341 25608 solver.cpp:228] Iteration 3880, loss = 0.0362833
I0525 13:47:20.094435 25608 solver.cpp:244]     Train net output #0: accuracy = 0.986725
I0525 13:47:20.094445 25608 solver.cpp:244]     Train net output #1: loss = 0.0362833 (* 1 = 0.0362833 loss)
I0525 13:47:20.094450 25608 sgd_solver.cpp:106] Iteration 3880, lr = 0.001
I0525 13:47:34.936929 25608 solver.cpp:337] Iteration 3900, Testing net (#0)
I0525 13:47:35.450067 25608 solver.cpp:404]     Test net output #0: accuracy = 0.982142
I0525 13:47:35.450101 25608 solver.cpp:404]     Test net output #1: loss = 0.0463726 (* 1 = 0.0463726 loss)
I0525 13:47:35.899827 25608 solver.cpp:228] Iteration 3900, loss = 0.0356945
I0525 13:47:35.899850 25608 solver.cpp:244]     Train net output #0: accuracy = 0.987688
I0525 13:47:35.899857 25608 solver.cpp:244]     Train net output #1: loss = 0.0356945 (* 1 = 0.0356945 loss)
I0525 13:47:35.899862 25608 sgd_solver.cpp:106] Iteration 3900, lr = 0.001
I0525 13:47:51.371834 25608 solver.cpp:228] Iteration 3920, loss = 0.0455136
I0525 13:47:51.371961 25608 solver.cpp:244]     Train net output #0: accuracy = 0.981636
I0525 13:47:51.371971 25608 solver.cpp:244]     Train net output #1: loss = 0.0455136 (* 1 = 0.0455136 loss)
I0525 13:47:51.371976 25608 sgd_solver.cpp:106] Iteration 3920, lr = 0.001
I0525 13:48:06.879994 25608 solver.cpp:228] Iteration 3940, loss = 0.033505
I0525 13:48:06.880019 25608 solver.cpp:244]     Train net output #0: accuracy = 0.989135
I0525 13:48:06.880025 25608 solver.cpp:244]     Train net output #1: loss = 0.033505 (* 1 = 0.033505 loss)
I0525 13:48:06.880030 25608 sgd_solver.cpp:106] Iteration 3940, lr = 0.001
I0525 13:48:22.212255 25608 solver.cpp:228] Iteration 3960, loss = 0.0425374
I0525 13:48:22.212349 25608 solver.cpp:244]     Train net output #0: accuracy = 0.985501
I0525 13:48:22.212358 25608 solver.cpp:244]     Train net output #1: loss = 0.0425374 (* 1 = 0.0425374 loss)
I0525 13:48:22.212363 25608 sgd_solver.cpp:106] Iteration 3960, lr = 0.001
I0525 13:48:37.584800 25608 solver.cpp:228] Iteration 3980, loss = 0.0411974
I0525 13:48:37.584823 25608 solver.cpp:244]     Train net output #0: accuracy = 0.98436
I0525 13:48:37.584830 25608 solver.cpp:244]     Train net output #1: loss = 0.0411974 (* 1 = 0.0411974 loss)
I0525 13:48:37.584835 25608 sgd_solver.cpp:106] Iteration 3980, lr = 0.001
I0525 13:48:52.458919 25608 solver.cpp:454] Snapshotting to binary proto file data/models/segnet_iter_4000.caffemodel
I0525 13:48:52.476754 25608 sgd_solver.cpp:273] Snapshotting solver state to binary proto file data/models/segnet_iter_4000.solverstate
I0525 13:48:52.485862 25608 solver.cpp:337] Iteration 4000, Testing net (#0)
I0525 13:48:53.002864 25608 solver.cpp:404]     Test net output #0: accuracy = 0.982986
I0525 13:48:53.002899 25608 solver.cpp:404]     Test net output #1: loss = 0.0461937 (* 1 = 0.0461937 loss)
I0525 13:48:53.454851 25608 solver.cpp:228] Iteration 4000, loss = 0.0496201
I0525 13:48:53.454875 25608 solver.cpp:244]     Train net output #0: accuracy = 0.981711
I0525 13:48:53.454881 25608 solver.cpp:244]     Train net output #1: loss = 0.0496201 (* 1 = 0.0496201 loss)
I0525 13:48:53.454885 25608 sgd_solver.cpp:106] Iteration 4000, lr = 0.001
I0525 13:49:08.766808 25608 solver.cpp:228] Iteration 4020, loss = 0.045321
I0525 13:49:08.766832 25608 solver.cpp:244]     Train net output #0: accuracy = 0.983431
I0525 13:49:08.766839 25608 solver.cpp:244]     Train net output #1: loss = 0.045321 (* 1 = 0.045321 loss)
I0525 13:49:08.766844 25608 sgd_solver.cpp:106] Iteration 4020, lr = 0.001
I0525 13:49:24.073545 25608 solver.cpp:228] Iteration 4040, loss = 0.0575178
I0525 13:49:24.073645 25608 solver.cpp:244]     Train net output #0: accuracy = 0.977734
I0525 13:49:24.073655 25608 solver.cpp:244]     Train net output #1: loss = 0.0575178 (* 1 = 0.0575178 loss)
I0525 13:49:24.073662 25608 sgd_solver.cpp:106] Iteration 4040, lr = 0.001
I0525 13:49:39.397936 25608 solver.cpp:228] Iteration 4060, loss = 0.0479937
I0525 13:49:39.397958 25608 solver.cpp:244]     Train net output #0: accuracy = 0.980338
I0525 13:49:39.397965 25608 solver.cpp:244]     Train net output #1: loss = 0.0479937 (* 1 = 0.0479937 loss)
I0525 13:49:39.397969 25608 sgd_solver.cpp:106] Iteration 4060, lr = 0.001
I0525 13:49:54.751946 25608 solver.cpp:228] Iteration 4080, loss = 0.0521064
I0525 13:49:54.752044 25608 solver.cpp:244]     Train net output #0: accuracy = 0.981302
I0525 13:49:54.752054 25608 solver.cpp:244]     Train net output #1: loss = 0.0521064 (* 1 = 0.0521064 loss)
I0525 13:49:54.752058 25608 sgd_solver.cpp:106] Iteration 4080, lr = 0.001
I0525 13:50:09.608922 25608 solver.cpp:337] Iteration 4100, Testing net (#0)
I0525 13:50:10.123240 25608 solver.cpp:404]     Test net output #0: accuracy = 0.985294
I0525 13:50:10.123275 25608 solver.cpp:404]     Test net output #1: loss = 0.0388217 (* 1 = 0.0388217 loss)
I0525 13:50:10.572350 25608 solver.cpp:228] Iteration 4100, loss = 0.0471535
I0525 13:50:10.572374 25608 solver.cpp:244]     Train net output #0: accuracy = 0.981157
I0525 13:50:10.572382 25608 solver.cpp:244]     Train net output #1: loss = 0.0471535 (* 1 = 0.0471535 loss)
I0525 13:50:10.572386 25608 sgd_solver.cpp:106] Iteration 4100, lr = 0.001
I0525 13:50:25.876754 25608 solver.cpp:228] Iteration 4120, loss = 0.0476995
I0525 13:50:25.876868 25608 solver.cpp:244]     Train net output #0: accuracy = 0.982745
I0525 13:50:25.876878 25608 solver.cpp:244]     Train net output #1: loss = 0.0476995 (* 1 = 0.0476995 loss)
I0525 13:50:25.876883 25608 sgd_solver.cpp:106] Iteration 4120, lr = 0.001
I0525 13:50:41.198000 25608 solver.cpp:228] Iteration 4140, loss = 0.0407182
I0525 13:50:41.198024 25608 solver.cpp:244]     Train net output #0: accuracy = 0.986763
I0525 13:50:41.198035 25608 solver.cpp:244]     Train net output #1: loss = 0.0407182 (* 1 = 0.0407182 loss)
I0525 13:50:41.198038 25608 sgd_solver.cpp:106] Iteration 4140, lr = 0.001
I0525 13:50:56.508299 25608 solver.cpp:228] Iteration 4160, loss = 0.0491392
I0525 13:50:56.508394 25608 solver.cpp:244]     Train net output #0: accuracy = 0.981399
I0525 13:50:56.508402 25608 solver.cpp:244]     Train net output #1: loss = 0.0491392 (* 1 = 0.0491392 loss)
I0525 13:50:56.508409 25608 sgd_solver.cpp:106] Iteration 4160, lr = 0.001
I0525 13:51:11.817649 25608 solver.cpp:228] Iteration 4180, loss = 0.0449575
I0525 13:51:11.817673 25608 solver.cpp:244]     Train net output #0: accuracy = 0.983924
I0525 13:51:11.817680 25608 solver.cpp:244]     Train net output #1: loss = 0.0449575 (* 1 = 0.0449575 loss)
I0525 13:51:11.817684 25608 sgd_solver.cpp:106] Iteration 4180, lr = 0.001
I0525 13:51:26.686064 25608 solver.cpp:337] Iteration 4200, Testing net (#0)
I0525 13:51:27.200237 25608 solver.cpp:404]     Test net output #0: accuracy = 0.981161
I0525 13:51:27.200270 25608 solver.cpp:404]     Test net output #1: loss = 0.0522299 (* 1 = 0.0522299 loss)
I0525 13:51:27.651464 25608 solver.cpp:228] Iteration 4200, loss = 0.0399917
I0525 13:51:27.651486 25608 solver.cpp:244]     Train net output #0: accuracy = 0.985108
I0525 13:51:27.651494 25608 solver.cpp:244]     Train net output #1: loss = 0.0399917 (* 1 = 0.0399917 loss)
I0525 13:51:27.651497 25608 sgd_solver.cpp:106] Iteration 4200, lr = 0.001
I0525 13:51:43.057947 25608 solver.cpp:228] Iteration 4220, loss = 0.038728
I0525 13:51:43.057972 25608 solver.cpp:244]     Train net output #0: accuracy = 0.985233
I0525 13:51:43.057979 25608 solver.cpp:244]     Train net output #1: loss = 0.038728 (* 1 = 0.038728 loss)
I0525 13:51:43.057984 25608 sgd_solver.cpp:106] Iteration 4220, lr = 0.001
I0525 13:51:59.018571 25608 solver.cpp:228] Iteration 4240, loss = 0.0432663
I0525 13:51:59.018676 25608 solver.cpp:244]     Train net output #0: accuracy = 0.983103
I0525 13:51:59.018685 25608 solver.cpp:244]     Train net output #1: loss = 0.0432663 (* 1 = 0.0432663 loss)
I0525 13:51:59.018692 25608 sgd_solver.cpp:106] Iteration 4240, lr = 0.001
I0525 13:52:14.744854 25608 solver.cpp:228] Iteration 4260, loss = 0.0472139
I0525 13:52:14.744881 25608 solver.cpp:244]     Train net output #0: accuracy = 0.98072
I0525 13:52:14.744889 25608 solver.cpp:244]     Train net output #1: loss = 0.0472139 (* 1 = 0.0472139 loss)
I0525 13:52:14.744892 25608 sgd_solver.cpp:106] Iteration 4260, lr = 0.001
I0525 13:52:30.131201 25608 solver.cpp:228] Iteration 4280, loss = 0.0451914
I0525 13:52:30.131302 25608 solver.cpp:244]     Train net output #0: accuracy = 0.981322
I0525 13:52:30.131312 25608 solver.cpp:244]     Train net output #1: loss = 0.0451914 (* 1 = 0.0451914 loss)
I0525 13:52:30.131316 25608 sgd_solver.cpp:106] Iteration 4280, lr = 0.001
I0525 13:52:44.965523 25608 solver.cpp:337] Iteration 4300, Testing net (#0)
I0525 13:52:45.478443 25608 solver.cpp:404]     Test net output #0: accuracy = 0.984098
I0525 13:52:45.478467 25608 solver.cpp:404]     Test net output #1: loss = 0.0457529 (* 1 = 0.0457529 loss)
I0525 13:52:45.926890 25608 solver.cpp:228] Iteration 4300, loss = 0.0420211
I0525 13:52:45.926926 25608 solver.cpp:244]     Train net output #0: accuracy = 0.983236
I0525 13:52:45.926934 25608 solver.cpp:244]     Train net output #1: loss = 0.0420211 (* 1 = 0.0420211 loss)
I0525 13:52:45.926939 25608 sgd_solver.cpp:106] Iteration 4300, lr = 0.001
I0525 13:53:01.225200 25608 solver.cpp:228] Iteration 4320, loss = 0.0366313
I0525 13:53:01.225327 25608 solver.cpp:244]     Train net output #0: accuracy = 0.987482
I0525 13:53:01.225337 25608 solver.cpp:244]     Train net output #1: loss = 0.0366313 (* 1 = 0.0366313 loss)
I0525 13:53:01.225342 25608 sgd_solver.cpp:106] Iteration 4320, lr = 0.001
I0525 13:53:16.585919 25608 solver.cpp:228] Iteration 4340, loss = 0.0457377
I0525 13:53:16.585944 25608 solver.cpp:244]     Train net output #0: accuracy = 0.983047
I0525 13:53:16.585952 25608 solver.cpp:244]     Train net output #1: loss = 0.0457377 (* 1 = 0.0457377 loss)
I0525 13:53:16.585955 25608 sgd_solver.cpp:106] Iteration 4340, lr = 0.001
I0525 13:53:31.900508 25608 solver.cpp:228] Iteration 4360, loss = 0.0520569
I0525 13:53:31.900614 25608 solver.cpp:244]     Train net output #0: accuracy = 0.978997
I0525 13:53:31.900624 25608 solver.cpp:244]     Train net output #1: loss = 0.0520569 (* 1 = 0.0520569 loss)
I0525 13:53:31.900629 25608 sgd_solver.cpp:106] Iteration 4360, lr = 0.001
I0525 13:53:47.217797 25608 solver.cpp:228] Iteration 4380, loss = 0.0455579
I0525 13:53:47.217819 25608 solver.cpp:244]     Train net output #0: accuracy = 0.984347
I0525 13:53:47.217826 25608 solver.cpp:244]     Train net output #1: loss = 0.0455579 (* 1 = 0.0455579 loss)
I0525 13:53:47.217831 25608 sgd_solver.cpp:106] Iteration 4380, lr = 0.001
I0525 13:54:02.100266 25608 solver.cpp:337] Iteration 4400, Testing net (#0)
I0525 13:54:02.614300 25608 solver.cpp:404]     Test net output #0: accuracy = 0.981584
I0525 13:54:02.614336 25608 solver.cpp:404]     Test net output #1: loss = 0.0450826 (* 1 = 0.0450826 loss)
I0525 13:54:03.066772 25608 solver.cpp:228] Iteration 4400, loss = 0.0365506
I0525 13:54:03.066797 25608 solver.cpp:244]     Train net output #0: accuracy = 0.986115
I0525 13:54:03.066803 25608 solver.cpp:244]     Train net output #1: loss = 0.0365506 (* 1 = 0.0365506 loss)
I0525 13:54:03.066808 25608 sgd_solver.cpp:106] Iteration 4400, lr = 0.001
I0525 13:54:18.472394 25608 solver.cpp:228] Iteration 4420, loss = 0.0446721
I0525 13:54:18.472417 25608 solver.cpp:244]     Train net output #0: accuracy = 0.982658
I0525 13:54:18.472424 25608 solver.cpp:244]     Train net output #1: loss = 0.0446721 (* 1 = 0.0446721 loss)
I0525 13:54:18.472429 25608 sgd_solver.cpp:106] Iteration 4420, lr = 0.001
I0525 13:54:33.939054 25608 solver.cpp:228] Iteration 4440, loss = 0.0395701
I0525 13:54:33.939146 25608 solver.cpp:244]     Train net output #0: accuracy = 0.983832
I0525 13:54:33.939154 25608 solver.cpp:244]     Train net output #1: loss = 0.0395701 (* 1 = 0.0395701 loss)
I0525 13:54:33.939159 25608 sgd_solver.cpp:106] Iteration 4440, lr = 0.001
I0525 13:54:49.391916 25608 solver.cpp:228] Iteration 4460, loss = 0.0425371
I0525 13:54:49.391950 25608 solver.cpp:244]     Train net output #0: accuracy = 0.983575
I0525 13:54:49.391957 25608 solver.cpp:244]     Train net output #1: loss = 0.0425371 (* 1 = 0.0425371 loss)
I0525 13:54:49.391962 25608 sgd_solver.cpp:106] Iteration 4460, lr = 0.001
I0525 13:55:04.970242 25608 solver.cpp:228] Iteration 4480, loss = 0.0503211
I0525 13:55:04.970335 25608 solver.cpp:244]     Train net output #0: accuracy = 0.981982
I0525 13:55:04.970343 25608 solver.cpp:244]     Train net output #1: loss = 0.0503211 (* 1 = 0.0503211 loss)
I0525 13:55:04.970348 25608 sgd_solver.cpp:106] Iteration 4480, lr = 0.001
I0525 13:55:20.016981 25608 solver.cpp:454] Snapshotting to binary proto file data/models/segnet_iter_4500.caffemodel
I0525 13:55:20.035259 25608 sgd_solver.cpp:273] Snapshotting solver state to binary proto file data/models/segnet_iter_4500.solverstate
I0525 13:55:20.044725 25608 solver.cpp:337] Iteration 4500, Testing net (#0)
I0525 13:55:20.575709 25608 solver.cpp:404]     Test net output #0: accuracy = 0.983231
I0525 13:55:20.575733 25608 solver.cpp:404]     Test net output #1: loss = 0.0461501 (* 1 = 0.0461501 loss)
I0525 13:55:21.030314 25608 solver.cpp:228] Iteration 4500, loss = 0.0338416
I0525 13:55:21.030342 25608 solver.cpp:244]     Train net output #0: accuracy = 0.988827
I0525 13:55:21.030350 25608 solver.cpp:244]     Train net output #1: loss = 0.0338416 (* 1 = 0.0338416 loss)
I0525 13:55:21.030354 25608 sgd_solver.cpp:106] Iteration 4500, lr = 0.001
I0525 13:55:36.560268 25608 solver.cpp:228] Iteration 4520, loss = 0.0434785
I0525 13:55:36.560379 25608 solver.cpp:244]     Train net output #0: accuracy = 0.983172
I0525 13:55:36.560389 25608 solver.cpp:244]     Train net output #1: loss = 0.0434785 (* 1 = 0.0434785 loss)
I0525 13:55:36.560394 25608 sgd_solver.cpp:106] Iteration 4520, lr = 0.001
I0525 13:55:52.197962 25608 solver.cpp:228] Iteration 4540, loss = 0.0383489
I0525 13:55:52.197985 25608 solver.cpp:244]     Train net output #0: accuracy = 0.985043
I0525 13:55:52.198004 25608 solver.cpp:244]     Train net output #1: loss = 0.0383489 (* 1 = 0.0383489 loss)
I0525 13:55:52.198009 25608 sgd_solver.cpp:106] Iteration 4540, lr = 0.001
I0525 13:56:07.934588 25608 solver.cpp:228] Iteration 4560, loss = 0.041497
I0525 13:56:07.934690 25608 solver.cpp:244]     Train net output #0: accuracy = 0.984071
I0525 13:56:07.934700 25608 solver.cpp:244]     Train net output #1: loss = 0.041497 (* 1 = 0.041497 loss)
I0525 13:56:07.934705 25608 sgd_solver.cpp:106] Iteration 4560, lr = 0.001
I0525 13:56:23.265900 25608 solver.cpp:228] Iteration 4580, loss = 0.043917
I0525 13:56:23.265924 25608 solver.cpp:244]     Train net output #0: accuracy = 0.982805
I0525 13:56:23.265931 25608 solver.cpp:244]     Train net output #1: loss = 0.043917 (* 1 = 0.043917 loss)
I0525 13:56:23.265935 25608 sgd_solver.cpp:106] Iteration 4580, lr = 0.001
I0525 13:56:38.185582 25608 solver.cpp:337] Iteration 4600, Testing net (#0)
I0525 13:56:38.699153 25608 solver.cpp:404]     Test net output #0: accuracy = 0.984554
I0525 13:56:38.699198 25608 solver.cpp:404]     Test net output #1: loss = 0.0389415 (* 1 = 0.0389415 loss)
I0525 13:56:39.149642 25608 solver.cpp:228] Iteration 4600, loss = 0.0418882
I0525 13:56:39.149677 25608 solver.cpp:244]     Train net output #0: accuracy = 0.984196
I0525 13:56:39.149684 25608 solver.cpp:244]     Train net output #1: loss = 0.0418882 (* 1 = 0.0418882 loss)
I0525 13:56:39.149688 25608 sgd_solver.cpp:106] Iteration 4600, lr = 0.001
I0525 13:56:54.464208 25608 solver.cpp:228] Iteration 4620, loss = 0.0470379
I0525 13:56:54.464231 25608 solver.cpp:244]     Train net output #0: accuracy = 0.981832
I0525 13:56:54.464238 25608 solver.cpp:244]     Train net output #1: loss = 0.0470379 (* 1 = 0.0470379 loss)
I0525 13:56:54.464242 25608 sgd_solver.cpp:106] Iteration 4620, lr = 0.001
I0525 13:57:09.779189 25608 solver.cpp:228] Iteration 4640, loss = 0.043142
I0525 13:57:09.779299 25608 solver.cpp:244]     Train net output #0: accuracy = 0.982523
I0525 13:57:09.779307 25608 solver.cpp:244]     Train net output #1: loss = 0.043142 (* 1 = 0.043142 loss)
I0525 13:57:09.779312 25608 sgd_solver.cpp:106] Iteration 4640, lr = 0.001
I0525 13:57:25.104182 25608 solver.cpp:228] Iteration 4660, loss = 0.0450208
I0525 13:57:25.104207 25608 solver.cpp:244]     Train net output #0: accuracy = 0.983387
I0525 13:57:25.104213 25608 solver.cpp:244]     Train net output #1: loss = 0.0450208 (* 1 = 0.0450208 loss)
I0525 13:57:25.104218 25608 sgd_solver.cpp:106] Iteration 4660, lr = 0.001
I0525 13:57:40.402719 25608 solver.cpp:228] Iteration 4680, loss = 0.0542947
I0525 13:57:40.402844 25608 solver.cpp:244]     Train net output #0: accuracy = 0.978738
I0525 13:57:40.402853 25608 solver.cpp:244]     Train net output #1: loss = 0.0542947 (* 1 = 0.0542947 loss)
I0525 13:57:40.402858 25608 sgd_solver.cpp:106] Iteration 4680, lr = 0.001
I0525 13:57:55.266213 25608 solver.cpp:337] Iteration 4700, Testing net (#0)
I0525 13:57:55.779326 25608 solver.cpp:404]     Test net output #0: accuracy = 0.982831
I0525 13:57:55.779350 25608 solver.cpp:404]     Test net output #1: loss = 0.0442019 (* 1 = 0.0442019 loss)
I0525 13:57:56.228811 25608 solver.cpp:228] Iteration 4700, loss = 0.0439566
I0525 13:57:56.228833 25608 solver.cpp:244]     Train net output #0: accuracy = 0.981639
I0525 13:57:56.228842 25608 solver.cpp:244]     Train net output #1: loss = 0.0439566 (* 1 = 0.0439566 loss)
I0525 13:57:56.228847 25608 sgd_solver.cpp:106] Iteration 4700, lr = 0.001
I0525 13:58:11.511230 25608 solver.cpp:228] Iteration 4720, loss = 0.0431032
I0525 13:58:11.511354 25608 solver.cpp:244]     Train net output #0: accuracy = 0.980888
I0525 13:58:11.511364 25608 solver.cpp:244]     Train net output #1: loss = 0.0431032 (* 1 = 0.0431032 loss)
I0525 13:58:11.511369 25608 sgd_solver.cpp:106] Iteration 4720, lr = 0.001
I0525 13:58:26.803194 25608 solver.cpp:228] Iteration 4740, loss = 0.0464396
I0525 13:58:26.803238 25608 solver.cpp:244]     Train net output #0: accuracy = 0.981854
I0525 13:58:26.803246 25608 solver.cpp:244]     Train net output #1: loss = 0.0464396 (* 1 = 0.0464396 loss)
I0525 13:58:26.803251 25608 sgd_solver.cpp:106] Iteration 4740, lr = 0.001
I0525 13:58:42.098490 25608 solver.cpp:228] Iteration 4760, loss = 0.0520663
I0525 13:58:42.098595 25608 solver.cpp:244]     Train net output #0: accuracy = 0.980174
I0525 13:58:42.098604 25608 solver.cpp:244]     Train net output #1: loss = 0.0520663 (* 1 = 0.0520663 loss)
I0525 13:58:42.098609 25608 sgd_solver.cpp:106] Iteration 4760, lr = 0.001
I0525 13:58:57.400406 25608 solver.cpp:228] Iteration 4780, loss = 0.0387831
I0525 13:58:57.400442 25608 solver.cpp:244]     Train net output #0: accuracy = 0.986685
I0525 13:58:57.400449 25608 solver.cpp:244]     Train net output #1: loss = 0.0387831 (* 1 = 0.0387831 loss)
I0525 13:58:57.400454 25608 sgd_solver.cpp:106] Iteration 4780, lr = 0.001
I0525 13:59:12.258982 25608 solver.cpp:337] Iteration 4800, Testing net (#0)
I0525 13:59:12.772425 25608 solver.cpp:404]     Test net output #0: accuracy = 0.981991
I0525 13:59:12.772459 25608 solver.cpp:404]     Test net output #1: loss = 0.0485466 (* 1 = 0.0485466 loss)
I0525 13:59:13.222827 25608 solver.cpp:228] Iteration 4800, loss = 0.0518532
I0525 13:59:13.222862 25608 solver.cpp:244]     Train net output #0: accuracy = 0.978645
I0525 13:59:13.222868 25608 solver.cpp:244]     Train net output #1: loss = 0.0518532 (* 1 = 0.0518532 loss)
I0525 13:59:13.222873 25608 sgd_solver.cpp:106] Iteration 4800, lr = 0.001
I0525 13:59:28.513597 25608 solver.cpp:228] Iteration 4820, loss = 0.0538361
I0525 13:59:28.513619 25608 solver.cpp:244]     Train net output #0: accuracy = 0.978759
I0525 13:59:28.513638 25608 solver.cpp:244]     Train net output #1: loss = 0.0538361 (* 1 = 0.0538361 loss)
I0525 13:59:28.513643 25608 sgd_solver.cpp:106] Iteration 4820, lr = 0.001
I0525 13:59:43.811933 25608 solver.cpp:228] Iteration 4840, loss = 0.0431295
I0525 13:59:43.812032 25608 solver.cpp:244]     Train net output #0: accuracy = 0.98395
I0525 13:59:43.812048 25608 solver.cpp:244]     Train net output #1: loss = 0.0431295 (* 1 = 0.0431295 loss)
I0525 13:59:43.812053 25608 sgd_solver.cpp:106] Iteration 4840, lr = 0.001
I0525 13:59:59.103719 25608 solver.cpp:228] Iteration 4860, loss = 0.0469774
I0525 13:59:59.103741 25608 solver.cpp:244]     Train net output #0: accuracy = 0.981486
I0525 13:59:59.103749 25608 solver.cpp:244]     Train net output #1: loss = 0.0469774 (* 1 = 0.0469774 loss)
I0525 13:59:59.103752 25608 sgd_solver.cpp:106] Iteration 4860, lr = 0.001
I0525 14:00:14.415397 25608 solver.cpp:228] Iteration 4880, loss = 0.0494279
I0525 14:00:14.415487 25608 solver.cpp:244]     Train net output #0: accuracy = 0.9804
I0525 14:00:14.415495 25608 solver.cpp:244]     Train net output #1: loss = 0.0494279 (* 1 = 0.0494279 loss)
I0525 14:00:14.415500 25608 sgd_solver.cpp:106] Iteration 4880, lr = 0.001
I0525 14:00:29.328966 25608 solver.cpp:337] Iteration 4900, Testing net (#0)
I0525 14:00:29.842345 25608 solver.cpp:404]     Test net output #0: accuracy = 0.98301
I0525 14:00:29.842380 25608 solver.cpp:404]     Test net output #1: loss = 0.0439807 (* 1 = 0.0439807 loss)
I0525 14:00:30.290650 25608 solver.cpp:228] Iteration 4900, loss = 0.0436888
I0525 14:00:30.290674 25608 solver.cpp:244]     Train net output #0: accuracy = 0.9831
I0525 14:00:30.290681 25608 solver.cpp:244]     Train net output #1: loss = 0.0436888 (* 1 = 0.0436888 loss)
I0525 14:00:30.290686 25608 sgd_solver.cpp:106] Iteration 4900, lr = 0.001
I0525 14:00:45.849035 25608 solver.cpp:228] Iteration 4920, loss = 0.0432016
I0525 14:00:45.849161 25608 solver.cpp:244]     Train net output #0: accuracy = 0.98362
I0525 14:00:45.849170 25608 solver.cpp:244]     Train net output #1: loss = 0.0432016 (* 1 = 0.0432016 loss)
I0525 14:00:45.849175 25608 sgd_solver.cpp:106] Iteration 4920, lr = 0.001
I0525 14:01:01.410130 25608 solver.cpp:228] Iteration 4940, loss = 0.0324516
I0525 14:01:01.410156 25608 solver.cpp:244]     Train net output #0: accuracy = 0.987992
I0525 14:01:01.410163 25608 solver.cpp:244]     Train net output #1: loss = 0.0324516 (* 1 = 0.0324516 loss)
I0525 14:01:01.410168 25608 sgd_solver.cpp:106] Iteration 4940, lr = 0.001
I0525 14:01:17.048357 25608 solver.cpp:228] Iteration 4960, loss = 0.0428197
I0525 14:01:17.048454 25608 solver.cpp:244]     Train net output #0: accuracy = 0.985467
I0525 14:01:17.048465 25608 solver.cpp:244]     Train net output #1: loss = 0.0428197 (* 1 = 0.0428197 loss)
I0525 14:01:17.048470 25608 sgd_solver.cpp:106] Iteration 4960, lr = 0.001
I0525 14:01:32.367458 25608 solver.cpp:228] Iteration 4980, loss = 0.04414
I0525 14:01:32.367482 25608 solver.cpp:244]     Train net output #0: accuracy = 0.98314
I0525 14:01:32.367489 25608 solver.cpp:244]     Train net output #1: loss = 0.04414 (* 1 = 0.04414 loss)
I0525 14:01:32.367493 25608 sgd_solver.cpp:106] Iteration 4980, lr = 0.001
I0525 14:01:47.328011 25608 solver.cpp:454] Snapshotting to binary proto file data/models/segnet_iter_5000.caffemodel
I0525 14:01:47.345976 25608 sgd_solver.cpp:273] Snapshotting solver state to binary proto file data/models/segnet_iter_5000.solverstate
I0525 14:01:47.355096 25608 solver.cpp:337] Iteration 5000, Testing net (#0)
I0525 14:01:47.942919 25608 solver.cpp:404]     Test net output #0: accuracy = 0.981311
I0525 14:01:47.942946 25608 solver.cpp:404]     Test net output #1: loss = 0.0491673 (* 1 = 0.0491673 loss)
I0525 14:01:48.404904 25608 solver.cpp:228] Iteration 5000, loss = 0.0437149
I0525 14:01:48.404929 25608 solver.cpp:244]     Train net output #0: accuracy = 0.983472
I0525 14:01:48.404937 25608 solver.cpp:244]     Train net output #1: loss = 0.0437149 (* 1 = 0.0437149 loss)
I0525 14:01:48.404942 25608 sgd_solver.cpp:106] Iteration 5000, lr = 0.001
I0525 14:02:03.956459 25608 solver.cpp:228] Iteration 5020, loss = 0.0418438
I0525 14:02:03.956486 25608 solver.cpp:244]     Train net output #0: accuracy = 0.984321
I0525 14:02:03.956493 25608 solver.cpp:244]     Train net output #1: loss = 0.0418438 (* 1 = 0.0418438 loss)
I0525 14:02:03.956498 25608 sgd_solver.cpp:106] Iteration 5020, lr = 0.001
I0525 14:02:19.634104 25608 solver.cpp:228] Iteration 5040, loss = 0.043535
I0525 14:02:19.634182 25608 solver.cpp:244]     Train net output #0: accuracy = 0.982497
I0525 14:02:19.634191 25608 solver.cpp:244]     Train net output #1: loss = 0.043535 (* 1 = 0.043535 loss)
I0525 14:02:19.634196 25608 sgd_solver.cpp:106] Iteration 5040, lr = 0.001
I0525 14:02:35.211362 25608 solver.cpp:228] Iteration 5060, loss = 0.0331378
I0525 14:02:35.211388 25608 solver.cpp:244]     Train net output #0: accuracy = 0.988135
I0525 14:02:35.211395 25608 solver.cpp:244]     Train net output #1: loss = 0.0331378 (* 1 = 0.0331378 loss)
I0525 14:02:35.211400 25608 sgd_solver.cpp:106] Iteration 5060, lr = 0.001
I0525 14:02:51.010416 25608 solver.cpp:228] Iteration 5080, loss = 0.0440233
I0525 14:02:51.010500 25608 solver.cpp:244]     Train net output #0: accuracy = 0.983311
I0525 14:02:51.010510 25608 solver.cpp:244]     Train net output #1: loss = 0.0440233 (* 1 = 0.0440233 loss)
I0525 14:02:51.010514 25608 sgd_solver.cpp:106] Iteration 5080, lr = 0.001
I0525 14:03:06.108785 25608 solver.cpp:337] Iteration 5100, Testing net (#0)
I0525 14:03:06.627475 25608 solver.cpp:404]     Test net output #0: accuracy = 0.987646
I0525 14:03:06.627501 25608 solver.cpp:404]     Test net output #1: loss = 0.0374575 (* 1 = 0.0374575 loss)
I0525 14:03:07.081518 25608 solver.cpp:228] Iteration 5100, loss = 0.035645
I0525 14:03:07.081543 25608 solver.cpp:244]     Train net output #0: accuracy = 0.986592
I0525 14:03:07.081550 25608 solver.cpp:244]     Train net output #1: loss = 0.035645 (* 1 = 0.035645 loss)
I0525 14:03:07.081554 25608 sgd_solver.cpp:106] Iteration 5100, lr = 0.001
I0525 14:03:22.566475 25608 solver.cpp:228] Iteration 5120, loss = 0.0377792
I0525 14:03:22.566596 25608 solver.cpp:244]     Train net output #0: accuracy = 0.985585
I0525 14:03:22.566606 25608 solver.cpp:244]     Train net output #1: loss = 0.0377792 (* 1 = 0.0377792 loss)
I0525 14:03:22.566612 25608 sgd_solver.cpp:106] Iteration 5120, lr = 0.001
I0525 14:03:38.045119 25608 solver.cpp:228] Iteration 5140, loss = 0.0407592
I0525 14:03:38.045143 25608 solver.cpp:244]     Train net output #0: accuracy = 0.982226
I0525 14:03:38.045150 25608 solver.cpp:244]     Train net output #1: loss = 0.0407592 (* 1 = 0.0407592 loss)
I0525 14:03:38.045155 25608 sgd_solver.cpp:106] Iteration 5140, lr = 0.001
I0525 14:03:53.481904 25608 solver.cpp:228] Iteration 5160, loss = 0.0484123
I0525 14:03:53.482020 25608 solver.cpp:244]     Train net output #0: accuracy = 0.98005
I0525 14:03:53.482030 25608 solver.cpp:244]     Train net output #1: loss = 0.0484123 (* 1 = 0.0484123 loss)
I0525 14:03:53.482034 25608 sgd_solver.cpp:106] Iteration 5160, lr = 0.001
I0525 14:04:09.008692 25608 solver.cpp:228] Iteration 5180, loss = 0.0573957
I0525 14:04:09.008718 25608 solver.cpp:244]     Train net output #0: accuracy = 0.976637
I0525 14:04:09.008724 25608 solver.cpp:244]     Train net output #1: loss = 0.0573957 (* 1 = 0.0573957 loss)
I0525 14:04:09.008729 25608 sgd_solver.cpp:106] Iteration 5180, lr = 0.001
I0525 14:04:24.241926 25608 solver.cpp:337] Iteration 5200, Testing net (#0)
I0525 14:04:24.773186 25608 solver.cpp:404]     Test net output #0: accuracy = 0.982962
I0525 14:04:24.773214 25608 solver.cpp:404]     Test net output #1: loss = 0.0453417 (* 1 = 0.0453417 loss)
I0525 14:04:25.237149 25608 solver.cpp:228] Iteration 5200, loss = 0.0358829
I0525 14:04:25.237174 25608 solver.cpp:244]     Train net output #0: accuracy = 0.98697
I0525 14:04:25.237181 25608 solver.cpp:244]     Train net output #1: loss = 0.0358829 (* 1 = 0.0358829 loss)
I0525 14:04:25.237186 25608 sgd_solver.cpp:106] Iteration 5200, lr = 0.001
I0525 14:04:41.034704 25608 solver.cpp:228] Iteration 5220, loss = 0.0540597
I0525 14:04:41.034729 25608 solver.cpp:244]     Train net output #0: accuracy = 0.977463
I0525 14:04:41.034735 25608 solver.cpp:244]     Train net output #1: loss = 0.0540597 (* 1 = 0.0540597 loss)
I0525 14:04:41.034740 25608 sgd_solver.cpp:106] Iteration 5220, lr = 0.001
I0525 14:04:56.651814 25608 solver.cpp:228] Iteration 5240, loss = 0.0503034
I0525 14:04:56.651911 25608 solver.cpp:244]     Train net output #0: accuracy = 0.979945
I0525 14:04:56.651919 25608 solver.cpp:244]     Train net output #1: loss = 0.0503034 (* 1 = 0.0503034 loss)
I0525 14:04:56.651924 25608 sgd_solver.cpp:106] Iteration 5240, lr = 0.001
I0525 14:05:12.295244 25608 solver.cpp:228] Iteration 5260, loss = 0.0376705
I0525 14:05:12.295269 25608 solver.cpp:244]     Train net output #0: accuracy = 0.985574
I0525 14:05:12.295287 25608 solver.cpp:244]     Train net output #1: loss = 0.0376705 (* 1 = 0.0376705 loss)
I0525 14:05:12.295291 25608 sgd_solver.cpp:106] Iteration 5260, lr = 0.001
I0525 14:05:27.827599 25608 solver.cpp:228] Iteration 5280, loss = 0.039975
I0525 14:05:27.827693 25608 solver.cpp:244]     Train net output #0: accuracy = 0.985395
I0525 14:05:27.827745 25608 solver.cpp:244]     Train net output #1: loss = 0.039975 (* 1 = 0.039975 loss)
I0525 14:05:27.827751 25608 sgd_solver.cpp:106] Iteration 5280, lr = 0.001
I0525 14:05:42.861537 25608 solver.cpp:337] Iteration 5300, Testing net (#0)
I0525 14:05:43.390451 25608 solver.cpp:404]     Test net output #0: accuracy = 0.983561
I0525 14:05:43.390480 25608 solver.cpp:404]     Test net output #1: loss = 0.0437754 (* 1 = 0.0437754 loss)
I0525 14:05:43.859428 25608 solver.cpp:228] Iteration 5300, loss = 0.0384195
I0525 14:05:43.859453 25608 solver.cpp:244]     Train net output #0: accuracy = 0.985189
I0525 14:05:43.859462 25608 solver.cpp:244]     Train net output #1: loss = 0.0384195 (* 1 = 0.0384195 loss)
I0525 14:05:43.859465 25608 sgd_solver.cpp:106] Iteration 5300, lr = 0.001
I0525 14:05:59.340289 25608 solver.cpp:228] Iteration 5320, loss = 0.0376122
I0525 14:05:59.340404 25608 solver.cpp:244]     Train net output #0: accuracy = 0.98524
I0525 14:05:59.340415 25608 solver.cpp:244]     Train net output #1: loss = 0.0376122 (* 1 = 0.0376122 loss)
I0525 14:05:59.340420 25608 sgd_solver.cpp:106] Iteration 5320, lr = 0.001
I0525 14:06:15.058276 25608 solver.cpp:228] Iteration 5340, loss = 0.0486617
I0525 14:06:15.058300 25608 solver.cpp:244]     Train net output #0: accuracy = 0.981257
I0525 14:06:15.058307 25608 solver.cpp:244]     Train net output #1: loss = 0.0486617 (* 1 = 0.0486617 loss)
I0525 14:06:15.058311 25608 sgd_solver.cpp:106] Iteration 5340, lr = 0.001
I0525 14:06:30.600234 25608 solver.cpp:228] Iteration 5360, loss = 0.0459516
I0525 14:06:30.600328 25608 solver.cpp:244]     Train net output #0: accuracy = 0.981682
I0525 14:06:30.600337 25608 solver.cpp:244]     Train net output #1: loss = 0.0459516 (* 1 = 0.0459516 loss)
I0525 14:06:30.600342 25608 sgd_solver.cpp:106] Iteration 5360, lr = 0.001
I0525 14:06:46.076802 25608 solver.cpp:228] Iteration 5380, loss = 0.0410352
I0525 14:06:46.076825 25608 solver.cpp:244]     Train net output #0: accuracy = 0.983124
I0525 14:06:46.076831 25608 solver.cpp:244]     Train net output #1: loss = 0.0410352 (* 1 = 0.0410352 loss)
I0525 14:06:46.076836 25608 sgd_solver.cpp:106] Iteration 5380, lr = 0.001
I0525 14:07:00.995519 25608 solver.cpp:337] Iteration 5400, Testing net (#0)
I0525 14:07:01.512250 25608 solver.cpp:404]     Test net output #0: accuracy = 0.982713
I0525 14:07:01.512274 25608 solver.cpp:404]     Test net output #1: loss = 0.0476576 (* 1 = 0.0476576 loss)
I0525 14:07:01.963225 25608 solver.cpp:228] Iteration 5400, loss = 0.044098
I0525 14:07:01.963263 25608 solver.cpp:244]     Train net output #0: accuracy = 0.981604
I0525 14:07:01.963269 25608 solver.cpp:244]     Train net output #1: loss = 0.044098 (* 1 = 0.044098 loss)
I0525 14:07:01.963274 25608 sgd_solver.cpp:106] Iteration 5400, lr = 0.001
I0525 14:07:17.333982 25608 solver.cpp:228] Iteration 5420, loss = 0.0470773
I0525 14:07:17.334007 25608 solver.cpp:244]     Train net output #0: accuracy = 0.982129
I0525 14:07:17.334013 25608 solver.cpp:244]     Train net output #1: loss = 0.0470773 (* 1 = 0.0470773 loss)
I0525 14:07:17.334017 25608 sgd_solver.cpp:106] Iteration 5420, lr = 0.001
I0525 14:07:32.704829 25608 solver.cpp:228] Iteration 5440, loss = 0.0404047
I0525 14:07:32.704923 25608 solver.cpp:244]     Train net output #0: accuracy = 0.985923
I0525 14:07:32.704933 25608 solver.cpp:244]     Train net output #1: loss = 0.0404047 (* 1 = 0.0404047 loss)
I0525 14:07:32.704937 25608 sgd_solver.cpp:106] Iteration 5440, lr = 0.001
I0525 14:07:48.150980 25608 solver.cpp:228] Iteration 5460, loss = 0.0422715
I0525 14:07:48.151005 25608 solver.cpp:244]     Train net output #0: accuracy = 0.98347
I0525 14:07:48.151011 25608 solver.cpp:244]     Train net output #1: loss = 0.0422715 (* 1 = 0.0422715 loss)
I0525 14:07:48.151016 25608 sgd_solver.cpp:106] Iteration 5460, lr = 0.001
I0525 14:08:03.603917 25608 solver.cpp:228] Iteration 5480, loss = 0.0381291
I0525 14:08:03.604013 25608 solver.cpp:244]     Train net output #0: accuracy = 0.985705
I0525 14:08:03.604023 25608 solver.cpp:244]     Train net output #1: loss = 0.0381291 (* 1 = 0.0381291 loss)
I0525 14:08:03.604027 25608 sgd_solver.cpp:106] Iteration 5480, lr = 0.001
I0525 14:08:18.641330 25608 solver.cpp:454] Snapshotting to binary proto file data/models/segnet_iter_5500.caffemodel
I0525 14:08:18.661131 25608 sgd_solver.cpp:273] Snapshotting solver state to binary proto file data/models/segnet_iter_5500.solverstate
I0525 14:08:18.671675 25608 solver.cpp:337] Iteration 5500, Testing net (#0)
I0525 14:08:19.189827 25608 solver.cpp:404]     Test net output #0: accuracy = 0.985614
I0525 14:08:19.189852 25608 solver.cpp:404]     Test net output #1: loss = 0.0414938 (* 1 = 0.0414938 loss)
I0525 14:08:19.647472 25608 solver.cpp:228] Iteration 5500, loss = 0.0394278
I0525 14:08:19.647497 25608 solver.cpp:244]     Train net output #0: accuracy = 0.985644
I0525 14:08:19.647505 25608 solver.cpp:244]     Train net output #1: loss = 0.0394278 (* 1 = 0.0394278 loss)
I0525 14:08:19.647510 25608 sgd_solver.cpp:106] Iteration 5500, lr = 0.001
I0525 14:08:35.102035 25608 solver.cpp:228] Iteration 5520, loss = 0.0460757
I0525 14:08:35.102152 25608 solver.cpp:244]     Train net output #0: accuracy = 0.981341
I0525 14:08:35.102162 25608 solver.cpp:244]     Train net output #1: loss = 0.0460757 (* 1 = 0.0460757 loss)
I0525 14:08:35.102167 25608 sgd_solver.cpp:106] Iteration 5520, lr = 0.001
I0525 14:08:50.556800 25608 solver.cpp:228] Iteration 5540, loss = 0.0349718
I0525 14:08:50.556824 25608 solver.cpp:244]     Train net output #0: accuracy = 0.987299
I0525 14:08:50.556843 25608 solver.cpp:244]     Train net output #1: loss = 0.0349718 (* 1 = 0.0349718 loss)
I0525 14:08:50.556849 25608 sgd_solver.cpp:106] Iteration 5540, lr = 0.001
I0525 14:09:05.994920 25608 solver.cpp:228] Iteration 5560, loss = 0.0355346
I0525 14:09:05.995014 25608 solver.cpp:244]     Train net output #0: accuracy = 0.987321
I0525 14:09:05.995024 25608 solver.cpp:244]     Train net output #1: loss = 0.0355346 (* 1 = 0.0355346 loss)
I0525 14:09:05.995028 25608 sgd_solver.cpp:106] Iteration 5560, lr = 0.001
I0525 14:09:21.636011 25608 solver.cpp:228] Iteration 5580, loss = 0.0372968
I0525 14:09:21.636036 25608 solver.cpp:244]     Train net output #0: accuracy = 0.985375
I0525 14:09:21.636044 25608 solver.cpp:244]     Train net output #1: loss = 0.0372968 (* 1 = 0.0372968 loss)
I0525 14:09:21.636049 25608 sgd_solver.cpp:106] Iteration 5580, lr = 0.001
I0525 14:09:36.626984 25608 solver.cpp:337] Iteration 5600, Testing net (#0)
I0525 14:09:37.141352 25608 solver.cpp:404]     Test net output #0: accuracy = 0.978768
I0525 14:09:37.141386 25608 solver.cpp:404]     Test net output #1: loss = 0.0501264 (* 1 = 0.0501264 loss)
I0525 14:09:37.593616 25608 solver.cpp:228] Iteration 5600, loss = 0.0399877
I0525 14:09:37.593639 25608 solver.cpp:244]     Train net output #0: accuracy = 0.98593
I0525 14:09:37.593647 25608 solver.cpp:244]     Train net output #1: loss = 0.0399877 (* 1 = 0.0399877 loss)
I0525 14:09:37.593652 25608 sgd_solver.cpp:106] Iteration 5600, lr = 0.001
I0525 14:09:53.039487 25608 solver.cpp:228] Iteration 5620, loss = 0.0385074
I0525 14:09:53.039510 25608 solver.cpp:244]     Train net output #0: accuracy = 0.986135
I0525 14:09:53.039517 25608 solver.cpp:244]     Train net output #1: loss = 0.0385074 (* 1 = 0.0385074 loss)
I0525 14:09:53.039521 25608 sgd_solver.cpp:106] Iteration 5620, lr = 0.001
