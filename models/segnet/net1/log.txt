I0625 13:37:52.451298 29029 caffe.cpp:185] Using GPUs 1
I0625 13:37:52.467531 29029 caffe.cpp:190] GPU 1: GeForce GTX TITAN X
I0625 13:37:52.828577 29029 solver.cpp:48] Initializing solver from parameters: 
test_iter: 16
test_interval: 100
base_lr: 0.01
display: 20
max_iter: 4000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 2000
snapshot: 500
snapshot_prefix: "data/models/segnet"
solver_mode: GPU
device_id: 1
net: "models/segnet/train_val.prototxt"
test_initialization: true
iter_size: 1
I0625 13:37:52.828690 29029 solver.cpp:91] Creating training net from net file: models/segnet/train_val.prototxt
I0625 13:37:52.830127 29029 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0625 13:37:52.830534 29029 net.cpp:49] Initializing net from parameters: 
name: "segnet"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "DenseImageData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_value: 100
    crop_h: 224
    crop_w: 288
  }
  dense_image_data_param {
    source: "data/train_seg.txt"
    batch_size: 32
    shuffle: true
    new_height: 224
    new_width: 310
    is_color: false
    root_folder: "data/raw/train/"
  }
}
layer {
  name: "conv1_1"
  type: "Convolution"
  bottom: "data"
  top: "conv1_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_1"
  type: "BatchNorm"
  bottom: "conv1_1"
  top: "conv1_1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale1_1"
  type: "Scale"
  bottom: "conv1_1"
  top: "conv1_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1_1"
  type: "ReLU"
  bottom: "conv1_1"
  top: "conv1_1"
}
layer {
  name: "conv1_2"
  type: "Convolution"
  bottom: "conv1_1"
  top: "conv1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_2"
  type: "BatchNorm"
  bottom: "conv1_2"
  top: "conv1_2"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale1_2"
  type: "Scale"
  bottom: "conv1_2"
  top: "conv1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1_2"
  type: "ReLU"
  bottom: "conv1_2"
  top: "conv1_2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1_2"
  top: "pool1"
  top: "pool1_mask"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2"
  top: "pool2_mask"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_1"
  type: "BatchNorm"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale3_1"
  type: "Scale"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv3_2"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv3_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_2"
  type: "BatchNorm"
  bottom: "conv3_2"
  top: "conv3_2"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale3_2"
  type: "Scale"
  bottom: "conv3_2"
  top: "conv3_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_2"
  type: "ReLU"
  bottom: "conv3_2"
  top: "conv3_2"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3_2"
  top: "pool3"
  top: "pool3_mask"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4"
  top: "pool4_mask"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv5_1"
  type: "Convolution"
  bottom: "pool4"
  top: "conv5_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn5_1"
  type: "BatchNorm"
  bottom: "conv5_1"
  top: "conv5_1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale5_1"
  type: "Scale"
  bottom: "conv5_1"
  top: "conv5_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5_1"
  type: "ReLU"
  bottom: "conv5_1"
  top: "conv5_1"
}
layer {
  name: "conv5_2"
  type: "Convolution"
  bottom: "conv5_1"
  top: "conv5_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn5_2"
  type: "BatchNorm"
  bottom: "conv5_2"
  top: "conv5_2"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale5_2"
  type: "Scale"
  bottom: "conv5_2"
  top: "conv5_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5_2"
  type: "ReLU"
  bottom: "conv5_2"
  top: "conv5_2"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5_2"
  top: "pool5"
  top: "pool5_mask"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "upsample5"
  type: "Upsample"
  bottom: "pool5"
  bottom: "pool5_mask"
  top: "pool5_D"
  upsample_param {
    scale: 2
  }
}
layer {
  name: "conv5_2_D"
  type: "Convolution"
  bottom: "pool5_D"
  top: "conv5_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn5_2_D"
  type: "BatchNorm"
  bottom: "conv5_2_D"
  top: "conv5_2_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale5_2_D"
  type: "Scale"
  bottom: "conv5_2_D"
  top: "conv5_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5_2_D"
  type: "ReLU"
  bottom: "conv5_2_D"
  top: "conv5_2_D"
}
layer {
  name: "conv5_1_D"
  type: "Convolution"
  bottom: "conv5_2_D"
  top: "conv5_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn5_1_D"
  type: "BatchNorm"
  bottom: "conv5_1_D"
  top: "conv5_1_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale5_1_D"
  type: "Scale"
  bottom: "conv5_1_D"
  top: "conv5_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5_1_D"
  type: "ReLU"
  bottom: "conv5_1_D"
  top: "conv5_1_D"
}
layer {
  name: "upsample4"
  type: "Upsample"
  bottom: "conv5_1_D"
  bottom: "pool4_mask"
  top: "pool4_D"
  upsample_param {
    scale: 2
  }
}
layer {
  name: "conv4_2_D"
  type: "Convolution"
  bottom: "pool4_D"
  top: "conv4_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2_D"
  type: "BatchNorm"
  bottom: "conv4_2_D"
  top: "conv4_2_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale4_2_D"
  type: "Scale"
  bottom: "conv4_2_D"
  top: "conv4_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4_2_D"
  type: "ReLU"
  bottom: "conv4_2_D"
  top: "conv4_2_D"
}
layer {
  name: "conv4_1_D"
  type: "Convolution"
  bottom: "conv4_2_D"
  top: "conv4_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1_D"
  type: "BatchNorm"
  bottom: "conv4_1_D"
  top: "conv4_1_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale4_1_D"
  type: "Scale"
  bottom: "conv4_1_D"
  top: "conv4_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4_1_D"
  type: "ReLU"
  bottom: "conv4_1_D"
  top: "conv4_1_D"
}
layer {
  name: "upsample3"
  type: "Upsample"
  bottom: "conv4_1_D"
  bottom: "pool3_mask"
  top: "pool3_D"
  upsample_param {
    scale: 2
  }
}
layer {
  name: "conv3_2_D"
  type: "Convolution"
  bottom: "pool3_D"
  top: "conv3_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3_2_D"
  type: "BatchNorm"
  bottom: "conv3_2_D"
  top: "conv3_2_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale3_2_D"
  type: "Scale"
  bottom: "conv3_2_D"
  top: "conv3_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_2_D"
  type: "ReLU"
  bottom: "conv3_2_D"
  top: "conv3_2_D"
}
layer {
  name: "conv3_1_D"
  type: "Convolution"
  bottom: "conv3_2_D"
  top: "conv3_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_1_D"
  type: "BatchNorm"
  bottom: "conv3_1_D"
  top: "conv3_1_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale3_1_D"
  type: "Scale"
  bottom: "conv3_1_D"
  top: "conv3_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_1_D"
  type: "ReLU"
  bottom: "conv3_1_D"
  top: "conv3_1_D"
}
layer {
  name: "upsample2"
  type: "Upsample"
  bottom: "conv3_1_D"
  bottom: "pool2_mask"
  top: "pool2_D"
  upsample_param {
    scale: 2
  }
}
layer {
  name: "conv2_2_D"
  type: "Convolution"
  bottom: "pool2_D"
  top: "conv2_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2_D"
  type: "BatchNorm"
  bottom: "conv2_2_D"
  top: "conv2_2_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale2_2_D"
  type: "Scale"
  bottom: "conv2_2_D"
  top: "conv2_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2_2_D"
  type: "ReLU"
  bottom: "conv2_2_D"
  top: "conv2_2_D"
}
layer {
  name: "conv2_1_D"
  type: "Convolution"
  bottom: "conv2_2_D"
  top: "conv2_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_1_D"
  type: "BatchNorm"
  bottom: "conv2_1_D"
  top: "conv2_1_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale2_1_D"
  type: "Scale"
  bottom: "conv2_1_D"
  top: "conv2_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2_1_D"
  type: "ReLU"
  bottom: "conv2_1_D"
  top: "conv2_1_D"
}
layer {
  name: "upsample1"
  type: "Upsample"
  bottom: "conv2_1_D"
  bottom: "pool1_mask"
  top: "pool1_D"
  upsample_param {
    scale: 2
  }
}
layer {
  name: "conv1_2_D"
  type: "Convolution"
  bottom: "pool1_D"
  top: "conv1_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_2_D"
  type: "BatchNorm"
  bottom: "conv1_2_D"
  top: "conv1_2_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale1_2_D"
  type: "Scale"
  bottom: "conv1_2_D"
  top: "conv1_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1_2_D"
  type: "ReLU"
  bottom: "conv1_2_D"
  top: "conv1_2_D"
}
layer {
  name: "conv1_1_D"
  type: "Convolution"
  bottom: "conv1_2_D"
  top: "conv1_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 2
    bias_term: true
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "conv1_1_D"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "conv1_1_D"
  bottom: "label"
  top: "accuracy"
}
I0625 13:37:52.830818 29029 layer_factory.hpp:77] Creating layer data
I0625 13:37:52.830852 29029 net.cpp:91] Creating Layer data
I0625 13:37:52.830857 29029 net.cpp:399] data -> data
I0625 13:37:52.830878 29029 net.cpp:399] data -> label
I0625 13:37:52.831204 29029 dense_image_data_layer.cpp:38] Opening file data/train_seg.txt
I0625 13:37:52.832089 29029 dense_image_data_layer.cpp:48] Shuffling data
I0625 13:37:52.832295 29029 dense_image_data_layer.cpp:53] A total of 2024 examples.
I0625 13:37:53.076050 29029 dense_image_data_layer.cpp:92] output data size: 1,1,224,288
I0625 13:37:53.077930 29029 net.cpp:141] Setting up data
I0625 13:37:53.077950 29029 net.cpp:148] Top shape: 1 1 224 288 (64512)
I0625 13:37:53.077955 29029 net.cpp:148] Top shape: 1 1 224 288 (64512)
I0625 13:37:53.077957 29029 net.cpp:156] Memory required for data: 516096
I0625 13:37:53.077965 29029 layer_factory.hpp:77] Creating layer label_data_1_split
I0625 13:37:53.077982 29029 net.cpp:91] Creating Layer label_data_1_split
I0625 13:37:53.077988 29029 net.cpp:425] label_data_1_split <- label
I0625 13:37:53.077998 29029 net.cpp:399] label_data_1_split -> label_data_1_split_0
I0625 13:37:53.078007 29029 net.cpp:399] label_data_1_split -> label_data_1_split_1
I0625 13:37:53.078094 29029 net.cpp:141] Setting up label_data_1_split
I0625 13:37:53.078101 29029 net.cpp:148] Top shape: 1 1 224 288 (64512)
I0625 13:37:53.078104 29029 net.cpp:148] Top shape: 1 1 224 288 (64512)
I0625 13:37:53.078107 29029 net.cpp:156] Memory required for data: 1032192
I0625 13:37:53.078109 29029 layer_factory.hpp:77] Creating layer conv1_1
I0625 13:37:53.078125 29029 net.cpp:91] Creating Layer conv1_1
I0625 13:37:53.078130 29029 net.cpp:425] conv1_1 <- data
I0625 13:37:53.078135 29029 net.cpp:399] conv1_1 -> conv1_1
I0625 13:37:53.269722 29029 net.cpp:141] Setting up conv1_1
I0625 13:37:53.269747 29029 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 13:37:53.269750 29029 net.cpp:156] Memory required for data: 9289728
I0625 13:37:53.269762 29029 layer_factory.hpp:77] Creating layer bn1_1
I0625 13:37:53.269778 29029 net.cpp:91] Creating Layer bn1_1
I0625 13:37:53.269781 29029 net.cpp:425] bn1_1 <- conv1_1
I0625 13:37:53.269786 29029 net.cpp:386] bn1_1 -> conv1_1 (in-place)
I0625 13:37:53.269976 29029 net.cpp:141] Setting up bn1_1
I0625 13:37:53.269985 29029 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 13:37:53.269987 29029 net.cpp:156] Memory required for data: 17547264
I0625 13:37:53.269996 29029 layer_factory.hpp:77] Creating layer scale1_1
I0625 13:37:53.270005 29029 net.cpp:91] Creating Layer scale1_1
I0625 13:37:53.270009 29029 net.cpp:425] scale1_1 <- conv1_1
I0625 13:37:53.270014 29029 net.cpp:386] scale1_1 -> conv1_1 (in-place)
I0625 13:37:53.270047 29029 layer_factory.hpp:77] Creating layer scale1_1
I0625 13:37:53.270229 29029 net.cpp:141] Setting up scale1_1
I0625 13:37:53.270237 29029 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 13:37:53.270241 29029 net.cpp:156] Memory required for data: 25804800
I0625 13:37:53.270246 29029 layer_factory.hpp:77] Creating layer relu1_1
I0625 13:37:53.270253 29029 net.cpp:91] Creating Layer relu1_1
I0625 13:37:53.270256 29029 net.cpp:425] relu1_1 <- conv1_1
I0625 13:37:53.270259 29029 net.cpp:386] relu1_1 -> conv1_1 (in-place)
I0625 13:37:53.270524 29029 net.cpp:141] Setting up relu1_1
I0625 13:37:53.270535 29029 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 13:37:53.270539 29029 net.cpp:156] Memory required for data: 34062336
I0625 13:37:53.270541 29029 layer_factory.hpp:77] Creating layer conv1_2
I0625 13:37:53.270550 29029 net.cpp:91] Creating Layer conv1_2
I0625 13:37:53.270553 29029 net.cpp:425] conv1_2 <- conv1_1
I0625 13:37:53.270557 29029 net.cpp:399] conv1_2 -> conv1_2
I0625 13:37:53.272127 29029 net.cpp:141] Setting up conv1_2
I0625 13:37:53.272140 29029 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 13:37:53.272143 29029 net.cpp:156] Memory required for data: 42319872
I0625 13:37:53.272148 29029 layer_factory.hpp:77] Creating layer bn1_2
I0625 13:37:53.272155 29029 net.cpp:91] Creating Layer bn1_2
I0625 13:37:53.272157 29029 net.cpp:425] bn1_2 <- conv1_2
I0625 13:37:53.272161 29029 net.cpp:386] bn1_2 -> conv1_2 (in-place)
I0625 13:37:53.272346 29029 net.cpp:141] Setting up bn1_2
I0625 13:37:53.272353 29029 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 13:37:53.272356 29029 net.cpp:156] Memory required for data: 50577408
I0625 13:37:53.272366 29029 layer_factory.hpp:77] Creating layer scale1_2
I0625 13:37:53.272372 29029 net.cpp:91] Creating Layer scale1_2
I0625 13:37:53.272377 29029 net.cpp:425] scale1_2 <- conv1_2
I0625 13:37:53.272397 29029 net.cpp:386] scale1_2 -> conv1_2 (in-place)
I0625 13:37:53.272430 29029 layer_factory.hpp:77] Creating layer scale1_2
I0625 13:37:53.273167 29029 net.cpp:141] Setting up scale1_2
I0625 13:37:53.273178 29029 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 13:37:53.273181 29029 net.cpp:156] Memory required for data: 58834944
I0625 13:37:53.273186 29029 layer_factory.hpp:77] Creating layer relu1_2
I0625 13:37:53.273191 29029 net.cpp:91] Creating Layer relu1_2
I0625 13:37:53.273195 29029 net.cpp:425] relu1_2 <- conv1_2
I0625 13:37:53.273198 29029 net.cpp:386] relu1_2 -> conv1_2 (in-place)
I0625 13:37:53.273334 29029 net.cpp:141] Setting up relu1_2
I0625 13:37:53.273344 29029 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 13:37:53.273346 29029 net.cpp:156] Memory required for data: 67092480
I0625 13:37:53.273350 29029 layer_factory.hpp:77] Creating layer pool1
I0625 13:37:53.273352 29029 layer_factory.cpp:90] cuDNN does not support multiple tops. Using Caffe's own pooling layer.
I0625 13:37:53.273357 29029 net.cpp:91] Creating Layer pool1
I0625 13:37:53.273360 29029 net.cpp:425] pool1 <- conv1_2
I0625 13:37:53.273363 29029 net.cpp:399] pool1 -> pool1
I0625 13:37:53.273371 29029 net.cpp:399] pool1 -> pool1_mask
I0625 13:37:53.273416 29029 net.cpp:141] Setting up pool1
I0625 13:37:53.273422 29029 net.cpp:148] Top shape: 1 32 112 144 (516096)
I0625 13:37:53.273425 29029 net.cpp:148] Top shape: 1 32 112 144 (516096)
I0625 13:37:53.273427 29029 net.cpp:156] Memory required for data: 71221248
I0625 13:37:53.273430 29029 layer_factory.hpp:77] Creating layer conv2_1
I0625 13:37:53.273437 29029 net.cpp:91] Creating Layer conv2_1
I0625 13:37:53.273440 29029 net.cpp:425] conv2_1 <- pool1
I0625 13:37:53.273445 29029 net.cpp:399] conv2_1 -> conv2_1
I0625 13:37:53.274416 29029 net.cpp:141] Setting up conv2_1
I0625 13:37:53.274428 29029 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 13:37:53.274432 29029 net.cpp:156] Memory required for data: 75350016
I0625 13:37:53.274436 29029 layer_factory.hpp:77] Creating layer bn2_1
I0625 13:37:53.274442 29029 net.cpp:91] Creating Layer bn2_1
I0625 13:37:53.274446 29029 net.cpp:425] bn2_1 <- conv2_1
I0625 13:37:53.274449 29029 net.cpp:386] bn2_1 -> conv2_1 (in-place)
I0625 13:37:53.274596 29029 net.cpp:141] Setting up bn2_1
I0625 13:37:53.274605 29029 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 13:37:53.274607 29029 net.cpp:156] Memory required for data: 79478784
I0625 13:37:53.274612 29029 layer_factory.hpp:77] Creating layer scale2_1
I0625 13:37:53.274618 29029 net.cpp:91] Creating Layer scale2_1
I0625 13:37:53.274621 29029 net.cpp:425] scale2_1 <- conv2_1
I0625 13:37:53.274626 29029 net.cpp:386] scale2_1 -> conv2_1 (in-place)
I0625 13:37:53.274657 29029 layer_factory.hpp:77] Creating layer scale2_1
I0625 13:37:53.274751 29029 net.cpp:141] Setting up scale2_1
I0625 13:37:53.274760 29029 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 13:37:53.274763 29029 net.cpp:156] Memory required for data: 83607552
I0625 13:37:53.274771 29029 layer_factory.hpp:77] Creating layer relu2_1
I0625 13:37:53.274776 29029 net.cpp:91] Creating Layer relu2_1
I0625 13:37:53.274780 29029 net.cpp:425] relu2_1 <- conv2_1
I0625 13:37:53.274783 29029 net.cpp:386] relu2_1 -> conv2_1 (in-place)
I0625 13:37:53.275043 29029 net.cpp:141] Setting up relu2_1
I0625 13:37:53.275054 29029 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 13:37:53.275056 29029 net.cpp:156] Memory required for data: 87736320
I0625 13:37:53.275060 29029 layer_factory.hpp:77] Creating layer conv2_2
I0625 13:37:53.275068 29029 net.cpp:91] Creating Layer conv2_2
I0625 13:37:53.275071 29029 net.cpp:425] conv2_2 <- conv2_1
I0625 13:37:53.275076 29029 net.cpp:399] conv2_2 -> conv2_2
I0625 13:37:53.276052 29029 net.cpp:141] Setting up conv2_2
I0625 13:37:53.276065 29029 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 13:37:53.276068 29029 net.cpp:156] Memory required for data: 91865088
I0625 13:37:53.276072 29029 layer_factory.hpp:77] Creating layer bn2_2
I0625 13:37:53.276080 29029 net.cpp:91] Creating Layer bn2_2
I0625 13:37:53.276093 29029 net.cpp:425] bn2_2 <- conv2_2
I0625 13:37:53.276098 29029 net.cpp:386] bn2_2 -> conv2_2 (in-place)
I0625 13:37:53.276252 29029 net.cpp:141] Setting up bn2_2
I0625 13:37:53.276260 29029 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 13:37:53.276262 29029 net.cpp:156] Memory required for data: 95993856
I0625 13:37:53.276268 29029 layer_factory.hpp:77] Creating layer scale2_2
I0625 13:37:53.276274 29029 net.cpp:91] Creating Layer scale2_2
I0625 13:37:53.276278 29029 net.cpp:425] scale2_2 <- conv2_2
I0625 13:37:53.276281 29029 net.cpp:386] scale2_2 -> conv2_2 (in-place)
I0625 13:37:53.276311 29029 layer_factory.hpp:77] Creating layer scale2_2
I0625 13:37:53.276409 29029 net.cpp:141] Setting up scale2_2
I0625 13:37:53.276417 29029 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 13:37:53.276418 29029 net.cpp:156] Memory required for data: 100122624
I0625 13:37:53.276423 29029 layer_factory.hpp:77] Creating layer relu2_2
I0625 13:37:53.276427 29029 net.cpp:91] Creating Layer relu2_2
I0625 13:37:53.276430 29029 net.cpp:425] relu2_2 <- conv2_2
I0625 13:37:53.276434 29029 net.cpp:386] relu2_2 -> conv2_2 (in-place)
I0625 13:37:53.276693 29029 net.cpp:141] Setting up relu2_2
I0625 13:37:53.276705 29029 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 13:37:53.276707 29029 net.cpp:156] Memory required for data: 104251392
I0625 13:37:53.276710 29029 layer_factory.hpp:77] Creating layer pool2
I0625 13:37:53.276713 29029 layer_factory.cpp:90] cuDNN does not support multiple tops. Using Caffe's own pooling layer.
I0625 13:37:53.276717 29029 net.cpp:91] Creating Layer pool2
I0625 13:37:53.276721 29029 net.cpp:425] pool2 <- conv2_2
I0625 13:37:53.276724 29029 net.cpp:399] pool2 -> pool2
I0625 13:37:53.276729 29029 net.cpp:399] pool2 -> pool2_mask
I0625 13:37:53.276763 29029 net.cpp:141] Setting up pool2
I0625 13:37:53.276768 29029 net.cpp:148] Top shape: 1 64 56 72 (258048)
I0625 13:37:53.276772 29029 net.cpp:148] Top shape: 1 64 56 72 (258048)
I0625 13:37:53.276773 29029 net.cpp:156] Memory required for data: 106315776
I0625 13:37:53.276777 29029 layer_factory.hpp:77] Creating layer conv3_1
I0625 13:37:53.276783 29029 net.cpp:91] Creating Layer conv3_1
I0625 13:37:53.276787 29029 net.cpp:425] conv3_1 <- pool2
I0625 13:37:53.276790 29029 net.cpp:399] conv3_1 -> conv3_1
I0625 13:37:53.278641 29029 net.cpp:141] Setting up conv3_1
I0625 13:37:53.278655 29029 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 13:37:53.278657 29029 net.cpp:156] Memory required for data: 108380160
I0625 13:37:53.278661 29029 layer_factory.hpp:77] Creating layer bn3_1
I0625 13:37:53.278667 29029 net.cpp:91] Creating Layer bn3_1
I0625 13:37:53.278671 29029 net.cpp:425] bn3_1 <- conv3_1
I0625 13:37:53.278676 29029 net.cpp:386] bn3_1 -> conv3_1 (in-place)
I0625 13:37:53.279402 29029 net.cpp:141] Setting up bn3_1
I0625 13:37:53.279413 29029 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 13:37:53.279417 29029 net.cpp:156] Memory required for data: 110444544
I0625 13:37:53.279422 29029 layer_factory.hpp:77] Creating layer scale3_1
I0625 13:37:53.279429 29029 net.cpp:91] Creating Layer scale3_1
I0625 13:37:53.279431 29029 net.cpp:425] scale3_1 <- conv3_1
I0625 13:37:53.279436 29029 net.cpp:386] scale3_1 -> conv3_1 (in-place)
I0625 13:37:53.279469 29029 layer_factory.hpp:77] Creating layer scale3_1
I0625 13:37:53.279553 29029 net.cpp:141] Setting up scale3_1
I0625 13:37:53.279561 29029 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 13:37:53.279563 29029 net.cpp:156] Memory required for data: 112508928
I0625 13:37:53.279567 29029 layer_factory.hpp:77] Creating layer relu3_1
I0625 13:37:53.279572 29029 net.cpp:91] Creating Layer relu3_1
I0625 13:37:53.279575 29029 net.cpp:425] relu3_1 <- conv3_1
I0625 13:37:53.279579 29029 net.cpp:386] relu3_1 -> conv3_1 (in-place)
I0625 13:37:53.279713 29029 net.cpp:141] Setting up relu3_1
I0625 13:37:53.279723 29029 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 13:37:53.279726 29029 net.cpp:156] Memory required for data: 114573312
I0625 13:37:53.279728 29029 layer_factory.hpp:77] Creating layer conv3_2
I0625 13:37:53.279745 29029 net.cpp:91] Creating Layer conv3_2
I0625 13:37:53.279749 29029 net.cpp:425] conv3_2 <- conv3_1
I0625 13:37:53.279753 29029 net.cpp:399] conv3_2 -> conv3_2
I0625 13:37:53.281487 29029 net.cpp:141] Setting up conv3_2
I0625 13:37:53.281499 29029 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 13:37:53.281502 29029 net.cpp:156] Memory required for data: 116637696
I0625 13:37:53.281507 29029 layer_factory.hpp:77] Creating layer bn3_2
I0625 13:37:53.281513 29029 net.cpp:91] Creating Layer bn3_2
I0625 13:37:53.281517 29029 net.cpp:425] bn3_2 <- conv3_2
I0625 13:37:53.281520 29029 net.cpp:386] bn3_2 -> conv3_2 (in-place)
I0625 13:37:53.281675 29029 net.cpp:141] Setting up bn3_2
I0625 13:37:53.281682 29029 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 13:37:53.281685 29029 net.cpp:156] Memory required for data: 118702080
I0625 13:37:53.281694 29029 layer_factory.hpp:77] Creating layer scale3_2
I0625 13:37:53.281700 29029 net.cpp:91] Creating Layer scale3_2
I0625 13:37:53.281703 29029 net.cpp:425] scale3_2 <- conv3_2
I0625 13:37:53.281708 29029 net.cpp:386] scale3_2 -> conv3_2 (in-place)
I0625 13:37:53.281738 29029 layer_factory.hpp:77] Creating layer scale3_2
I0625 13:37:53.281821 29029 net.cpp:141] Setting up scale3_2
I0625 13:37:53.281827 29029 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 13:37:53.281831 29029 net.cpp:156] Memory required for data: 120766464
I0625 13:37:53.281834 29029 layer_factory.hpp:77] Creating layer relu3_2
I0625 13:37:53.281839 29029 net.cpp:91] Creating Layer relu3_2
I0625 13:37:53.281841 29029 net.cpp:425] relu3_2 <- conv3_2
I0625 13:37:53.281846 29029 net.cpp:386] relu3_2 -> conv3_2 (in-place)
I0625 13:37:53.282110 29029 net.cpp:141] Setting up relu3_2
I0625 13:37:53.282121 29029 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 13:37:53.282124 29029 net.cpp:156] Memory required for data: 122830848
I0625 13:37:53.282127 29029 layer_factory.hpp:77] Creating layer pool3
I0625 13:37:53.282130 29029 layer_factory.cpp:90] cuDNN does not support multiple tops. Using Caffe's own pooling layer.
I0625 13:37:53.282135 29029 net.cpp:91] Creating Layer pool3
I0625 13:37:53.282137 29029 net.cpp:425] pool3 <- conv3_2
I0625 13:37:53.282142 29029 net.cpp:399] pool3 -> pool3
I0625 13:37:53.282147 29029 net.cpp:399] pool3 -> pool3_mask
I0625 13:37:53.282181 29029 net.cpp:141] Setting up pool3
I0625 13:37:53.282186 29029 net.cpp:148] Top shape: 1 128 28 36 (129024)
I0625 13:37:53.282189 29029 net.cpp:148] Top shape: 1 128 28 36 (129024)
I0625 13:37:53.282191 29029 net.cpp:156] Memory required for data: 123863040
I0625 13:37:53.282194 29029 layer_factory.hpp:77] Creating layer conv4_1
I0625 13:37:53.282202 29029 net.cpp:91] Creating Layer conv4_1
I0625 13:37:53.282203 29029 net.cpp:425] conv4_1 <- pool3
I0625 13:37:53.282208 29029 net.cpp:399] conv4_1 -> conv4_1
I0625 13:37:53.285717 29029 net.cpp:141] Setting up conv4_1
I0625 13:37:53.285735 29029 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 13:37:53.285742 29029 net.cpp:156] Memory required for data: 124895232
I0625 13:37:53.285748 29029 layer_factory.hpp:77] Creating layer bn4_1
I0625 13:37:53.285759 29029 net.cpp:91] Creating Layer bn4_1
I0625 13:37:53.285766 29029 net.cpp:425] bn4_1 <- conv4_1
I0625 13:37:53.285773 29029 net.cpp:386] bn4_1 -> conv4_1 (in-place)
I0625 13:37:53.286051 29029 net.cpp:141] Setting up bn4_1
I0625 13:37:53.286067 29029 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 13:37:53.286072 29029 net.cpp:156] Memory required for data: 125927424
I0625 13:37:53.286080 29029 layer_factory.hpp:77] Creating layer scale4_1
I0625 13:37:53.286087 29029 net.cpp:91] Creating Layer scale4_1
I0625 13:37:53.286089 29029 net.cpp:425] scale4_1 <- conv4_1
I0625 13:37:53.286097 29029 net.cpp:386] scale4_1 -> conv4_1 (in-place)
I0625 13:37:53.286151 29029 layer_factory.hpp:77] Creating layer scale4_1
I0625 13:37:53.286341 29029 net.cpp:141] Setting up scale4_1
I0625 13:37:53.286357 29029 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 13:37:53.286362 29029 net.cpp:156] Memory required for data: 126959616
I0625 13:37:53.286383 29029 layer_factory.hpp:77] Creating layer relu4_1
I0625 13:37:53.286398 29029 net.cpp:91] Creating Layer relu4_1
I0625 13:37:53.286403 29029 net.cpp:425] relu4_1 <- conv4_1
I0625 13:37:53.286412 29029 net.cpp:386] relu4_1 -> conv4_1 (in-place)
I0625 13:37:53.286794 29029 net.cpp:141] Setting up relu4_1
I0625 13:37:53.286806 29029 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 13:37:53.286809 29029 net.cpp:156] Memory required for data: 127991808
I0625 13:37:53.286813 29029 layer_factory.hpp:77] Creating layer conv4_2
I0625 13:37:53.286823 29029 net.cpp:91] Creating Layer conv4_2
I0625 13:37:53.286825 29029 net.cpp:425] conv4_2 <- conv4_1
I0625 13:37:53.286830 29029 net.cpp:399] conv4_2 -> conv4_2
I0625 13:37:53.292340 29029 net.cpp:141] Setting up conv4_2
I0625 13:37:53.292353 29029 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 13:37:53.292356 29029 net.cpp:156] Memory required for data: 129024000
I0625 13:37:53.292361 29029 layer_factory.hpp:77] Creating layer bn4_2
I0625 13:37:53.292368 29029 net.cpp:91] Creating Layer bn4_2
I0625 13:37:53.292371 29029 net.cpp:425] bn4_2 <- conv4_2
I0625 13:37:53.292376 29029 net.cpp:386] bn4_2 -> conv4_2 (in-place)
I0625 13:37:53.292537 29029 net.cpp:141] Setting up bn4_2
I0625 13:37:53.292544 29029 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 13:37:53.292546 29029 net.cpp:156] Memory required for data: 130056192
I0625 13:37:53.292552 29029 layer_factory.hpp:77] Creating layer scale4_2
I0625 13:37:53.292560 29029 net.cpp:91] Creating Layer scale4_2
I0625 13:37:53.292563 29029 net.cpp:425] scale4_2 <- conv4_2
I0625 13:37:53.292567 29029 net.cpp:386] scale4_2 -> conv4_2 (in-place)
I0625 13:37:53.292601 29029 layer_factory.hpp:77] Creating layer scale4_2
I0625 13:37:53.292695 29029 net.cpp:141] Setting up scale4_2
I0625 13:37:53.292701 29029 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 13:37:53.292703 29029 net.cpp:156] Memory required for data: 131088384
I0625 13:37:53.292708 29029 layer_factory.hpp:77] Creating layer relu4_2
I0625 13:37:53.292712 29029 net.cpp:91] Creating Layer relu4_2
I0625 13:37:53.292716 29029 net.cpp:425] relu4_2 <- conv4_2
I0625 13:37:53.292719 29029 net.cpp:386] relu4_2 -> conv4_2 (in-place)
I0625 13:37:53.292870 29029 net.cpp:141] Setting up relu4_2
I0625 13:37:53.292879 29029 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 13:37:53.292882 29029 net.cpp:156] Memory required for data: 132120576
I0625 13:37:53.292884 29029 layer_factory.hpp:77] Creating layer pool4
I0625 13:37:53.292887 29029 layer_factory.cpp:90] cuDNN does not support multiple tops. Using Caffe's own pooling layer.
I0625 13:37:53.292892 29029 net.cpp:91] Creating Layer pool4
I0625 13:37:53.292894 29029 net.cpp:425] pool4 <- conv4_2
I0625 13:37:53.292901 29029 net.cpp:399] pool4 -> pool4
I0625 13:37:53.292906 29029 net.cpp:399] pool4 -> pool4_mask
I0625 13:37:53.292946 29029 net.cpp:141] Setting up pool4
I0625 13:37:53.292953 29029 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 13:37:53.292956 29029 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 13:37:53.292958 29029 net.cpp:156] Memory required for data: 132636672
I0625 13:37:53.292960 29029 layer_factory.hpp:77] Creating layer conv5_1
I0625 13:37:53.292968 29029 net.cpp:91] Creating Layer conv5_1
I0625 13:37:53.292971 29029 net.cpp:425] conv5_1 <- pool4
I0625 13:37:53.292976 29029 net.cpp:399] conv5_1 -> conv5_1
I0625 13:37:53.298410 29029 net.cpp:141] Setting up conv5_1
I0625 13:37:53.298424 29029 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 13:37:53.298427 29029 net.cpp:156] Memory required for data: 132894720
I0625 13:37:53.298434 29029 layer_factory.hpp:77] Creating layer bn5_1
I0625 13:37:53.298439 29029 net.cpp:91] Creating Layer bn5_1
I0625 13:37:53.298444 29029 net.cpp:425] bn5_1 <- conv5_1
I0625 13:37:53.298449 29029 net.cpp:386] bn5_1 -> conv5_1 (in-place)
I0625 13:37:53.298606 29029 net.cpp:141] Setting up bn5_1
I0625 13:37:53.298614 29029 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 13:37:53.298616 29029 net.cpp:156] Memory required for data: 133152768
I0625 13:37:53.298632 29029 layer_factory.hpp:77] Creating layer scale5_1
I0625 13:37:53.298640 29029 net.cpp:91] Creating Layer scale5_1
I0625 13:37:53.298643 29029 net.cpp:425] scale5_1 <- conv5_1
I0625 13:37:53.298647 29029 net.cpp:386] scale5_1 -> conv5_1 (in-place)
I0625 13:37:53.298686 29029 layer_factory.hpp:77] Creating layer scale5_1
I0625 13:37:53.298799 29029 net.cpp:141] Setting up scale5_1
I0625 13:37:53.298809 29029 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 13:37:53.298812 29029 net.cpp:156] Memory required for data: 133410816
I0625 13:37:53.298820 29029 layer_factory.hpp:77] Creating layer relu5_1
I0625 13:37:53.298832 29029 net.cpp:91] Creating Layer relu5_1
I0625 13:37:53.298836 29029 net.cpp:425] relu5_1 <- conv5_1
I0625 13:37:53.298840 29029 net.cpp:386] relu5_1 -> conv5_1 (in-place)
I0625 13:37:53.299115 29029 net.cpp:141] Setting up relu5_1
I0625 13:37:53.299127 29029 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 13:37:53.299130 29029 net.cpp:156] Memory required for data: 133668864
I0625 13:37:53.299134 29029 layer_factory.hpp:77] Creating layer conv5_2
I0625 13:37:53.299144 29029 net.cpp:91] Creating Layer conv5_2
I0625 13:37:53.299146 29029 net.cpp:425] conv5_2 <- conv5_1
I0625 13:37:53.299157 29029 net.cpp:399] conv5_2 -> conv5_2
I0625 13:37:53.304872 29029 net.cpp:141] Setting up conv5_2
I0625 13:37:53.304885 29029 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 13:37:53.304888 29029 net.cpp:156] Memory required for data: 133926912
I0625 13:37:53.304893 29029 layer_factory.hpp:77] Creating layer bn5_2
I0625 13:37:53.304901 29029 net.cpp:91] Creating Layer bn5_2
I0625 13:37:53.304904 29029 net.cpp:425] bn5_2 <- conv5_2
I0625 13:37:53.304910 29029 net.cpp:386] bn5_2 -> conv5_2 (in-place)
I0625 13:37:53.305070 29029 net.cpp:141] Setting up bn5_2
I0625 13:37:53.305078 29029 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 13:37:53.305079 29029 net.cpp:156] Memory required for data: 134184960
I0625 13:37:53.305085 29029 layer_factory.hpp:77] Creating layer scale5_2
I0625 13:37:53.305091 29029 net.cpp:91] Creating Layer scale5_2
I0625 13:37:53.305094 29029 net.cpp:425] scale5_2 <- conv5_2
I0625 13:37:53.305099 29029 net.cpp:386] scale5_2 -> conv5_2 (in-place)
I0625 13:37:53.305132 29029 layer_factory.hpp:77] Creating layer scale5_2
I0625 13:37:53.305233 29029 net.cpp:141] Setting up scale5_2
I0625 13:37:53.305243 29029 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 13:37:53.305245 29029 net.cpp:156] Memory required for data: 134443008
I0625 13:37:53.305249 29029 layer_factory.hpp:77] Creating layer relu5_2
I0625 13:37:53.305254 29029 net.cpp:91] Creating Layer relu5_2
I0625 13:37:53.305258 29029 net.cpp:425] relu5_2 <- conv5_2
I0625 13:37:53.305261 29029 net.cpp:386] relu5_2 -> conv5_2 (in-place)
I0625 13:37:53.305541 29029 net.cpp:141] Setting up relu5_2
I0625 13:37:53.305552 29029 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 13:37:53.305555 29029 net.cpp:156] Memory required for data: 134701056
I0625 13:37:53.305558 29029 layer_factory.hpp:77] Creating layer pool5
I0625 13:37:53.305562 29029 layer_factory.cpp:90] cuDNN does not support multiple tops. Using Caffe's own pooling layer.
I0625 13:37:53.305568 29029 net.cpp:91] Creating Layer pool5
I0625 13:37:53.305572 29029 net.cpp:425] pool5 <- conv5_2
I0625 13:37:53.305575 29029 net.cpp:399] pool5 -> pool5
I0625 13:37:53.305582 29029 net.cpp:399] pool5 -> pool5_mask
I0625 13:37:53.305621 29029 net.cpp:141] Setting up pool5
I0625 13:37:53.305626 29029 net.cpp:148] Top shape: 1 256 7 9 (16128)
I0625 13:37:53.305629 29029 net.cpp:148] Top shape: 1 256 7 9 (16128)
I0625 13:37:53.305631 29029 net.cpp:156] Memory required for data: 134830080
I0625 13:37:53.305634 29029 layer_factory.hpp:77] Creating layer upsample5
I0625 13:37:53.305644 29029 net.cpp:91] Creating Layer upsample5
I0625 13:37:53.305647 29029 net.cpp:425] upsample5 <- pool5
I0625 13:37:53.305650 29029 net.cpp:425] upsample5 <- pool5_mask
I0625 13:37:53.305655 29029 net.cpp:399] upsample5 -> pool5_D
I0625 13:37:53.305663 29029 upsample_layer.cpp:31] Params 'pad_out_{}_' are deprecated. Please declare upsample height and width useing the upsample_h, upsample_w parameters.
I0625 13:37:53.305698 29029 net.cpp:141] Setting up upsample5
I0625 13:37:53.305703 29029 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 13:37:53.305706 29029 net.cpp:156] Memory required for data: 135088128
I0625 13:37:53.305708 29029 layer_factory.hpp:77] Creating layer conv5_2_D
I0625 13:37:53.305716 29029 net.cpp:91] Creating Layer conv5_2_D
I0625 13:37:53.305719 29029 net.cpp:425] conv5_2_D <- pool5_D
I0625 13:37:53.305723 29029 net.cpp:399] conv5_2_D -> conv5_2_D
I0625 13:37:53.311144 29029 net.cpp:141] Setting up conv5_2_D
I0625 13:37:53.311161 29029 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 13:37:53.311164 29029 net.cpp:156] Memory required for data: 135346176
I0625 13:37:53.311168 29029 layer_factory.hpp:77] Creating layer bn5_2_D
I0625 13:37:53.311177 29029 net.cpp:91] Creating Layer bn5_2_D
I0625 13:37:53.311179 29029 net.cpp:425] bn5_2_D <- conv5_2_D
I0625 13:37:53.311184 29029 net.cpp:386] bn5_2_D -> conv5_2_D (in-place)
I0625 13:37:53.311352 29029 net.cpp:141] Setting up bn5_2_D
I0625 13:37:53.311359 29029 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 13:37:53.311362 29029 net.cpp:156] Memory required for data: 135604224
I0625 13:37:53.311368 29029 layer_factory.hpp:77] Creating layer scale5_2_D
I0625 13:37:53.311373 29029 net.cpp:91] Creating Layer scale5_2_D
I0625 13:37:53.311377 29029 net.cpp:425] scale5_2_D <- conv5_2_D
I0625 13:37:53.311381 29029 net.cpp:386] scale5_2_D -> conv5_2_D (in-place)
I0625 13:37:53.311416 29029 layer_factory.hpp:77] Creating layer scale5_2_D
I0625 13:37:53.311506 29029 net.cpp:141] Setting up scale5_2_D
I0625 13:37:53.311511 29029 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 13:37:53.311514 29029 net.cpp:156] Memory required for data: 135862272
I0625 13:37:53.311528 29029 layer_factory.hpp:77] Creating layer relu5_2_D
I0625 13:37:53.311533 29029 net.cpp:91] Creating Layer relu5_2_D
I0625 13:37:53.311535 29029 net.cpp:425] relu5_2_D <- conv5_2_D
I0625 13:37:53.311539 29029 net.cpp:386] relu5_2_D -> conv5_2_D (in-place)
I0625 13:37:53.311694 29029 net.cpp:141] Setting up relu5_2_D
I0625 13:37:53.311704 29029 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 13:37:53.311707 29029 net.cpp:156] Memory required for data: 136120320
I0625 13:37:53.311709 29029 layer_factory.hpp:77] Creating layer conv5_1_D
I0625 13:37:53.311717 29029 net.cpp:91] Creating Layer conv5_1_D
I0625 13:37:53.311720 29029 net.cpp:425] conv5_1_D <- conv5_2_D
I0625 13:37:53.311725 29029 net.cpp:399] conv5_1_D -> conv5_1_D
I0625 13:37:53.317144 29029 net.cpp:141] Setting up conv5_1_D
I0625 13:37:53.317157 29029 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 13:37:53.317159 29029 net.cpp:156] Memory required for data: 136378368
I0625 13:37:53.317165 29029 layer_factory.hpp:77] Creating layer bn5_1_D
I0625 13:37:53.317173 29029 net.cpp:91] Creating Layer bn5_1_D
I0625 13:37:53.317175 29029 net.cpp:425] bn5_1_D <- conv5_1_D
I0625 13:37:53.317180 29029 net.cpp:386] bn5_1_D -> conv5_1_D (in-place)
I0625 13:37:53.317345 29029 net.cpp:141] Setting up bn5_1_D
I0625 13:37:53.317353 29029 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 13:37:53.317355 29029 net.cpp:156] Memory required for data: 136636416
I0625 13:37:53.317361 29029 layer_factory.hpp:77] Creating layer scale5_1_D
I0625 13:37:53.317368 29029 net.cpp:91] Creating Layer scale5_1_D
I0625 13:37:53.317371 29029 net.cpp:425] scale5_1_D <- conv5_1_D
I0625 13:37:53.317374 29029 net.cpp:386] scale5_1_D -> conv5_1_D (in-place)
I0625 13:37:53.317409 29029 layer_factory.hpp:77] Creating layer scale5_1_D
I0625 13:37:53.317500 29029 net.cpp:141] Setting up scale5_1_D
I0625 13:37:53.317507 29029 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 13:37:53.317509 29029 net.cpp:156] Memory required for data: 136894464
I0625 13:37:53.317513 29029 layer_factory.hpp:77] Creating layer relu5_1_D
I0625 13:37:53.317519 29029 net.cpp:91] Creating Layer relu5_1_D
I0625 13:37:53.317523 29029 net.cpp:425] relu5_1_D <- conv5_1_D
I0625 13:37:53.317525 29029 net.cpp:386] relu5_1_D -> conv5_1_D (in-place)
I0625 13:37:53.317821 29029 net.cpp:141] Setting up relu5_1_D
I0625 13:37:53.317833 29029 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 13:37:53.317836 29029 net.cpp:156] Memory required for data: 137152512
I0625 13:37:53.317839 29029 layer_factory.hpp:77] Creating layer upsample4
I0625 13:37:53.317845 29029 net.cpp:91] Creating Layer upsample4
I0625 13:37:53.317847 29029 net.cpp:425] upsample4 <- conv5_1_D
I0625 13:37:53.317853 29029 net.cpp:425] upsample4 <- pool4_mask
I0625 13:37:53.317858 29029 net.cpp:399] upsample4 -> pool4_D
I0625 13:37:53.317863 29029 upsample_layer.cpp:31] Params 'pad_out_{}_' are deprecated. Please declare upsample height and width useing the upsample_h, upsample_w parameters.
I0625 13:37:53.317890 29029 net.cpp:141] Setting up upsample4
I0625 13:37:53.317894 29029 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 13:37:53.317898 29029 net.cpp:156] Memory required for data: 138184704
I0625 13:37:53.317899 29029 layer_factory.hpp:77] Creating layer conv4_2_D
I0625 13:37:53.317909 29029 net.cpp:91] Creating Layer conv4_2_D
I0625 13:37:53.317911 29029 net.cpp:425] conv4_2_D <- pool4_D
I0625 13:37:53.317916 29029 net.cpp:399] conv4_2_D -> conv4_2_D
I0625 13:37:53.323223 29029 net.cpp:141] Setting up conv4_2_D
I0625 13:37:53.323237 29029 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 13:37:53.323240 29029 net.cpp:156] Memory required for data: 139216896
I0625 13:37:53.323246 29029 layer_factory.hpp:77] Creating layer bn4_2_D
I0625 13:37:53.323251 29029 net.cpp:91] Creating Layer bn4_2_D
I0625 13:37:53.323254 29029 net.cpp:425] bn4_2_D <- conv4_2_D
I0625 13:37:53.323259 29029 net.cpp:386] bn4_2_D -> conv4_2_D (in-place)
I0625 13:37:53.323446 29029 net.cpp:141] Setting up bn4_2_D
I0625 13:37:53.323452 29029 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 13:37:53.323456 29029 net.cpp:156] Memory required for data: 140249088
I0625 13:37:53.323462 29029 layer_factory.hpp:77] Creating layer scale4_2_D
I0625 13:37:53.323467 29029 net.cpp:91] Creating Layer scale4_2_D
I0625 13:37:53.323470 29029 net.cpp:425] scale4_2_D <- conv4_2_D
I0625 13:37:53.323473 29029 net.cpp:386] scale4_2_D -> conv4_2_D (in-place)
I0625 13:37:53.323509 29029 layer_factory.hpp:77] Creating layer scale4_2_D
I0625 13:37:53.323608 29029 net.cpp:141] Setting up scale4_2_D
I0625 13:37:53.323616 29029 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 13:37:53.323618 29029 net.cpp:156] Memory required for data: 141281280
I0625 13:37:53.323623 29029 layer_factory.hpp:77] Creating layer relu4_2_D
I0625 13:37:53.323629 29029 net.cpp:91] Creating Layer relu4_2_D
I0625 13:37:53.323632 29029 net.cpp:425] relu4_2_D <- conv4_2_D
I0625 13:37:53.323635 29029 net.cpp:386] relu4_2_D -> conv4_2_D (in-place)
I0625 13:37:53.323921 29029 net.cpp:141] Setting up relu4_2_D
I0625 13:37:53.323933 29029 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 13:37:53.323936 29029 net.cpp:156] Memory required for data: 142313472
I0625 13:37:53.323940 29029 layer_factory.hpp:77] Creating layer conv4_1_D
I0625 13:37:53.323947 29029 net.cpp:91] Creating Layer conv4_1_D
I0625 13:37:53.323951 29029 net.cpp:425] conv4_1_D <- conv4_2_D
I0625 13:37:53.323956 29029 net.cpp:399] conv4_1_D -> conv4_1_D
I0625 13:37:53.327253 29029 net.cpp:141] Setting up conv4_1_D
I0625 13:37:53.327266 29029 net.cpp:148] Top shape: 1 128 28 36 (129024)
I0625 13:37:53.327270 29029 net.cpp:156] Memory required for data: 142829568
I0625 13:37:53.327273 29029 layer_factory.hpp:77] Creating layer bn4_1_D
I0625 13:37:53.327281 29029 net.cpp:91] Creating Layer bn4_1_D
I0625 13:37:53.327285 29029 net.cpp:425] bn4_1_D <- conv4_1_D
I0625 13:37:53.327288 29029 net.cpp:386] bn4_1_D -> conv4_1_D (in-place)
I0625 13:37:53.327467 29029 net.cpp:141] Setting up bn4_1_D
I0625 13:37:53.327474 29029 net.cpp:148] Top shape: 1 128 28 36 (129024)
I0625 13:37:53.327476 29029 net.cpp:156] Memory required for data: 143345664
I0625 13:37:53.327482 29029 layer_factory.hpp:77] Creating layer scale4_1_D
I0625 13:37:53.327489 29029 net.cpp:91] Creating Layer scale4_1_D
I0625 13:37:53.327502 29029 net.cpp:425] scale4_1_D <- conv4_1_D
I0625 13:37:53.327507 29029 net.cpp:386] scale4_1_D -> conv4_1_D (in-place)
I0625 13:37:53.327546 29029 layer_factory.hpp:77] Creating layer scale4_1_D
I0625 13:37:53.327646 29029 net.cpp:141] Setting up scale4_1_D
I0625 13:37:53.327652 29029 net.cpp:148] Top shape: 1 128 28 36 (129024)
I0625 13:37:53.327656 29029 net.cpp:156] Memory required for data: 143861760
I0625 13:37:53.327661 29029 layer_factory.hpp:77] Creating layer relu4_1_D
I0625 13:37:53.327672 29029 net.cpp:91] Creating Layer relu4_1_D
I0625 13:37:53.327674 29029 net.cpp:425] relu4_1_D <- conv4_1_D
I0625 13:37:53.327678 29029 net.cpp:386] relu4_1_D -> conv4_1_D (in-place)
I0625 13:37:53.327831 29029 net.cpp:141] Setting up relu4_1_D
I0625 13:37:53.327839 29029 net.cpp:148] Top shape: 1 128 28 36 (129024)
I0625 13:37:53.327842 29029 net.cpp:156] Memory required for data: 144377856
I0625 13:37:53.327846 29029 layer_factory.hpp:77] Creating layer upsample3
I0625 13:37:53.327854 29029 net.cpp:91] Creating Layer upsample3
I0625 13:37:53.327857 29029 net.cpp:425] upsample3 <- conv4_1_D
I0625 13:37:53.327862 29029 net.cpp:425] upsample3 <- pool3_mask
I0625 13:37:53.327867 29029 net.cpp:399] upsample3 -> pool3_D
I0625 13:37:53.327872 29029 upsample_layer.cpp:31] Params 'pad_out_{}_' are deprecated. Please declare upsample height and width useing the upsample_h, upsample_w parameters.
I0625 13:37:53.327898 29029 net.cpp:141] Setting up upsample3
I0625 13:37:53.327901 29029 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 13:37:53.327903 29029 net.cpp:156] Memory required for data: 146442240
I0625 13:37:53.327906 29029 layer_factory.hpp:77] Creating layer conv3_2_D
I0625 13:37:53.327915 29029 net.cpp:91] Creating Layer conv3_2_D
I0625 13:37:53.327919 29029 net.cpp:425] conv3_2_D <- pool3_D
I0625 13:37:53.327922 29029 net.cpp:399] conv3_2_D -> conv3_2_D
I0625 13:37:53.330418 29029 net.cpp:141] Setting up conv3_2_D
I0625 13:37:53.330431 29029 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 13:37:53.330435 29029 net.cpp:156] Memory required for data: 148506624
I0625 13:37:53.330440 29029 layer_factory.hpp:77] Creating layer bn3_2_D
I0625 13:37:53.330447 29029 net.cpp:91] Creating Layer bn3_2_D
I0625 13:37:53.330451 29029 net.cpp:425] bn3_2_D <- conv3_2_D
I0625 13:37:53.330454 29029 net.cpp:386] bn3_2_D -> conv3_2_D (in-place)
I0625 13:37:53.330636 29029 net.cpp:141] Setting up bn3_2_D
I0625 13:37:53.330642 29029 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 13:37:53.330646 29029 net.cpp:156] Memory required for data: 150571008
I0625 13:37:53.330651 29029 layer_factory.hpp:77] Creating layer scale3_2_D
I0625 13:37:53.330657 29029 net.cpp:91] Creating Layer scale3_2_D
I0625 13:37:53.330659 29029 net.cpp:425] scale3_2_D <- conv3_2_D
I0625 13:37:53.330664 29029 net.cpp:386] scale3_2_D -> conv3_2_D (in-place)
I0625 13:37:53.330699 29029 layer_factory.hpp:77] Creating layer scale3_2_D
I0625 13:37:53.330802 29029 net.cpp:141] Setting up scale3_2_D
I0625 13:37:53.330808 29029 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 13:37:53.330811 29029 net.cpp:156] Memory required for data: 152635392
I0625 13:37:53.330816 29029 layer_factory.hpp:77] Creating layer relu3_2_D
I0625 13:37:53.330821 29029 net.cpp:91] Creating Layer relu3_2_D
I0625 13:37:53.330822 29029 net.cpp:425] relu3_2_D <- conv3_2_D
I0625 13:37:53.330826 29029 net.cpp:386] relu3_2_D -> conv3_2_D (in-place)
I0625 13:37:53.331111 29029 net.cpp:141] Setting up relu3_2_D
I0625 13:37:53.331123 29029 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 13:37:53.331125 29029 net.cpp:156] Memory required for data: 154699776
I0625 13:37:53.331128 29029 layer_factory.hpp:77] Creating layer conv3_1_D
I0625 13:37:53.331137 29029 net.cpp:91] Creating Layer conv3_1_D
I0625 13:37:53.331140 29029 net.cpp:425] conv3_1_D <- conv3_2_D
I0625 13:37:53.331146 29029 net.cpp:399] conv3_1_D -> conv3_1_D
I0625 13:37:53.332566 29029 net.cpp:141] Setting up conv3_1_D
I0625 13:37:53.332581 29029 net.cpp:148] Top shape: 1 64 56 72 (258048)
I0625 13:37:53.332583 29029 net.cpp:156] Memory required for data: 155731968
I0625 13:37:53.332598 29029 layer_factory.hpp:77] Creating layer bn3_1_D
I0625 13:37:53.332604 29029 net.cpp:91] Creating Layer bn3_1_D
I0625 13:37:53.332608 29029 net.cpp:425] bn3_1_D <- conv3_1_D
I0625 13:37:53.332613 29029 net.cpp:386] bn3_1_D -> conv3_1_D (in-place)
I0625 13:37:53.332813 29029 net.cpp:141] Setting up bn3_1_D
I0625 13:37:53.332821 29029 net.cpp:148] Top shape: 1 64 56 72 (258048)
I0625 13:37:53.332823 29029 net.cpp:156] Memory required for data: 156764160
I0625 13:37:53.332829 29029 layer_factory.hpp:77] Creating layer scale3_1_D
I0625 13:37:53.332835 29029 net.cpp:91] Creating Layer scale3_1_D
I0625 13:37:53.332839 29029 net.cpp:425] scale3_1_D <- conv3_1_D
I0625 13:37:53.332851 29029 net.cpp:386] scale3_1_D -> conv3_1_D (in-place)
I0625 13:37:53.332911 29029 layer_factory.hpp:77] Creating layer scale3_1_D
I0625 13:37:53.333041 29029 net.cpp:141] Setting up scale3_1_D
I0625 13:37:53.333051 29029 net.cpp:148] Top shape: 1 64 56 72 (258048)
I0625 13:37:53.333055 29029 net.cpp:156] Memory required for data: 157796352
I0625 13:37:53.333063 29029 layer_factory.hpp:77] Creating layer relu3_1_D
I0625 13:37:53.333076 29029 net.cpp:91] Creating Layer relu3_1_D
I0625 13:37:53.333079 29029 net.cpp:425] relu3_1_D <- conv3_1_D
I0625 13:37:53.333083 29029 net.cpp:386] relu3_1_D -> conv3_1_D (in-place)
I0625 13:37:53.333372 29029 net.cpp:141] Setting up relu3_1_D
I0625 13:37:53.333384 29029 net.cpp:148] Top shape: 1 64 56 72 (258048)
I0625 13:37:53.333386 29029 net.cpp:156] Memory required for data: 158828544
I0625 13:37:53.333389 29029 layer_factory.hpp:77] Creating layer upsample2
I0625 13:37:53.333396 29029 net.cpp:91] Creating Layer upsample2
I0625 13:37:53.333400 29029 net.cpp:425] upsample2 <- conv3_1_D
I0625 13:37:53.333403 29029 net.cpp:425] upsample2 <- pool2_mask
I0625 13:37:53.333407 29029 net.cpp:399] upsample2 -> pool2_D
I0625 13:37:53.333413 29029 upsample_layer.cpp:31] Params 'pad_out_{}_' are deprecated. Please declare upsample height and width useing the upsample_h, upsample_w parameters.
I0625 13:37:53.333439 29029 net.cpp:141] Setting up upsample2
I0625 13:37:53.333444 29029 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 13:37:53.333446 29029 net.cpp:156] Memory required for data: 162957312
I0625 13:37:53.333449 29029 layer_factory.hpp:77] Creating layer conv2_2_D
I0625 13:37:53.333457 29029 net.cpp:91] Creating Layer conv2_2_D
I0625 13:37:53.333464 29029 net.cpp:425] conv2_2_D <- pool2_D
I0625 13:37:53.333469 29029 net.cpp:399] conv2_2_D -> conv2_2_D
I0625 13:37:53.334720 29029 net.cpp:141] Setting up conv2_2_D
I0625 13:37:53.334733 29029 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 13:37:53.334735 29029 net.cpp:156] Memory required for data: 167086080
I0625 13:37:53.334740 29029 layer_factory.hpp:77] Creating layer bn2_2_D
I0625 13:37:53.334748 29029 net.cpp:91] Creating Layer bn2_2_D
I0625 13:37:53.334750 29029 net.cpp:425] bn2_2_D <- conv2_2_D
I0625 13:37:53.334755 29029 net.cpp:386] bn2_2_D -> conv2_2_D (in-place)
I0625 13:37:53.334961 29029 net.cpp:141] Setting up bn2_2_D
I0625 13:37:53.334969 29029 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 13:37:53.334971 29029 net.cpp:156] Memory required for data: 171214848
I0625 13:37:53.334977 29029 layer_factory.hpp:77] Creating layer scale2_2_D
I0625 13:37:53.334985 29029 net.cpp:91] Creating Layer scale2_2_D
I0625 13:37:53.334987 29029 net.cpp:425] scale2_2_D <- conv2_2_D
I0625 13:37:53.334991 29029 net.cpp:386] scale2_2_D -> conv2_2_D (in-place)
I0625 13:37:53.335033 29029 layer_factory.hpp:77] Creating layer scale2_2_D
I0625 13:37:53.335324 29029 net.cpp:141] Setting up scale2_2_D
I0625 13:37:53.335353 29029 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 13:37:53.335366 29029 net.cpp:156] Memory required for data: 175343616
I0625 13:37:53.335386 29029 layer_factory.hpp:77] Creating layer relu2_2_D
I0625 13:37:53.335412 29029 net.cpp:91] Creating Layer relu2_2_D
I0625 13:37:53.335427 29029 net.cpp:425] relu2_2_D <- conv2_2_D
I0625 13:37:53.335444 29029 net.cpp:386] relu2_2_D -> conv2_2_D (in-place)
I0625 13:37:53.335871 29029 net.cpp:141] Setting up relu2_2_D
I0625 13:37:53.335896 29029 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 13:37:53.335906 29029 net.cpp:156] Memory required for data: 179472384
I0625 13:37:53.335916 29029 layer_factory.hpp:77] Creating layer conv2_1_D
I0625 13:37:53.335947 29029 net.cpp:91] Creating Layer conv2_1_D
I0625 13:37:53.335958 29029 net.cpp:425] conv2_1_D <- conv2_2_D
I0625 13:37:53.335976 29029 net.cpp:399] conv2_1_D -> conv2_1_D
I0625 13:37:53.338079 29029 net.cpp:141] Setting up conv2_1_D
I0625 13:37:53.338105 29029 net.cpp:148] Top shape: 1 32 112 144 (516096)
I0625 13:37:53.338116 29029 net.cpp:156] Memory required for data: 181536768
I0625 13:37:53.338132 29029 layer_factory.hpp:77] Creating layer bn2_1_D
I0625 13:37:53.338156 29029 net.cpp:91] Creating Layer bn2_1_D
I0625 13:37:53.338171 29029 net.cpp:425] bn2_1_D <- conv2_1_D
I0625 13:37:53.338189 29029 net.cpp:386] bn2_1_D -> conv2_1_D (in-place)
I0625 13:37:53.338642 29029 net.cpp:141] Setting up bn2_1_D
I0625 13:37:53.338661 29029 net.cpp:148] Top shape: 1 32 112 144 (516096)
I0625 13:37:53.338671 29029 net.cpp:156] Memory required for data: 183601152
I0625 13:37:53.338692 29029 layer_factory.hpp:77] Creating layer scale2_1_D
I0625 13:37:53.338714 29029 net.cpp:91] Creating Layer scale2_1_D
I0625 13:37:53.338727 29029 net.cpp:425] scale2_1_D <- conv2_1_D
I0625 13:37:53.338743 29029 net.cpp:386] scale2_1_D -> conv2_1_D (in-place)
I0625 13:37:53.338842 29029 layer_factory.hpp:77] Creating layer scale2_1_D
I0625 13:37:53.339128 29029 net.cpp:141] Setting up scale2_1_D
I0625 13:37:53.339146 29029 net.cpp:148] Top shape: 1 32 112 144 (516096)
I0625 13:37:53.339170 29029 net.cpp:156] Memory required for data: 185665536
I0625 13:37:53.339189 29029 layer_factory.hpp:77] Creating layer relu2_1_D
I0625 13:37:53.339210 29029 net.cpp:91] Creating Layer relu2_1_D
I0625 13:37:53.339224 29029 net.cpp:425] relu2_1_D <- conv2_1_D
I0625 13:37:53.339239 29029 net.cpp:386] relu2_1_D -> conv2_1_D (in-place)
I0625 13:37:53.339787 29029 net.cpp:141] Setting up relu2_1_D
I0625 13:37:53.339812 29029 net.cpp:148] Top shape: 1 32 112 144 (516096)
I0625 13:37:53.339823 29029 net.cpp:156] Memory required for data: 187729920
I0625 13:37:53.339831 29029 layer_factory.hpp:77] Creating layer upsample1
I0625 13:37:53.339851 29029 net.cpp:91] Creating Layer upsample1
I0625 13:37:53.339865 29029 net.cpp:425] upsample1 <- conv2_1_D
I0625 13:37:53.339879 29029 net.cpp:425] upsample1 <- pool1_mask
I0625 13:37:53.339896 29029 net.cpp:399] upsample1 -> pool1_D
I0625 13:37:53.339920 29029 upsample_layer.cpp:31] Params 'pad_out_{}_' are deprecated. Please declare upsample height and width useing the upsample_h, upsample_w parameters.
I0625 13:37:53.339990 29029 net.cpp:141] Setting up upsample1
I0625 13:37:53.340008 29029 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 13:37:53.340016 29029 net.cpp:156] Memory required for data: 195987456
I0625 13:37:53.340026 29029 layer_factory.hpp:77] Creating layer conv1_2_D
I0625 13:37:53.340054 29029 net.cpp:91] Creating Layer conv1_2_D
I0625 13:37:53.340060 29029 net.cpp:425] conv1_2_D <- pool1_D
I0625 13:37:53.340070 29029 net.cpp:399] conv1_2_D -> conv1_2_D
I0625 13:37:53.341995 29029 net.cpp:141] Setting up conv1_2_D
I0625 13:37:53.342017 29029 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 13:37:53.342023 29029 net.cpp:156] Memory required for data: 204244992
I0625 13:37:53.342033 29029 layer_factory.hpp:77] Creating layer bn1_2_D
I0625 13:37:53.342046 29029 net.cpp:91] Creating Layer bn1_2_D
I0625 13:37:53.342051 29029 net.cpp:425] bn1_2_D <- conv1_2_D
I0625 13:37:53.342062 29029 net.cpp:386] bn1_2_D -> conv1_2_D (in-place)
I0625 13:37:53.343559 29029 net.cpp:141] Setting up bn1_2_D
I0625 13:37:53.343581 29029 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 13:37:53.343588 29029 net.cpp:156] Memory required for data: 212502528
I0625 13:37:53.343603 29029 layer_factory.hpp:77] Creating layer scale1_2_D
I0625 13:37:53.343618 29029 net.cpp:91] Creating Layer scale1_2_D
I0625 13:37:53.343642 29029 net.cpp:425] scale1_2_D <- conv1_2_D
I0625 13:37:53.343652 29029 net.cpp:386] scale1_2_D -> conv1_2_D (in-place)
I0625 13:37:53.343749 29029 layer_factory.hpp:77] Creating layer scale1_2_D
I0625 13:37:53.344118 29029 net.cpp:141] Setting up scale1_2_D
I0625 13:37:53.344132 29029 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 13:37:53.344138 29029 net.cpp:156] Memory required for data: 220760064
I0625 13:37:53.344148 29029 layer_factory.hpp:77] Creating layer relu1_2_D
I0625 13:37:53.344157 29029 net.cpp:91] Creating Layer relu1_2_D
I0625 13:37:53.344163 29029 net.cpp:425] relu1_2_D <- conv1_2_D
I0625 13:37:53.344175 29029 net.cpp:386] relu1_2_D -> conv1_2_D (in-place)
I0625 13:37:53.344776 29029 net.cpp:141] Setting up relu1_2_D
I0625 13:37:53.344799 29029 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 13:37:53.344805 29029 net.cpp:156] Memory required for data: 229017600
I0625 13:37:53.344812 29029 layer_factory.hpp:77] Creating layer conv1_1_D
I0625 13:37:53.344831 29029 net.cpp:91] Creating Layer conv1_1_D
I0625 13:37:53.344841 29029 net.cpp:425] conv1_1_D <- conv1_2_D
I0625 13:37:53.344856 29029 net.cpp:399] conv1_1_D -> conv1_1_D
I0625 13:37:53.347271 29029 net.cpp:141] Setting up conv1_1_D
I0625 13:37:53.347304 29029 net.cpp:148] Top shape: 1 2 224 288 (129024)
I0625 13:37:53.347322 29029 net.cpp:156] Memory required for data: 229533696
I0625 13:37:53.347343 29029 layer_factory.hpp:77] Creating layer conv1_1_D_conv1_1_D_0_split
I0625 13:37:53.347359 29029 net.cpp:91] Creating Layer conv1_1_D_conv1_1_D_0_split
I0625 13:37:53.347369 29029 net.cpp:425] conv1_1_D_conv1_1_D_0_split <- conv1_1_D
I0625 13:37:53.347381 29029 net.cpp:399] conv1_1_D_conv1_1_D_0_split -> conv1_1_D_conv1_1_D_0_split_0
I0625 13:37:53.347394 29029 net.cpp:399] conv1_1_D_conv1_1_D_0_split -> conv1_1_D_conv1_1_D_0_split_1
I0625 13:37:53.347487 29029 net.cpp:141] Setting up conv1_1_D_conv1_1_D_0_split
I0625 13:37:53.347501 29029 net.cpp:148] Top shape: 1 2 224 288 (129024)
I0625 13:37:53.347508 29029 net.cpp:148] Top shape: 1 2 224 288 (129024)
I0625 13:37:53.347513 29029 net.cpp:156] Memory required for data: 230565888
I0625 13:37:53.347519 29029 layer_factory.hpp:77] Creating layer loss
I0625 13:37:53.347528 29029 net.cpp:91] Creating Layer loss
I0625 13:37:53.347534 29029 net.cpp:425] loss <- conv1_1_D_conv1_1_D_0_split_0
I0625 13:37:53.347549 29029 net.cpp:425] loss <- label_data_1_split_0
I0625 13:37:53.347559 29029 net.cpp:399] loss -> loss
I0625 13:37:53.347574 29029 layer_factory.hpp:77] Creating layer loss
I0625 13:37:53.349279 29029 net.cpp:141] Setting up loss
I0625 13:37:53.349311 29029 net.cpp:148] Top shape: (1)
I0625 13:37:53.349323 29029 net.cpp:151]     with loss weight 1
I0625 13:37:53.349357 29029 net.cpp:156] Memory required for data: 230565892
I0625 13:37:53.349377 29029 layer_factory.hpp:77] Creating layer accuracy
I0625 13:37:53.349396 29029 net.cpp:91] Creating Layer accuracy
I0625 13:37:53.349403 29029 net.cpp:425] accuracy <- conv1_1_D_conv1_1_D_0_split_1
I0625 13:37:53.349412 29029 net.cpp:425] accuracy <- label_data_1_split_1
I0625 13:37:53.349426 29029 net.cpp:399] accuracy -> accuracy
I0625 13:37:53.349442 29029 net.cpp:141] Setting up accuracy
I0625 13:37:53.349449 29029 net.cpp:148] Top shape: (1)
I0625 13:37:53.349454 29029 net.cpp:156] Memory required for data: 230565896
I0625 13:37:53.349460 29029 net.cpp:219] accuracy does not need backward computation.
I0625 13:37:53.349467 29029 net.cpp:217] loss needs backward computation.
I0625 13:37:53.349473 29029 net.cpp:217] conv1_1_D_conv1_1_D_0_split needs backward computation.
I0625 13:37:53.349479 29029 net.cpp:217] conv1_1_D needs backward computation.
I0625 13:37:53.349485 29029 net.cpp:217] relu1_2_D needs backward computation.
I0625 13:37:53.349490 29029 net.cpp:217] scale1_2_D needs backward computation.
I0625 13:37:53.349495 29029 net.cpp:217] bn1_2_D needs backward computation.
I0625 13:37:53.349499 29029 net.cpp:217] conv1_2_D needs backward computation.
I0625 13:37:53.349505 29029 net.cpp:217] upsample1 needs backward computation.
I0625 13:37:53.349529 29029 net.cpp:217] relu2_1_D needs backward computation.
I0625 13:37:53.349535 29029 net.cpp:217] scale2_1_D needs backward computation.
I0625 13:37:53.349539 29029 net.cpp:217] bn2_1_D needs backward computation.
I0625 13:37:53.349545 29029 net.cpp:217] conv2_1_D needs backward computation.
I0625 13:37:53.349550 29029 net.cpp:217] relu2_2_D needs backward computation.
I0625 13:37:53.349555 29029 net.cpp:217] scale2_2_D needs backward computation.
I0625 13:37:53.349560 29029 net.cpp:217] bn2_2_D needs backward computation.
I0625 13:37:53.349565 29029 net.cpp:217] conv2_2_D needs backward computation.
I0625 13:37:53.349570 29029 net.cpp:217] upsample2 needs backward computation.
I0625 13:37:53.349576 29029 net.cpp:217] relu3_1_D needs backward computation.
I0625 13:37:53.349581 29029 net.cpp:217] scale3_1_D needs backward computation.
I0625 13:37:53.349586 29029 net.cpp:217] bn3_1_D needs backward computation.
I0625 13:37:53.349591 29029 net.cpp:217] conv3_1_D needs backward computation.
I0625 13:37:53.349596 29029 net.cpp:217] relu3_2_D needs backward computation.
I0625 13:37:53.349601 29029 net.cpp:217] scale3_2_D needs backward computation.
I0625 13:37:53.349606 29029 net.cpp:217] bn3_2_D needs backward computation.
I0625 13:37:53.349611 29029 net.cpp:217] conv3_2_D needs backward computation.
I0625 13:37:53.349617 29029 net.cpp:217] upsample3 needs backward computation.
I0625 13:37:53.349623 29029 net.cpp:217] relu4_1_D needs backward computation.
I0625 13:37:53.349628 29029 net.cpp:217] scale4_1_D needs backward computation.
I0625 13:37:53.349633 29029 net.cpp:217] bn4_1_D needs backward computation.
I0625 13:37:53.349638 29029 net.cpp:217] conv4_1_D needs backward computation.
I0625 13:37:53.349644 29029 net.cpp:217] relu4_2_D needs backward computation.
I0625 13:37:53.349649 29029 net.cpp:217] scale4_2_D needs backward computation.
I0625 13:37:53.349654 29029 net.cpp:217] bn4_2_D needs backward computation.
I0625 13:37:53.349658 29029 net.cpp:217] conv4_2_D needs backward computation.
I0625 13:37:53.349664 29029 net.cpp:217] upsample4 needs backward computation.
I0625 13:37:53.349673 29029 net.cpp:217] relu5_1_D needs backward computation.
I0625 13:37:53.349683 29029 net.cpp:217] scale5_1_D needs backward computation.
I0625 13:37:53.349687 29029 net.cpp:217] bn5_1_D needs backward computation.
I0625 13:37:53.349692 29029 net.cpp:217] conv5_1_D needs backward computation.
I0625 13:37:53.349697 29029 net.cpp:217] relu5_2_D needs backward computation.
I0625 13:37:53.349704 29029 net.cpp:217] scale5_2_D needs backward computation.
I0625 13:37:53.349709 29029 net.cpp:217] bn5_2_D needs backward computation.
I0625 13:37:53.349714 29029 net.cpp:217] conv5_2_D needs backward computation.
I0625 13:37:53.349719 29029 net.cpp:217] upsample5 needs backward computation.
I0625 13:37:53.349725 29029 net.cpp:217] pool5 needs backward computation.
I0625 13:37:53.349731 29029 net.cpp:217] relu5_2 needs backward computation.
I0625 13:37:53.349736 29029 net.cpp:217] scale5_2 needs backward computation.
I0625 13:37:53.349742 29029 net.cpp:217] bn5_2 needs backward computation.
I0625 13:37:53.349747 29029 net.cpp:217] conv5_2 needs backward computation.
I0625 13:37:53.349753 29029 net.cpp:217] relu5_1 needs backward computation.
I0625 13:37:53.349762 29029 net.cpp:217] scale5_1 needs backward computation.
I0625 13:37:53.349767 29029 net.cpp:217] bn5_1 needs backward computation.
I0625 13:37:53.349776 29029 net.cpp:217] conv5_1 needs backward computation.
I0625 13:37:53.349781 29029 net.cpp:217] pool4 needs backward computation.
I0625 13:37:53.349789 29029 net.cpp:217] relu4_2 needs backward computation.
I0625 13:37:53.349795 29029 net.cpp:217] scale4_2 needs backward computation.
I0625 13:37:53.349802 29029 net.cpp:217] bn4_2 needs backward computation.
I0625 13:37:53.349807 29029 net.cpp:217] conv4_2 needs backward computation.
I0625 13:37:53.349813 29029 net.cpp:217] relu4_1 needs backward computation.
I0625 13:37:53.349820 29029 net.cpp:217] scale4_1 needs backward computation.
I0625 13:37:53.349836 29029 net.cpp:217] bn4_1 needs backward computation.
I0625 13:37:53.349841 29029 net.cpp:217] conv4_1 needs backward computation.
I0625 13:37:53.349848 29029 net.cpp:217] pool3 needs backward computation.
I0625 13:37:53.349853 29029 net.cpp:217] relu3_2 needs backward computation.
I0625 13:37:53.349860 29029 net.cpp:217] scale3_2 needs backward computation.
I0625 13:37:53.349865 29029 net.cpp:217] bn3_2 needs backward computation.
I0625 13:37:53.349872 29029 net.cpp:217] conv3_2 needs backward computation.
I0625 13:37:53.349877 29029 net.cpp:217] relu3_1 needs backward computation.
I0625 13:37:53.349884 29029 net.cpp:217] scale3_1 needs backward computation.
I0625 13:37:53.349890 29029 net.cpp:217] bn3_1 needs backward computation.
I0625 13:37:53.349897 29029 net.cpp:217] conv3_1 needs backward computation.
I0625 13:37:53.349902 29029 net.cpp:217] pool2 needs backward computation.
I0625 13:37:53.349910 29029 net.cpp:217] relu2_2 needs backward computation.
I0625 13:37:53.349917 29029 net.cpp:217] scale2_2 needs backward computation.
I0625 13:37:53.349922 29029 net.cpp:217] bn2_2 needs backward computation.
I0625 13:37:53.349930 29029 net.cpp:217] conv2_2 needs backward computation.
I0625 13:37:53.349934 29029 net.cpp:217] relu2_1 needs backward computation.
I0625 13:37:53.349941 29029 net.cpp:217] scale2_1 needs backward computation.
I0625 13:37:53.349947 29029 net.cpp:217] bn2_1 needs backward computation.
I0625 13:37:53.349953 29029 net.cpp:217] conv2_1 needs backward computation.
I0625 13:37:53.349959 29029 net.cpp:217] pool1 needs backward computation.
I0625 13:37:53.349967 29029 net.cpp:217] relu1_2 needs backward computation.
I0625 13:37:53.349972 29029 net.cpp:217] scale1_2 needs backward computation.
I0625 13:37:53.349978 29029 net.cpp:217] bn1_2 needs backward computation.
I0625 13:37:53.349984 29029 net.cpp:217] conv1_2 needs backward computation.
I0625 13:37:53.349990 29029 net.cpp:217] relu1_1 needs backward computation.
I0625 13:37:53.349997 29029 net.cpp:217] scale1_1 needs backward computation.
I0625 13:37:53.350003 29029 net.cpp:217] bn1_1 needs backward computation.
I0625 13:37:53.350008 29029 net.cpp:217] conv1_1 needs backward computation.
I0625 13:37:53.350015 29029 net.cpp:219] label_data_1_split does not need backward computation.
I0625 13:37:53.350023 29029 net.cpp:219] data does not need backward computation.
I0625 13:37:53.350030 29029 net.cpp:261] This network produces output accuracy
I0625 13:37:53.350036 29029 net.cpp:261] This network produces output loss
I0625 13:37:53.350111 29029 net.cpp:274] Network initialization done.
I0625 13:37:53.354351 29029 solver.cpp:181] Creating test net (#0) specified by net file: models/segnet/train_val.prototxt
I0625 13:37:53.354564 29029 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0625 13:37:53.355552 29029 net.cpp:49] Initializing net from parameters: 
name: "segnet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "DenseImageData"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mean_value: 100
    crop_h: 224
    crop_w: 288
  }
  dense_image_data_param {
    source: "data/val_seg.txt"
    batch_size: 1
    shuffle: true
    new_height: 224
    new_width: 310
    is_color: false
    root_folder: "data/raw/train/"
  }
}
layer {
  name: "conv1_1"
  type: "Convolution"
  bottom: "data"
  top: "conv1_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_1"
  type: "BatchNorm"
  bottom: "conv1_1"
  top: "conv1_1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale1_1"
  type: "Scale"
  bottom: "conv1_1"
  top: "conv1_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1_1"
  type: "ReLU"
  bottom: "conv1_1"
  top: "conv1_1"
}
layer {
  name: "conv1_2"
  type: "Convolution"
  bottom: "conv1_1"
  top: "conv1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_2"
  type: "BatchNorm"
  bottom: "conv1_2"
  top: "conv1_2"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale1_2"
  type: "Scale"
  bottom: "conv1_2"
  top: "conv1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1_2"
  type: "ReLU"
  bottom: "conv1_2"
  top: "conv1_2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1_2"
  top: "pool1"
  top: "pool1_mask"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2"
  top: "pool2_mask"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_1"
  type: "BatchNorm"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale3_1"
  type: "Scale"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv3_2"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv3_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_2"
  type: "BatchNorm"
  bottom: "conv3_2"
  top: "conv3_2"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale3_2"
  type: "Scale"
  bottom: "conv3_2"
  top: "conv3_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_2"
  type: "ReLU"
  bottom: "conv3_2"
  top: "conv3_2"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3_2"
  top: "pool3"
  top: "pool3_mask"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4"
  top: "pool4_mask"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv5_1"
  type: "Convolution"
  bottom: "pool4"
  top: "conv5_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn5_1"
  type: "BatchNorm"
  bottom: "conv5_1"
  top: "conv5_1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale5_1"
  type: "Scale"
  bottom: "conv5_1"
  top: "conv5_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5_1"
  type: "ReLU"
  bottom: "conv5_1"
  top: "conv5_1"
}
layer {
  name: "conv5_2"
  type: "Convolution"
  bottom: "conv5_1"
  top: "conv5_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn5_2"
  type: "BatchNorm"
  bottom: "conv5_2"
  top: "conv5_2"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale5_2"
  type: "Scale"
  bottom: "conv5_2"
  top: "conv5_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5_2"
  type: "ReLU"
  bottom: "conv5_2"
  top: "conv5_2"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5_2"
  top: "pool5"
  top: "pool5_mask"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "upsample5"
  type: "Upsample"
  bottom: "pool5"
  bottom: "pool5_mask"
  top: "pool5_D"
  upsample_param {
    scale: 2
  }
}
layer {
  name: "conv5_2_D"
  type: "Convolution"
  bottom: "pool5_D"
  top: "conv5_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn5_2_D"
  type: "BatchNorm"
  bottom: "conv5_2_D"
  top: "conv5_2_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale5_2_D"
  type: "Scale"
  bottom: "conv5_2_D"
  top: "conv5_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5_2_D"
  type: "ReLU"
  bottom: "conv5_2_D"
  top: "conv5_2_D"
}
layer {
  name: "conv5_1_D"
  type: "Convolution"
  bottom: "conv5_2_D"
  top: "conv5_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn5_1_D"
  type: "BatchNorm"
  bottom: "conv5_1_D"
  top: "conv5_1_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale5_1_D"
  type: "Scale"
  bottom: "conv5_1_D"
  top: "conv5_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5_1_D"
  type: "ReLU"
  bottom: "conv5_1_D"
  top: "conv5_1_D"
}
layer {
  name: "upsample4"
  type: "Upsample"
  bottom: "conv5_1_D"
  bottom: "pool4_mask"
  top: "pool4_D"
  upsample_param {
    scale: 2
  }
}
layer {
  name: "conv4_2_D"
  type: "Convolution"
  bottom: "pool4_D"
  top: "conv4_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2_D"
  type: "BatchNorm"
  bottom: "conv4_2_D"
  top: "conv4_2_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale4_2_D"
  type: "Scale"
  bottom: "conv4_2_D"
  top: "conv4_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4_2_D"
  type: "ReLU"
  bottom: "conv4_2_D"
  top: "conv4_2_D"
}
layer {
  name: "conv4_1_D"
  type: "Convolution"
  bottom: "conv4_2_D"
  top: "conv4_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1_D"
  type: "BatchNorm"
  bottom: "conv4_1_D"
  top: "conv4_1_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale4_1_D"
  type: "Scale"
  bottom: "conv4_1_D"
  top: "conv4_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4_1_D"
  type: "ReLU"
  bottom: "conv4_1_D"
  top: "conv4_1_D"
}
layer {
  name: "upsample3"
  type: "Upsample"
  bottom: "conv4_1_D"
  bottom: "pool3_mask"
  top: "pool3_D"
  upsample_param {
    scale: 2
  }
}
layer {
  name: "conv3_2_D"
  type: "Convolution"
  bottom: "pool3_D"
  top: "conv3_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3_2_D"
  type: "BatchNorm"
  bottom: "conv3_2_D"
  top: "conv3_2_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale3_2_D"
  type: "Scale"
  bottom: "conv3_2_D"
  top: "conv3_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_2_D"
  type: "ReLU"
  bottom: "conv3_2_D"
  top: "conv3_2_D"
}
layer {
  name: "conv3_1_D"
  type: "Convolution"
  bottom: "conv3_2_D"
  top: "conv3_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_1_D"
  type: "BatchNorm"
  bottom: "conv3_1_D"
  top: "conv3_1_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale3_1_D"
  type: "Scale"
  bottom: "conv3_1_D"
  top: "conv3_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_1_D"
  type: "ReLU"
  bottom: "conv3_1_D"
  top: "conv3_1_D"
}
layer {
  name: "upsample2"
  type: "Upsample"
  bottom: "conv3_1_D"
  bottom: "pool2_mask"
  top: "pool2_D"
  upsample_param {
    scale: 2
  }
}
layer {
  name: "conv2_2_D"
  type: "Convolution"
  bottom: "pool2_D"
  top: "conv2_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2_D"
  type: "BatchNorm"
  bottom: "conv2_2_D"
  top: "conv2_2_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale2_2_D"
  type: "Scale"
  bottom: "conv2_2_D"
  top: "conv2_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2_2_D"
  type: "ReLU"
  bottom: "conv2_2_D"
  top: "conv2_2_D"
}
layer {
  name: "conv2_1_D"
  type: "Convolution"
  bottom: "conv2_2_D"
  top: "conv2_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_1_D"
  type: "BatchNorm"
  bottom: "conv2_1_D"
  top: "conv2_1_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale2_1_D"
  type: "Scale"
  bottom: "conv2_1_D"
  top: "conv2_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2_1_D"
  type: "ReLU"
  bottom: "conv2_1_D"
  top: "conv2_1_D"
}
layer {
  name: "upsample1"
  type: "Upsample"
  bottom: "conv2_1_D"
  bottom: "pool1_mask"
  top: "pool1_D"
  upsample_param {
    scale: 2
  }
}
layer {
  name: "conv1_2_D"
  type: "Convolution"
  bottom: "pool1_D"
  top: "conv1_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_2_D"
  type: "BatchNorm"
  bottom: "conv1_2_D"
  top: "conv1_2_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale1_2_D"
  type: "Scale"
  bottom: "conv1_2_D"
  top: "conv1_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1_2_D"
  type: "ReLU"
  bottom: "conv1_2_D"
  top: "conv1_2_D"
}
layer {
  name: "conv1_1_D"
  type: "Convolution"
  bottom: "conv1_2_D"
  top: "conv1_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 2
    bias_term: true
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "conv1_1_D"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "conv1_1_D"
  bottom: "label"
  top: "accuracy"
}
I0625 13:37:53.356057 29029 layer_factory.hpp:77] Creating layer data
I0625 13:37:53.356081 29029 net.cpp:91] Creating Layer data
I0625 13:37:53.356088 29029 net.cpp:399] data -> data
I0625 13:37:53.356102 29029 net.cpp:399] data -> label
I0625 13:37:53.356123 29029 dense_image_data_layer.cpp:38] Opening file data/val_seg.txt
I0625 13:37:53.356549 29029 dense_image_data_layer.cpp:48] Shuffling data
I0625 13:37:53.356642 29029 dense_image_data_layer.cpp:53] A total of 299 examples.
I0625 13:37:53.370586 29029 dense_image_data_layer.cpp:92] output data size: 1,1,224,288
I0625 13:37:53.372486 29029 net.cpp:141] Setting up data
I0625 13:37:53.372512 29029 net.cpp:148] Top shape: 1 1 224 288 (64512)
I0625 13:37:53.372520 29029 net.cpp:148] Top shape: 1 1 224 288 (64512)
I0625 13:37:53.372525 29029 net.cpp:156] Memory required for data: 516096
I0625 13:37:53.372534 29029 layer_factory.hpp:77] Creating layer label_data_1_split
I0625 13:37:53.372550 29029 net.cpp:91] Creating Layer label_data_1_split
I0625 13:37:53.372556 29029 net.cpp:425] label_data_1_split <- label
I0625 13:37:53.372566 29029 net.cpp:399] label_data_1_split -> label_data_1_split_0
I0625 13:37:53.372580 29029 net.cpp:399] label_data_1_split -> label_data_1_split_1
I0625 13:37:53.372668 29029 net.cpp:141] Setting up label_data_1_split
I0625 13:37:53.372681 29029 net.cpp:148] Top shape: 1 1 224 288 (64512)
I0625 13:37:53.372687 29029 net.cpp:148] Top shape: 1 1 224 288 (64512)
I0625 13:37:53.372691 29029 net.cpp:156] Memory required for data: 1032192
I0625 13:37:53.372696 29029 layer_factory.hpp:77] Creating layer conv1_1
I0625 13:37:53.372714 29029 net.cpp:91] Creating Layer conv1_1
I0625 13:37:53.372720 29029 net.cpp:425] conv1_1 <- data
I0625 13:37:53.372727 29029 net.cpp:399] conv1_1 -> conv1_1
I0625 13:37:53.374928 29029 net.cpp:141] Setting up conv1_1
I0625 13:37:53.374951 29029 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 13:37:53.374958 29029 net.cpp:156] Memory required for data: 9289728
I0625 13:37:53.374969 29029 layer_factory.hpp:77] Creating layer bn1_1
I0625 13:37:53.374980 29029 net.cpp:91] Creating Layer bn1_1
I0625 13:37:53.374986 29029 net.cpp:425] bn1_1 <- conv1_1
I0625 13:37:53.374994 29029 net.cpp:386] bn1_1 -> conv1_1 (in-place)
I0625 13:37:53.376318 29029 net.cpp:141] Setting up bn1_1
I0625 13:37:53.376339 29029 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 13:37:53.376344 29029 net.cpp:156] Memory required for data: 17547264
I0625 13:37:53.376384 29029 layer_factory.hpp:77] Creating layer scale1_1
I0625 13:37:53.376399 29029 net.cpp:91] Creating Layer scale1_1
I0625 13:37:53.376404 29029 net.cpp:425] scale1_1 <- conv1_1
I0625 13:37:53.376411 29029 net.cpp:386] scale1_1 -> conv1_1 (in-place)
I0625 13:37:53.376490 29029 layer_factory.hpp:77] Creating layer scale1_1
I0625 13:37:53.376814 29029 net.cpp:141] Setting up scale1_1
I0625 13:37:53.376829 29029 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 13:37:53.376834 29029 net.cpp:156] Memory required for data: 25804800
I0625 13:37:53.376845 29029 layer_factory.hpp:77] Creating layer relu1_1
I0625 13:37:53.376855 29029 net.cpp:91] Creating Layer relu1_1
I0625 13:37:53.376860 29029 net.cpp:425] relu1_1 <- conv1_1
I0625 13:37:53.376868 29029 net.cpp:386] relu1_1 -> conv1_1 (in-place)
I0625 13:37:53.377359 29029 net.cpp:141] Setting up relu1_1
I0625 13:37:53.377379 29029 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 13:37:53.377387 29029 net.cpp:156] Memory required for data: 34062336
I0625 13:37:53.377391 29029 layer_factory.hpp:77] Creating layer conv1_2
I0625 13:37:53.377409 29029 net.cpp:91] Creating Layer conv1_2
I0625 13:37:53.377416 29029 net.cpp:425] conv1_2 <- conv1_1
I0625 13:37:53.377425 29029 net.cpp:399] conv1_2 -> conv1_2
I0625 13:37:53.379099 29029 net.cpp:141] Setting up conv1_2
I0625 13:37:53.379120 29029 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 13:37:53.379127 29029 net.cpp:156] Memory required for data: 42319872
I0625 13:37:53.379137 29029 layer_factory.hpp:77] Creating layer bn1_2
I0625 13:37:53.379158 29029 net.cpp:91] Creating Layer bn1_2
I0625 13:37:53.379166 29029 net.cpp:425] bn1_2 <- conv1_2
I0625 13:37:53.379175 29029 net.cpp:386] bn1_2 -> conv1_2 (in-place)
I0625 13:37:53.379601 29029 net.cpp:141] Setting up bn1_2
I0625 13:37:53.379613 29029 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 13:37:53.379617 29029 net.cpp:156] Memory required for data: 50577408
I0625 13:37:53.379632 29029 layer_factory.hpp:77] Creating layer scale1_2
I0625 13:37:53.379645 29029 net.cpp:91] Creating Layer scale1_2
I0625 13:37:53.379652 29029 net.cpp:425] scale1_2 <- conv1_2
I0625 13:37:53.379659 29029 net.cpp:386] scale1_2 -> conv1_2 (in-place)
I0625 13:37:53.379727 29029 layer_factory.hpp:77] Creating layer scale1_2
I0625 13:37:53.380864 29029 net.cpp:141] Setting up scale1_2
I0625 13:37:53.380883 29029 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 13:37:53.380890 29029 net.cpp:156] Memory required for data: 58834944
I0625 13:37:53.380899 29029 layer_factory.hpp:77] Creating layer relu1_2
I0625 13:37:53.380910 29029 net.cpp:91] Creating Layer relu1_2
I0625 13:37:53.380916 29029 net.cpp:425] relu1_2 <- conv1_2
I0625 13:37:53.380923 29029 net.cpp:386] relu1_2 -> conv1_2 (in-place)
I0625 13:37:53.381391 29029 net.cpp:141] Setting up relu1_2
I0625 13:37:53.381408 29029 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 13:37:53.381415 29029 net.cpp:156] Memory required for data: 67092480
I0625 13:37:53.381420 29029 layer_factory.hpp:77] Creating layer pool1
I0625 13:37:53.381427 29029 layer_factory.cpp:90] cuDNN does not support multiple tops. Using Caffe's own pooling layer.
I0625 13:37:53.381436 29029 net.cpp:91] Creating Layer pool1
I0625 13:37:53.381443 29029 net.cpp:425] pool1 <- conv1_2
I0625 13:37:53.381450 29029 net.cpp:399] pool1 -> pool1
I0625 13:37:53.381461 29029 net.cpp:399] pool1 -> pool1_mask
I0625 13:37:53.381541 29029 net.cpp:141] Setting up pool1
I0625 13:37:53.381551 29029 net.cpp:148] Top shape: 1 32 112 144 (516096)
I0625 13:37:53.381558 29029 net.cpp:148] Top shape: 1 32 112 144 (516096)
I0625 13:37:53.381564 29029 net.cpp:156] Memory required for data: 71221248
I0625 13:37:53.381568 29029 layer_factory.hpp:77] Creating layer conv2_1
I0625 13:37:53.381580 29029 net.cpp:91] Creating Layer conv2_1
I0625 13:37:53.381587 29029 net.cpp:425] conv2_1 <- pool1
I0625 13:37:53.381595 29029 net.cpp:399] conv2_1 -> conv2_1
I0625 13:37:53.383450 29029 net.cpp:141] Setting up conv2_1
I0625 13:37:53.383469 29029 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 13:37:53.383492 29029 net.cpp:156] Memory required for data: 75350016
I0625 13:37:53.383502 29029 layer_factory.hpp:77] Creating layer bn2_1
I0625 13:37:53.383512 29029 net.cpp:91] Creating Layer bn2_1
I0625 13:37:53.383519 29029 net.cpp:425] bn2_1 <- conv2_1
I0625 13:37:53.383527 29029 net.cpp:386] bn2_1 -> conv2_1 (in-place)
I0625 13:37:53.383906 29029 net.cpp:141] Setting up bn2_1
I0625 13:37:53.383918 29029 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 13:37:53.383924 29029 net.cpp:156] Memory required for data: 79478784
I0625 13:37:53.383934 29029 layer_factory.hpp:77] Creating layer scale2_1
I0625 13:37:53.383945 29029 net.cpp:91] Creating Layer scale2_1
I0625 13:37:53.383952 29029 net.cpp:425] scale2_1 <- conv2_1
I0625 13:37:53.383960 29029 net.cpp:386] scale2_1 -> conv2_1 (in-place)
I0625 13:37:53.384029 29029 layer_factory.hpp:77] Creating layer scale2_1
I0625 13:37:53.385076 29029 net.cpp:141] Setting up scale2_1
I0625 13:37:53.385094 29029 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 13:37:53.385100 29029 net.cpp:156] Memory required for data: 83607552
I0625 13:37:53.385115 29029 layer_factory.hpp:77] Creating layer relu2_1
I0625 13:37:53.385126 29029 net.cpp:91] Creating Layer relu2_1
I0625 13:37:53.385133 29029 net.cpp:425] relu2_1 <- conv2_1
I0625 13:37:53.385140 29029 net.cpp:386] relu2_1 -> conv2_1 (in-place)
I0625 13:37:53.385427 29029 net.cpp:141] Setting up relu2_1
I0625 13:37:53.385442 29029 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 13:37:53.385447 29029 net.cpp:156] Memory required for data: 87736320
I0625 13:37:53.385452 29029 layer_factory.hpp:77] Creating layer conv2_2
I0625 13:37:53.385465 29029 net.cpp:91] Creating Layer conv2_2
I0625 13:37:53.385471 29029 net.cpp:425] conv2_2 <- conv2_1
I0625 13:37:53.385480 29029 net.cpp:399] conv2_2 -> conv2_2
I0625 13:37:53.387734 29029 net.cpp:141] Setting up conv2_2
I0625 13:37:53.387754 29029 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 13:37:53.387761 29029 net.cpp:156] Memory required for data: 91865088
I0625 13:37:53.387770 29029 layer_factory.hpp:77] Creating layer bn2_2
I0625 13:37:53.387784 29029 net.cpp:91] Creating Layer bn2_2
I0625 13:37:53.387791 29029 net.cpp:425] bn2_2 <- conv2_2
I0625 13:37:53.387799 29029 net.cpp:386] bn2_2 -> conv2_2 (in-place)
I0625 13:37:53.388183 29029 net.cpp:141] Setting up bn2_2
I0625 13:37:53.388195 29029 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 13:37:53.388201 29029 net.cpp:156] Memory required for data: 95993856
I0625 13:37:53.388212 29029 layer_factory.hpp:77] Creating layer scale2_2
I0625 13:37:53.388223 29029 net.cpp:91] Creating Layer scale2_2
I0625 13:37:53.388229 29029 net.cpp:425] scale2_2 <- conv2_2
I0625 13:37:53.388236 29029 net.cpp:386] scale2_2 -> conv2_2 (in-place)
I0625 13:37:53.388308 29029 layer_factory.hpp:77] Creating layer scale2_2
I0625 13:37:53.388521 29029 net.cpp:141] Setting up scale2_2
I0625 13:37:53.388532 29029 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 13:37:53.388538 29029 net.cpp:156] Memory required for data: 100122624
I0625 13:37:53.388547 29029 layer_factory.hpp:77] Creating layer relu2_2
I0625 13:37:53.388556 29029 net.cpp:91] Creating Layer relu2_2
I0625 13:37:53.388562 29029 net.cpp:425] relu2_2 <- conv2_2
I0625 13:37:53.388569 29029 net.cpp:386] relu2_2 -> conv2_2 (in-place)
I0625 13:37:53.389040 29029 net.cpp:141] Setting up relu2_2
I0625 13:37:53.389058 29029 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 13:37:53.389065 29029 net.cpp:156] Memory required for data: 104251392
I0625 13:37:53.389070 29029 layer_factory.hpp:77] Creating layer pool2
I0625 13:37:53.389078 29029 layer_factory.cpp:90] cuDNN does not support multiple tops. Using Caffe's own pooling layer.
I0625 13:37:53.389088 29029 net.cpp:91] Creating Layer pool2
I0625 13:37:53.389096 29029 net.cpp:425] pool2 <- conv2_2
I0625 13:37:53.389102 29029 net.cpp:399] pool2 -> pool2
I0625 13:37:53.389113 29029 net.cpp:399] pool2 -> pool2_mask
I0625 13:37:53.389192 29029 net.cpp:141] Setting up pool2
I0625 13:37:53.389202 29029 net.cpp:148] Top shape: 1 64 56 72 (258048)
I0625 13:37:53.389225 29029 net.cpp:148] Top shape: 1 64 56 72 (258048)
I0625 13:37:53.389231 29029 net.cpp:156] Memory required for data: 106315776
I0625 13:37:53.389236 29029 layer_factory.hpp:77] Creating layer conv3_1
I0625 13:37:53.389248 29029 net.cpp:91] Creating Layer conv3_1
I0625 13:37:53.389255 29029 net.cpp:425] conv3_1 <- pool2
I0625 13:37:53.389262 29029 net.cpp:399] conv3_1 -> conv3_1
I0625 13:37:53.391602 29029 net.cpp:141] Setting up conv3_1
I0625 13:37:53.391621 29029 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 13:37:53.391628 29029 net.cpp:156] Memory required for data: 108380160
I0625 13:37:53.391636 29029 layer_factory.hpp:77] Creating layer bn3_1
I0625 13:37:53.391647 29029 net.cpp:91] Creating Layer bn3_1
I0625 13:37:53.391654 29029 net.cpp:425] bn3_1 <- conv3_1
I0625 13:37:53.391661 29029 net.cpp:386] bn3_1 -> conv3_1 (in-place)
I0625 13:37:53.392011 29029 net.cpp:141] Setting up bn3_1
I0625 13:37:53.392024 29029 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 13:37:53.392029 29029 net.cpp:156] Memory required for data: 110444544
I0625 13:37:53.392040 29029 layer_factory.hpp:77] Creating layer scale3_1
I0625 13:37:53.392050 29029 net.cpp:91] Creating Layer scale3_1
I0625 13:37:53.392056 29029 net.cpp:425] scale3_1 <- conv3_1
I0625 13:37:53.392063 29029 net.cpp:386] scale3_1 -> conv3_1 (in-place)
I0625 13:37:53.392127 29029 layer_factory.hpp:77] Creating layer scale3_1
I0625 13:37:53.392315 29029 net.cpp:141] Setting up scale3_1
I0625 13:37:53.392325 29029 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 13:37:53.392329 29029 net.cpp:156] Memory required for data: 112508928
I0625 13:37:53.392338 29029 layer_factory.hpp:77] Creating layer relu3_1
I0625 13:37:53.392346 29029 net.cpp:91] Creating Layer relu3_1
I0625 13:37:53.392352 29029 net.cpp:425] relu3_1 <- conv3_1
I0625 13:37:53.392359 29029 net.cpp:386] relu3_1 -> conv3_1 (in-place)
I0625 13:37:53.392797 29029 net.cpp:141] Setting up relu3_1
I0625 13:37:53.392815 29029 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 13:37:53.392822 29029 net.cpp:156] Memory required for data: 114573312
I0625 13:37:53.392827 29029 layer_factory.hpp:77] Creating layer conv3_2
I0625 13:37:53.392838 29029 net.cpp:91] Creating Layer conv3_2
I0625 13:37:53.392845 29029 net.cpp:425] conv3_2 <- conv3_1
I0625 13:37:53.392853 29029 net.cpp:399] conv3_2 -> conv3_2
I0625 13:37:53.396934 29029 net.cpp:141] Setting up conv3_2
I0625 13:37:53.396953 29029 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 13:37:53.396958 29029 net.cpp:156] Memory required for data: 116637696
I0625 13:37:53.396967 29029 layer_factory.hpp:77] Creating layer bn3_2
I0625 13:37:53.396977 29029 net.cpp:91] Creating Layer bn3_2
I0625 13:37:53.396983 29029 net.cpp:425] bn3_2 <- conv3_2
I0625 13:37:53.396991 29029 net.cpp:386] bn3_2 -> conv3_2 (in-place)
I0625 13:37:53.397338 29029 net.cpp:141] Setting up bn3_2
I0625 13:37:53.397351 29029 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 13:37:53.397354 29029 net.cpp:156] Memory required for data: 118702080
I0625 13:37:53.397372 29029 layer_factory.hpp:77] Creating layer scale3_2
I0625 13:37:53.397382 29029 net.cpp:91] Creating Layer scale3_2
I0625 13:37:53.397387 29029 net.cpp:425] scale3_2 <- conv3_2
I0625 13:37:53.397392 29029 net.cpp:386] scale3_2 -> conv3_2 (in-place)
I0625 13:37:53.397464 29029 layer_factory.hpp:77] Creating layer scale3_2
I0625 13:37:53.397658 29029 net.cpp:141] Setting up scale3_2
I0625 13:37:53.397670 29029 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 13:37:53.397673 29029 net.cpp:156] Memory required for data: 120766464
I0625 13:37:53.397681 29029 layer_factory.hpp:77] Creating layer relu3_2
I0625 13:37:53.397688 29029 net.cpp:91] Creating Layer relu3_2
I0625 13:37:53.397697 29029 net.cpp:425] relu3_2 <- conv3_2
I0625 13:37:53.397708 29029 net.cpp:386] relu3_2 -> conv3_2 (in-place)
I0625 13:37:53.398002 29029 net.cpp:141] Setting up relu3_2
I0625 13:37:53.398020 29029 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 13:37:53.398028 29029 net.cpp:156] Memory required for data: 122830848
I0625 13:37:53.398058 29029 layer_factory.hpp:77] Creating layer pool3
I0625 13:37:53.398069 29029 layer_factory.cpp:90] cuDNN does not support multiple tops. Using Caffe's own pooling layer.
I0625 13:37:53.398080 29029 net.cpp:91] Creating Layer pool3
I0625 13:37:53.398092 29029 net.cpp:425] pool3 <- conv3_2
I0625 13:37:53.398103 29029 net.cpp:399] pool3 -> pool3
I0625 13:37:53.398113 29029 net.cpp:399] pool3 -> pool3_mask
I0625 13:37:53.398192 29029 net.cpp:141] Setting up pool3
I0625 13:37:53.398203 29029 net.cpp:148] Top shape: 1 128 28 36 (129024)
I0625 13:37:53.398210 29029 net.cpp:148] Top shape: 1 128 28 36 (129024)
I0625 13:37:53.398214 29029 net.cpp:156] Memory required for data: 123863040
I0625 13:37:53.398219 29029 layer_factory.hpp:77] Creating layer conv4_1
I0625 13:37:53.398231 29029 net.cpp:91] Creating Layer conv4_1
I0625 13:37:53.398236 29029 net.cpp:425] conv4_1 <- pool3
I0625 13:37:53.398244 29029 net.cpp:399] conv4_1 -> conv4_1
I0625 13:37:53.403774 29029 net.cpp:141] Setting up conv4_1
I0625 13:37:53.403795 29029 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 13:37:53.403800 29029 net.cpp:156] Memory required for data: 124895232
I0625 13:37:53.403807 29029 layer_factory.hpp:77] Creating layer bn4_1
I0625 13:37:53.403822 29029 net.cpp:91] Creating Layer bn4_1
I0625 13:37:53.403830 29029 net.cpp:425] bn4_1 <- conv4_1
I0625 13:37:53.403838 29029 net.cpp:386] bn4_1 -> conv4_1 (in-place)
I0625 13:37:53.404181 29029 net.cpp:141] Setting up bn4_1
I0625 13:37:53.404191 29029 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 13:37:53.404196 29029 net.cpp:156] Memory required for data: 125927424
I0625 13:37:53.404206 29029 layer_factory.hpp:77] Creating layer scale4_1
I0625 13:37:53.404217 29029 net.cpp:91] Creating Layer scale4_1
I0625 13:37:53.404222 29029 net.cpp:425] scale4_1 <- conv4_1
I0625 13:37:53.404228 29029 net.cpp:386] scale4_1 -> conv4_1 (in-place)
I0625 13:37:53.404294 29029 layer_factory.hpp:77] Creating layer scale4_1
I0625 13:37:53.404489 29029 net.cpp:141] Setting up scale4_1
I0625 13:37:53.404498 29029 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 13:37:53.404502 29029 net.cpp:156] Memory required for data: 126959616
I0625 13:37:53.404510 29029 layer_factory.hpp:77] Creating layer relu4_1
I0625 13:37:53.404523 29029 net.cpp:91] Creating Layer relu4_1
I0625 13:37:53.404530 29029 net.cpp:425] relu4_1 <- conv4_1
I0625 13:37:53.404536 29029 net.cpp:386] relu4_1 -> conv4_1 (in-place)
I0625 13:37:53.404986 29029 net.cpp:141] Setting up relu4_1
I0625 13:37:53.405002 29029 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 13:37:53.405009 29029 net.cpp:156] Memory required for data: 127991808
I0625 13:37:53.405014 29029 layer_factory.hpp:77] Creating layer conv4_2
I0625 13:37:53.405026 29029 net.cpp:91] Creating Layer conv4_2
I0625 13:37:53.405033 29029 net.cpp:425] conv4_2 <- conv4_1
I0625 13:37:53.405042 29029 net.cpp:399] conv4_2 -> conv4_2
I0625 13:37:53.413647 29029 net.cpp:141] Setting up conv4_2
I0625 13:37:53.413668 29029 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 13:37:53.413673 29029 net.cpp:156] Memory required for data: 129024000
I0625 13:37:53.413681 29029 layer_factory.hpp:77] Creating layer bn4_2
I0625 13:37:53.413696 29029 net.cpp:91] Creating Layer bn4_2
I0625 13:37:53.413702 29029 net.cpp:425] bn4_2 <- conv4_2
I0625 13:37:53.413709 29029 net.cpp:386] bn4_2 -> conv4_2 (in-place)
I0625 13:37:53.414047 29029 net.cpp:141] Setting up bn4_2
I0625 13:37:53.414057 29029 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 13:37:53.414063 29029 net.cpp:156] Memory required for data: 130056192
I0625 13:37:53.414073 29029 layer_factory.hpp:77] Creating layer scale4_2
I0625 13:37:53.414085 29029 net.cpp:91] Creating Layer scale4_2
I0625 13:37:53.414091 29029 net.cpp:425] scale4_2 <- conv4_2
I0625 13:37:53.414098 29029 net.cpp:386] scale4_2 -> conv4_2 (in-place)
I0625 13:37:53.414165 29029 layer_factory.hpp:77] Creating layer scale4_2
I0625 13:37:53.414361 29029 net.cpp:141] Setting up scale4_2
I0625 13:37:53.414371 29029 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 13:37:53.414391 29029 net.cpp:156] Memory required for data: 131088384
I0625 13:37:53.414399 29029 layer_factory.hpp:77] Creating layer relu4_2
I0625 13:37:53.414409 29029 net.cpp:91] Creating Layer relu4_2
I0625 13:37:53.414415 29029 net.cpp:425] relu4_2 <- conv4_2
I0625 13:37:53.414422 29029 net.cpp:386] relu4_2 -> conv4_2 (in-place)
I0625 13:37:53.414860 29029 net.cpp:141] Setting up relu4_2
I0625 13:37:53.414876 29029 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 13:37:53.414882 29029 net.cpp:156] Memory required for data: 132120576
I0625 13:37:53.414887 29029 layer_factory.hpp:77] Creating layer pool4
I0625 13:37:53.414893 29029 layer_factory.cpp:90] cuDNN does not support multiple tops. Using Caffe's own pooling layer.
I0625 13:37:53.414903 29029 net.cpp:91] Creating Layer pool4
I0625 13:37:53.414911 29029 net.cpp:425] pool4 <- conv4_2
I0625 13:37:53.414917 29029 net.cpp:399] pool4 -> pool4
I0625 13:37:53.414929 29029 net.cpp:399] pool4 -> pool4_mask
I0625 13:37:53.415004 29029 net.cpp:141] Setting up pool4
I0625 13:37:53.415014 29029 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 13:37:53.415019 29029 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 13:37:53.415024 29029 net.cpp:156] Memory required for data: 132636672
I0625 13:37:53.415029 29029 layer_factory.hpp:77] Creating layer conv5_1
I0625 13:37:53.415042 29029 net.cpp:91] Creating Layer conv5_1
I0625 13:37:53.415048 29029 net.cpp:425] conv5_1 <- pool4
I0625 13:37:53.415056 29029 net.cpp:399] conv5_1 -> conv5_1
I0625 13:37:53.423553 29029 net.cpp:141] Setting up conv5_1
I0625 13:37:53.423583 29029 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 13:37:53.423591 29029 net.cpp:156] Memory required for data: 132894720
I0625 13:37:53.423602 29029 layer_factory.hpp:77] Creating layer bn5_1
I0625 13:37:53.423616 29029 net.cpp:91] Creating Layer bn5_1
I0625 13:37:53.423629 29029 net.cpp:425] bn5_1 <- conv5_1
I0625 13:37:53.423645 29029 net.cpp:386] bn5_1 -> conv5_1 (in-place)
I0625 13:37:53.424131 29029 net.cpp:141] Setting up bn5_1
I0625 13:37:53.424154 29029 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 13:37:53.424165 29029 net.cpp:156] Memory required for data: 133152768
I0625 13:37:53.424180 29029 layer_factory.hpp:77] Creating layer scale5_1
I0625 13:37:53.424196 29029 net.cpp:91] Creating Layer scale5_1
I0625 13:37:53.424206 29029 net.cpp:425] scale5_1 <- conv5_1
I0625 13:37:53.424219 29029 net.cpp:386] scale5_1 -> conv5_1 (in-place)
I0625 13:37:53.424338 29029 layer_factory.hpp:77] Creating layer scale5_1
I0625 13:37:53.424556 29029 net.cpp:141] Setting up scale5_1
I0625 13:37:53.424571 29029 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 13:37:53.424574 29029 net.cpp:156] Memory required for data: 133410816
I0625 13:37:53.424582 29029 layer_factory.hpp:77] Creating layer relu5_1
I0625 13:37:53.424592 29029 net.cpp:91] Creating Layer relu5_1
I0625 13:37:53.424598 29029 net.cpp:425] relu5_1 <- conv5_1
I0625 13:37:53.424604 29029 net.cpp:386] relu5_1 -> conv5_1 (in-place)
I0625 13:37:53.424872 29029 net.cpp:141] Setting up relu5_1
I0625 13:37:53.424885 29029 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 13:37:53.424890 29029 net.cpp:156] Memory required for data: 133668864
I0625 13:37:53.424896 29029 layer_factory.hpp:77] Creating layer conv5_2
I0625 13:37:53.424909 29029 net.cpp:91] Creating Layer conv5_2
I0625 13:37:53.424916 29029 net.cpp:425] conv5_2 <- conv5_1
I0625 13:37:53.424923 29029 net.cpp:399] conv5_2 -> conv5_2
I0625 13:37:53.433281 29029 net.cpp:141] Setting up conv5_2
I0625 13:37:53.433305 29029 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 13:37:53.433310 29029 net.cpp:156] Memory required for data: 133926912
I0625 13:37:53.433317 29029 layer_factory.hpp:77] Creating layer bn5_2
I0625 13:37:53.433328 29029 net.cpp:91] Creating Layer bn5_2
I0625 13:37:53.433334 29029 net.cpp:425] bn5_2 <- conv5_2
I0625 13:37:53.433341 29029 net.cpp:386] bn5_2 -> conv5_2 (in-place)
I0625 13:37:53.433655 29029 net.cpp:141] Setting up bn5_2
I0625 13:37:53.433663 29029 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 13:37:53.433667 29029 net.cpp:156] Memory required for data: 134184960
I0625 13:37:53.433692 29029 layer_factory.hpp:77] Creating layer scale5_2
I0625 13:37:53.433703 29029 net.cpp:91] Creating Layer scale5_2
I0625 13:37:53.433708 29029 net.cpp:425] scale5_2 <- conv5_2
I0625 13:37:53.433714 29029 net.cpp:386] scale5_2 -> conv5_2 (in-place)
I0625 13:37:53.433781 29029 layer_factory.hpp:77] Creating layer scale5_2
I0625 13:37:53.433957 29029 net.cpp:141] Setting up scale5_2
I0625 13:37:53.433966 29029 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 13:37:53.433969 29029 net.cpp:156] Memory required for data: 134443008
I0625 13:37:53.433976 29029 layer_factory.hpp:77] Creating layer relu5_2
I0625 13:37:53.433986 29029 net.cpp:91] Creating Layer relu5_2
I0625 13:37:53.433992 29029 net.cpp:425] relu5_2 <- conv5_2
I0625 13:37:53.434000 29029 net.cpp:386] relu5_2 -> conv5_2 (in-place)
I0625 13:37:53.434427 29029 net.cpp:141] Setting up relu5_2
I0625 13:37:53.434442 29029 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 13:37:53.434448 29029 net.cpp:156] Memory required for data: 134701056
I0625 13:37:53.434451 29029 layer_factory.hpp:77] Creating layer pool5
I0625 13:37:53.434456 29029 layer_factory.cpp:90] cuDNN does not support multiple tops. Using Caffe's own pooling layer.
I0625 13:37:53.434463 29029 net.cpp:91] Creating Layer pool5
I0625 13:37:53.434468 29029 net.cpp:425] pool5 <- conv5_2
I0625 13:37:53.434476 29029 net.cpp:399] pool5 -> pool5
I0625 13:37:53.434484 29029 net.cpp:399] pool5 -> pool5_mask
I0625 13:37:53.434556 29029 net.cpp:141] Setting up pool5
I0625 13:37:53.434566 29029 net.cpp:148] Top shape: 1 256 7 9 (16128)
I0625 13:37:53.434571 29029 net.cpp:148] Top shape: 1 256 7 9 (16128)
I0625 13:37:53.434574 29029 net.cpp:156] Memory required for data: 134830080
I0625 13:37:53.434578 29029 layer_factory.hpp:77] Creating layer upsample5
I0625 13:37:53.434588 29029 net.cpp:91] Creating Layer upsample5
I0625 13:37:53.434593 29029 net.cpp:425] upsample5 <- pool5
I0625 13:37:53.434598 29029 net.cpp:425] upsample5 <- pool5_mask
I0625 13:37:53.434607 29029 net.cpp:399] upsample5 -> pool5_D
I0625 13:37:53.434615 29029 upsample_layer.cpp:31] Params 'pad_out_{}_' are deprecated. Please declare upsample height and width useing the upsample_h, upsample_w parameters.
I0625 13:37:53.434650 29029 net.cpp:141] Setting up upsample5
I0625 13:37:53.434659 29029 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 13:37:53.434661 29029 net.cpp:156] Memory required for data: 135088128
I0625 13:37:53.434665 29029 layer_factory.hpp:77] Creating layer conv5_2_D
I0625 13:37:53.434677 29029 net.cpp:91] Creating Layer conv5_2_D
I0625 13:37:53.434684 29029 net.cpp:425] conv5_2_D <- pool5_D
I0625 13:37:53.434690 29029 net.cpp:399] conv5_2_D -> conv5_2_D
I0625 13:37:53.442528 29029 net.cpp:141] Setting up conv5_2_D
I0625 13:37:53.442546 29029 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 13:37:53.442551 29029 net.cpp:156] Memory required for data: 135346176
I0625 13:37:53.442558 29029 layer_factory.hpp:77] Creating layer bn5_2_D
I0625 13:37:53.442567 29029 net.cpp:91] Creating Layer bn5_2_D
I0625 13:37:53.442572 29029 net.cpp:425] bn5_2_D <- conv5_2_D
I0625 13:37:53.442579 29029 net.cpp:386] bn5_2_D -> conv5_2_D (in-place)
I0625 13:37:53.442876 29029 net.cpp:141] Setting up bn5_2_D
I0625 13:37:53.442886 29029 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 13:37:53.442889 29029 net.cpp:156] Memory required for data: 135604224
I0625 13:37:53.442898 29029 layer_factory.hpp:77] Creating layer scale5_2_D
I0625 13:37:53.442906 29029 net.cpp:91] Creating Layer scale5_2_D
I0625 13:37:53.442910 29029 net.cpp:425] scale5_2_D <- conv5_2_D
I0625 13:37:53.442917 29029 net.cpp:386] scale5_2_D -> conv5_2_D (in-place)
I0625 13:37:53.442975 29029 layer_factory.hpp:77] Creating layer scale5_2_D
I0625 13:37:53.443142 29029 net.cpp:141] Setting up scale5_2_D
I0625 13:37:53.443157 29029 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 13:37:53.443161 29029 net.cpp:156] Memory required for data: 135862272
I0625 13:37:53.443181 29029 layer_factory.hpp:77] Creating layer relu5_2_D
I0625 13:37:53.443212 29029 net.cpp:91] Creating Layer relu5_2_D
I0625 13:37:53.443220 29029 net.cpp:425] relu5_2_D <- conv5_2_D
I0625 13:37:53.443229 29029 net.cpp:386] relu5_2_D -> conv5_2_D (in-place)
I0625 13:37:53.443632 29029 net.cpp:141] Setting up relu5_2_D
I0625 13:37:53.443648 29029 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 13:37:53.443651 29029 net.cpp:156] Memory required for data: 136120320
I0625 13:37:53.443656 29029 layer_factory.hpp:77] Creating layer conv5_1_D
I0625 13:37:53.443668 29029 net.cpp:91] Creating Layer conv5_1_D
I0625 13:37:53.443673 29029 net.cpp:425] conv5_1_D <- conv5_2_D
I0625 13:37:53.443681 29029 net.cpp:399] conv5_1_D -> conv5_1_D
I0625 13:37:53.451671 29029 net.cpp:141] Setting up conv5_1_D
I0625 13:37:53.451690 29029 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 13:37:53.451695 29029 net.cpp:156] Memory required for data: 136378368
I0625 13:37:53.451702 29029 layer_factory.hpp:77] Creating layer bn5_1_D
I0625 13:37:53.451714 29029 net.cpp:91] Creating Layer bn5_1_D
I0625 13:37:53.451719 29029 net.cpp:425] bn5_1_D <- conv5_1_D
I0625 13:37:53.451725 29029 net.cpp:386] bn5_1_D -> conv5_1_D (in-place)
I0625 13:37:53.452033 29029 net.cpp:141] Setting up bn5_1_D
I0625 13:37:53.452044 29029 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 13:37:53.452046 29029 net.cpp:156] Memory required for data: 136636416
I0625 13:37:53.452054 29029 layer_factory.hpp:77] Creating layer scale5_1_D
I0625 13:37:53.452064 29029 net.cpp:91] Creating Layer scale5_1_D
I0625 13:37:53.452067 29029 net.cpp:425] scale5_1_D <- conv5_1_D
I0625 13:37:53.452074 29029 net.cpp:386] scale5_1_D -> conv5_1_D (in-place)
I0625 13:37:53.452136 29029 layer_factory.hpp:77] Creating layer scale5_1_D
I0625 13:37:53.452302 29029 net.cpp:141] Setting up scale5_1_D
I0625 13:37:53.452311 29029 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 13:37:53.452314 29029 net.cpp:156] Memory required for data: 136894464
I0625 13:37:53.452321 29029 layer_factory.hpp:77] Creating layer relu5_1_D
I0625 13:37:53.452327 29029 net.cpp:91] Creating Layer relu5_1_D
I0625 13:37:53.452332 29029 net.cpp:425] relu5_1_D <- conv5_1_D
I0625 13:37:53.452337 29029 net.cpp:386] relu5_1_D -> conv5_1_D (in-place)
I0625 13:37:53.452563 29029 net.cpp:141] Setting up relu5_1_D
I0625 13:37:53.452575 29029 net.cpp:148] Top shape: 1 256 14 18 (64512)
I0625 13:37:53.452579 29029 net.cpp:156] Memory required for data: 137152512
I0625 13:37:53.452582 29029 layer_factory.hpp:77] Creating layer upsample4
I0625 13:37:53.452590 29029 net.cpp:91] Creating Layer upsample4
I0625 13:37:53.452594 29029 net.cpp:425] upsample4 <- conv5_1_D
I0625 13:37:53.452600 29029 net.cpp:425] upsample4 <- pool4_mask
I0625 13:37:53.452605 29029 net.cpp:399] upsample4 -> pool4_D
I0625 13:37:53.452613 29029 upsample_layer.cpp:31] Params 'pad_out_{}_' are deprecated. Please declare upsample height and width useing the upsample_h, upsample_w parameters.
I0625 13:37:53.452652 29029 net.cpp:141] Setting up upsample4
I0625 13:37:53.452661 29029 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 13:37:53.452664 29029 net.cpp:156] Memory required for data: 138184704
I0625 13:37:53.452667 29029 layer_factory.hpp:77] Creating layer conv4_2_D
I0625 13:37:53.452678 29029 net.cpp:91] Creating Layer conv4_2_D
I0625 13:37:53.452682 29029 net.cpp:425] conv4_2_D <- pool4_D
I0625 13:37:53.452689 29029 net.cpp:399] conv4_2_D -> conv4_2_D
I0625 13:37:53.460320 29029 net.cpp:141] Setting up conv4_2_D
I0625 13:37:53.460340 29029 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 13:37:53.460345 29029 net.cpp:156] Memory required for data: 139216896
I0625 13:37:53.460351 29029 layer_factory.hpp:77] Creating layer bn4_2_D
I0625 13:37:53.460362 29029 net.cpp:91] Creating Layer bn4_2_D
I0625 13:37:53.460367 29029 net.cpp:425] bn4_2_D <- conv4_2_D
I0625 13:37:53.460373 29029 net.cpp:386] bn4_2_D -> conv4_2_D (in-place)
I0625 13:37:53.460682 29029 net.cpp:141] Setting up bn4_2_D
I0625 13:37:53.460691 29029 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 13:37:53.460695 29029 net.cpp:156] Memory required for data: 140249088
I0625 13:37:53.460717 29029 layer_factory.hpp:77] Creating layer scale4_2_D
I0625 13:37:53.460727 29029 net.cpp:91] Creating Layer scale4_2_D
I0625 13:37:53.460732 29029 net.cpp:425] scale4_2_D <- conv4_2_D
I0625 13:37:53.460737 29029 net.cpp:386] scale4_2_D -> conv4_2_D (in-place)
I0625 13:37:53.460803 29029 layer_factory.hpp:77] Creating layer scale4_2_D
I0625 13:37:53.460983 29029 net.cpp:141] Setting up scale4_2_D
I0625 13:37:53.460991 29029 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 13:37:53.460994 29029 net.cpp:156] Memory required for data: 141281280
I0625 13:37:53.461000 29029 layer_factory.hpp:77] Creating layer relu4_2_D
I0625 13:37:53.461009 29029 net.cpp:91] Creating Layer relu4_2_D
I0625 13:37:53.461012 29029 net.cpp:425] relu4_2_D <- conv4_2_D
I0625 13:37:53.461017 29029 net.cpp:386] relu4_2_D -> conv4_2_D (in-place)
I0625 13:37:53.461415 29029 net.cpp:141] Setting up relu4_2_D
I0625 13:37:53.461429 29029 net.cpp:148] Top shape: 1 256 28 36 (258048)
I0625 13:37:53.461433 29029 net.cpp:156] Memory required for data: 142313472
I0625 13:37:53.461437 29029 layer_factory.hpp:77] Creating layer conv4_1_D
I0625 13:37:53.461452 29029 net.cpp:91] Creating Layer conv4_1_D
I0625 13:37:53.461455 29029 net.cpp:425] conv4_1_D <- conv4_2_D
I0625 13:37:53.461462 29029 net.cpp:399] conv4_1_D -> conv4_1_D
I0625 13:37:53.465838 29029 net.cpp:141] Setting up conv4_1_D
I0625 13:37:53.465853 29029 net.cpp:148] Top shape: 1 128 28 36 (129024)
I0625 13:37:53.465857 29029 net.cpp:156] Memory required for data: 142829568
I0625 13:37:53.465863 29029 layer_factory.hpp:77] Creating layer bn4_1_D
I0625 13:37:53.465873 29029 net.cpp:91] Creating Layer bn4_1_D
I0625 13:37:53.465876 29029 net.cpp:425] bn4_1_D <- conv4_1_D
I0625 13:37:53.465883 29029 net.cpp:386] bn4_1_D -> conv4_1_D (in-place)
I0625 13:37:53.466177 29029 net.cpp:141] Setting up bn4_1_D
I0625 13:37:53.466192 29029 net.cpp:148] Top shape: 1 128 28 36 (129024)
I0625 13:37:53.466194 29029 net.cpp:156] Memory required for data: 143345664
I0625 13:37:53.466202 29029 layer_factory.hpp:77] Creating layer scale4_1_D
I0625 13:37:53.466209 29029 net.cpp:91] Creating Layer scale4_1_D
I0625 13:37:53.466213 29029 net.cpp:425] scale4_1_D <- conv4_1_D
I0625 13:37:53.466220 29029 net.cpp:386] scale4_1_D -> conv4_1_D (in-place)
I0625 13:37:53.466277 29029 layer_factory.hpp:77] Creating layer scale4_1_D
I0625 13:37:53.466476 29029 net.cpp:141] Setting up scale4_1_D
I0625 13:37:53.466486 29029 net.cpp:148] Top shape: 1 128 28 36 (129024)
I0625 13:37:53.466490 29029 net.cpp:156] Memory required for data: 143861760
I0625 13:37:53.466496 29029 layer_factory.hpp:77] Creating layer relu4_1_D
I0625 13:37:53.466511 29029 net.cpp:91] Creating Layer relu4_1_D
I0625 13:37:53.466516 29029 net.cpp:425] relu4_1_D <- conv4_1_D
I0625 13:37:53.466521 29029 net.cpp:386] relu4_1_D -> conv4_1_D (in-place)
I0625 13:37:53.466902 29029 net.cpp:141] Setting up relu4_1_D
I0625 13:37:53.466917 29029 net.cpp:148] Top shape: 1 128 28 36 (129024)
I0625 13:37:53.466922 29029 net.cpp:156] Memory required for data: 144377856
I0625 13:37:53.466927 29029 layer_factory.hpp:77] Creating layer upsample3
I0625 13:37:53.466935 29029 net.cpp:91] Creating Layer upsample3
I0625 13:37:53.466939 29029 net.cpp:425] upsample3 <- conv4_1_D
I0625 13:37:53.466944 29029 net.cpp:425] upsample3 <- pool3_mask
I0625 13:37:53.466950 29029 net.cpp:399] upsample3 -> pool3_D
I0625 13:37:53.466958 29029 upsample_layer.cpp:31] Params 'pad_out_{}_' are deprecated. Please declare upsample height and width useing the upsample_h, upsample_w parameters.
I0625 13:37:53.467000 29029 net.cpp:141] Setting up upsample3
I0625 13:37:53.467008 29029 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 13:37:53.467011 29029 net.cpp:156] Memory required for data: 146442240
I0625 13:37:53.467015 29029 layer_factory.hpp:77] Creating layer conv3_2_D
I0625 13:37:53.467025 29029 net.cpp:91] Creating Layer conv3_2_D
I0625 13:37:53.467031 29029 net.cpp:425] conv3_2_D <- pool3_D
I0625 13:37:53.467038 29029 net.cpp:399] conv3_2_D -> conv3_2_D
I0625 13:37:53.470351 29029 net.cpp:141] Setting up conv3_2_D
I0625 13:37:53.470367 29029 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 13:37:53.470371 29029 net.cpp:156] Memory required for data: 148506624
I0625 13:37:53.470381 29029 layer_factory.hpp:77] Creating layer bn3_2_D
I0625 13:37:53.470388 29029 net.cpp:91] Creating Layer bn3_2_D
I0625 13:37:53.470392 29029 net.cpp:425] bn3_2_D <- conv3_2_D
I0625 13:37:53.470399 29029 net.cpp:386] bn3_2_D -> conv3_2_D (in-place)
I0625 13:37:53.470700 29029 net.cpp:141] Setting up bn3_2_D
I0625 13:37:53.470710 29029 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 13:37:53.470713 29029 net.cpp:156] Memory required for data: 150571008
I0625 13:37:53.470721 29029 layer_factory.hpp:77] Creating layer scale3_2_D
I0625 13:37:53.470728 29029 net.cpp:91] Creating Layer scale3_2_D
I0625 13:37:53.470732 29029 net.cpp:425] scale3_2_D <- conv3_2_D
I0625 13:37:53.470736 29029 net.cpp:386] scale3_2_D -> conv3_2_D (in-place)
I0625 13:37:53.470794 29029 layer_factory.hpp:77] Creating layer scale3_2_D
I0625 13:37:53.470978 29029 net.cpp:141] Setting up scale3_2_D
I0625 13:37:53.470986 29029 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 13:37:53.470990 29029 net.cpp:156] Memory required for data: 152635392
I0625 13:37:53.470996 29029 layer_factory.hpp:77] Creating layer relu3_2_D
I0625 13:37:53.471005 29029 net.cpp:91] Creating Layer relu3_2_D
I0625 13:37:53.471009 29029 net.cpp:425] relu3_2_D <- conv3_2_D
I0625 13:37:53.471015 29029 net.cpp:386] relu3_2_D -> conv3_2_D (in-place)
I0625 13:37:53.471257 29029 net.cpp:141] Setting up relu3_2_D
I0625 13:37:53.471271 29029 net.cpp:148] Top shape: 1 128 56 72 (516096)
I0625 13:37:53.471274 29029 net.cpp:156] Memory required for data: 154699776
I0625 13:37:53.471278 29029 layer_factory.hpp:77] Creating layer conv3_1_D
I0625 13:37:53.471288 29029 net.cpp:91] Creating Layer conv3_1_D
I0625 13:37:53.471292 29029 net.cpp:425] conv3_1_D <- conv3_2_D
I0625 13:37:53.471302 29029 net.cpp:399] conv3_1_D -> conv3_1_D
I0625 13:37:53.473295 29029 net.cpp:141] Setting up conv3_1_D
I0625 13:37:53.473316 29029 net.cpp:148] Top shape: 1 64 56 72 (258048)
I0625 13:37:53.473326 29029 net.cpp:156] Memory required for data: 155731968
I0625 13:37:53.473335 29029 layer_factory.hpp:77] Creating layer bn3_1_D
I0625 13:37:53.473352 29029 net.cpp:91] Creating Layer bn3_1_D
I0625 13:37:53.473362 29029 net.cpp:425] bn3_1_D <- conv3_1_D
I0625 13:37:53.473374 29029 net.cpp:386] bn3_1_D -> conv3_1_D (in-place)
I0625 13:37:53.473740 29029 net.cpp:141] Setting up bn3_1_D
I0625 13:37:53.473753 29029 net.cpp:148] Top shape: 1 64 56 72 (258048)
I0625 13:37:53.473757 29029 net.cpp:156] Memory required for data: 156764160
I0625 13:37:53.473765 29029 layer_factory.hpp:77] Creating layer scale3_1_D
I0625 13:37:53.473775 29029 net.cpp:91] Creating Layer scale3_1_D
I0625 13:37:53.473781 29029 net.cpp:425] scale3_1_D <- conv3_1_D
I0625 13:37:53.473788 29029 net.cpp:386] scale3_1_D -> conv3_1_D (in-place)
I0625 13:37:53.473851 29029 layer_factory.hpp:77] Creating layer scale3_1_D
I0625 13:37:53.474036 29029 net.cpp:141] Setting up scale3_1_D
I0625 13:37:53.474045 29029 net.cpp:148] Top shape: 1 64 56 72 (258048)
I0625 13:37:53.474048 29029 net.cpp:156] Memory required for data: 157796352
I0625 13:37:53.474055 29029 layer_factory.hpp:77] Creating layer relu3_1_D
I0625 13:37:53.474061 29029 net.cpp:91] Creating Layer relu3_1_D
I0625 13:37:53.474064 29029 net.cpp:425] relu3_1_D <- conv3_1_D
I0625 13:37:53.474071 29029 net.cpp:386] relu3_1_D -> conv3_1_D (in-place)
I0625 13:37:53.474510 29029 net.cpp:141] Setting up relu3_1_D
I0625 13:37:53.474525 29029 net.cpp:148] Top shape: 1 64 56 72 (258048)
I0625 13:37:53.474529 29029 net.cpp:156] Memory required for data: 158828544
I0625 13:37:53.474534 29029 layer_factory.hpp:77] Creating layer upsample2
I0625 13:37:53.474542 29029 net.cpp:91] Creating Layer upsample2
I0625 13:37:53.474546 29029 net.cpp:425] upsample2 <- conv3_1_D
I0625 13:37:53.474551 29029 net.cpp:425] upsample2 <- pool2_mask
I0625 13:37:53.474557 29029 net.cpp:399] upsample2 -> pool2_D
I0625 13:37:53.474581 29029 upsample_layer.cpp:31] Params 'pad_out_{}_' are deprecated. Please declare upsample height and width useing the upsample_h, upsample_w parameters.
I0625 13:37:53.474624 29029 net.cpp:141] Setting up upsample2
I0625 13:37:53.474634 29029 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 13:37:53.474638 29029 net.cpp:156] Memory required for data: 162957312
I0625 13:37:53.474642 29029 layer_factory.hpp:77] Creating layer conv2_2_D
I0625 13:37:53.474652 29029 net.cpp:91] Creating Layer conv2_2_D
I0625 13:37:53.474655 29029 net.cpp:425] conv2_2_D <- pool2_D
I0625 13:37:53.474661 29029 net.cpp:399] conv2_2_D -> conv2_2_D
I0625 13:37:53.476243 29029 net.cpp:141] Setting up conv2_2_D
I0625 13:37:53.476266 29029 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 13:37:53.476275 29029 net.cpp:156] Memory required for data: 167086080
I0625 13:37:53.476285 29029 layer_factory.hpp:77] Creating layer bn2_2_D
I0625 13:37:53.476300 29029 net.cpp:91] Creating Layer bn2_2_D
I0625 13:37:53.476306 29029 net.cpp:425] bn2_2_D <- conv2_2_D
I0625 13:37:53.476318 29029 net.cpp:386] bn2_2_D -> conv2_2_D (in-place)
I0625 13:37:53.476668 29029 net.cpp:141] Setting up bn2_2_D
I0625 13:37:53.476680 29029 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 13:37:53.476683 29029 net.cpp:156] Memory required for data: 171214848
I0625 13:37:53.476691 29029 layer_factory.hpp:77] Creating layer scale2_2_D
I0625 13:37:53.476699 29029 net.cpp:91] Creating Layer scale2_2_D
I0625 13:37:53.476703 29029 net.cpp:425] scale2_2_D <- conv2_2_D
I0625 13:37:53.476711 29029 net.cpp:386] scale2_2_D -> conv2_2_D (in-place)
I0625 13:37:53.476770 29029 layer_factory.hpp:77] Creating layer scale2_2_D
I0625 13:37:53.476958 29029 net.cpp:141] Setting up scale2_2_D
I0625 13:37:53.476968 29029 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 13:37:53.476970 29029 net.cpp:156] Memory required for data: 175343616
I0625 13:37:53.476976 29029 layer_factory.hpp:77] Creating layer relu2_2_D
I0625 13:37:53.476981 29029 net.cpp:91] Creating Layer relu2_2_D
I0625 13:37:53.476986 29029 net.cpp:425] relu2_2_D <- conv2_2_D
I0625 13:37:53.476990 29029 net.cpp:386] relu2_2_D -> conv2_2_D (in-place)
I0625 13:37:53.477435 29029 net.cpp:141] Setting up relu2_2_D
I0625 13:37:53.477452 29029 net.cpp:148] Top shape: 1 64 112 144 (1032192)
I0625 13:37:53.477457 29029 net.cpp:156] Memory required for data: 179472384
I0625 13:37:53.477460 29029 layer_factory.hpp:77] Creating layer conv2_1_D
I0625 13:37:53.477471 29029 net.cpp:91] Creating Layer conv2_1_D
I0625 13:37:53.477478 29029 net.cpp:425] conv2_1_D <- conv2_2_D
I0625 13:37:53.477484 29029 net.cpp:399] conv2_1_D -> conv2_1_D
I0625 13:37:53.478950 29029 net.cpp:141] Setting up conv2_1_D
I0625 13:37:53.478965 29029 net.cpp:148] Top shape: 1 32 112 144 (516096)
I0625 13:37:53.478970 29029 net.cpp:156] Memory required for data: 181536768
I0625 13:37:53.478976 29029 layer_factory.hpp:77] Creating layer bn2_1_D
I0625 13:37:53.478982 29029 net.cpp:91] Creating Layer bn2_1_D
I0625 13:37:53.478986 29029 net.cpp:425] bn2_1_D <- conv2_1_D
I0625 13:37:53.478993 29029 net.cpp:386] bn2_1_D -> conv2_1_D (in-place)
I0625 13:37:53.479393 29029 net.cpp:141] Setting up bn2_1_D
I0625 13:37:53.479408 29029 net.cpp:148] Top shape: 1 32 112 144 (516096)
I0625 13:37:53.479411 29029 net.cpp:156] Memory required for data: 183601152
I0625 13:37:53.479420 29029 layer_factory.hpp:77] Creating layer scale2_1_D
I0625 13:37:53.479429 29029 net.cpp:91] Creating Layer scale2_1_D
I0625 13:37:53.479432 29029 net.cpp:425] scale2_1_D <- conv2_1_D
I0625 13:37:53.479440 29029 net.cpp:386] scale2_1_D -> conv2_1_D (in-place)
I0625 13:37:53.479506 29029 layer_factory.hpp:77] Creating layer scale2_1_D
I0625 13:37:53.479701 29029 net.cpp:141] Setting up scale2_1_D
I0625 13:37:53.479709 29029 net.cpp:148] Top shape: 1 32 112 144 (516096)
I0625 13:37:53.479712 29029 net.cpp:156] Memory required for data: 185665536
I0625 13:37:53.479718 29029 layer_factory.hpp:77] Creating layer relu2_1_D
I0625 13:37:53.479724 29029 net.cpp:91] Creating Layer relu2_1_D
I0625 13:37:53.479742 29029 net.cpp:425] relu2_1_D <- conv2_1_D
I0625 13:37:53.479748 29029 net.cpp:386] relu2_1_D -> conv2_1_D (in-place)
I0625 13:37:53.479957 29029 net.cpp:141] Setting up relu2_1_D
I0625 13:37:53.479969 29029 net.cpp:148] Top shape: 1 32 112 144 (516096)
I0625 13:37:53.479972 29029 net.cpp:156] Memory required for data: 187729920
I0625 13:37:53.479975 29029 layer_factory.hpp:77] Creating layer upsample1
I0625 13:37:53.479984 29029 net.cpp:91] Creating Layer upsample1
I0625 13:37:53.479987 29029 net.cpp:425] upsample1 <- conv2_1_D
I0625 13:37:53.479991 29029 net.cpp:425] upsample1 <- pool1_mask
I0625 13:37:53.479996 29029 net.cpp:399] upsample1 -> pool1_D
I0625 13:37:53.480003 29029 upsample_layer.cpp:31] Params 'pad_out_{}_' are deprecated. Please declare upsample height and width useing the upsample_h, upsample_w parameters.
I0625 13:37:53.480041 29029 net.cpp:141] Setting up upsample1
I0625 13:37:53.480049 29029 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 13:37:53.480052 29029 net.cpp:156] Memory required for data: 195987456
I0625 13:37:53.480056 29029 layer_factory.hpp:77] Creating layer conv1_2_D
I0625 13:37:53.480067 29029 net.cpp:91] Creating Layer conv1_2_D
I0625 13:37:53.480070 29029 net.cpp:425] conv1_2_D <- pool1_D
I0625 13:37:53.480075 29029 net.cpp:399] conv1_2_D -> conv1_2_D
I0625 13:37:53.481717 29029 net.cpp:141] Setting up conv1_2_D
I0625 13:37:53.481732 29029 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 13:37:53.481736 29029 net.cpp:156] Memory required for data: 204244992
I0625 13:37:53.481742 29029 layer_factory.hpp:77] Creating layer bn1_2_D
I0625 13:37:53.481751 29029 net.cpp:91] Creating Layer bn1_2_D
I0625 13:37:53.481755 29029 net.cpp:425] bn1_2_D <- conv1_2_D
I0625 13:37:53.481762 29029 net.cpp:386] bn1_2_D -> conv1_2_D (in-place)
I0625 13:37:53.482818 29029 net.cpp:141] Setting up bn1_2_D
I0625 13:37:53.482833 29029 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 13:37:53.482837 29029 net.cpp:156] Memory required for data: 212502528
I0625 13:37:53.482846 29029 layer_factory.hpp:77] Creating layer scale1_2_D
I0625 13:37:53.482854 29029 net.cpp:91] Creating Layer scale1_2_D
I0625 13:37:53.482858 29029 net.cpp:425] scale1_2_D <- conv1_2_D
I0625 13:37:53.482864 29029 net.cpp:386] scale1_2_D -> conv1_2_D (in-place)
I0625 13:37:53.482931 29029 layer_factory.hpp:77] Creating layer scale1_2_D
I0625 13:37:53.483196 29029 net.cpp:141] Setting up scale1_2_D
I0625 13:37:53.483211 29029 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 13:37:53.483217 29029 net.cpp:156] Memory required for data: 220760064
I0625 13:37:53.483227 29029 layer_factory.hpp:77] Creating layer relu1_2_D
I0625 13:37:53.483240 29029 net.cpp:91] Creating Layer relu1_2_D
I0625 13:37:53.483248 29029 net.cpp:425] relu1_2_D <- conv1_2_D
I0625 13:37:53.483258 29029 net.cpp:386] relu1_2_D -> conv1_2_D (in-place)
I0625 13:37:53.483674 29029 net.cpp:141] Setting up relu1_2_D
I0625 13:37:53.483687 29029 net.cpp:148] Top shape: 1 32 224 288 (2064384)
I0625 13:37:53.483691 29029 net.cpp:156] Memory required for data: 229017600
I0625 13:37:53.483695 29029 layer_factory.hpp:77] Creating layer conv1_1_D
I0625 13:37:53.483712 29029 net.cpp:91] Creating Layer conv1_1_D
I0625 13:37:53.483716 29029 net.cpp:425] conv1_1_D <- conv1_2_D
I0625 13:37:53.483724 29029 net.cpp:399] conv1_1_D -> conv1_1_D
I0625 13:37:53.485406 29029 net.cpp:141] Setting up conv1_1_D
I0625 13:37:53.485421 29029 net.cpp:148] Top shape: 1 2 224 288 (129024)
I0625 13:37:53.485424 29029 net.cpp:156] Memory required for data: 229533696
I0625 13:37:53.485433 29029 layer_factory.hpp:77] Creating layer conv1_1_D_conv1_1_D_0_split
I0625 13:37:53.485445 29029 net.cpp:91] Creating Layer conv1_1_D_conv1_1_D_0_split
I0625 13:37:53.485452 29029 net.cpp:425] conv1_1_D_conv1_1_D_0_split <- conv1_1_D
I0625 13:37:53.485462 29029 net.cpp:399] conv1_1_D_conv1_1_D_0_split -> conv1_1_D_conv1_1_D_0_split_0
I0625 13:37:53.485477 29029 net.cpp:399] conv1_1_D_conv1_1_D_0_split -> conv1_1_D_conv1_1_D_0_split_1
I0625 13:37:53.485548 29029 net.cpp:141] Setting up conv1_1_D_conv1_1_D_0_split
I0625 13:37:53.485571 29029 net.cpp:148] Top shape: 1 2 224 288 (129024)
I0625 13:37:53.485577 29029 net.cpp:148] Top shape: 1 2 224 288 (129024)
I0625 13:37:53.485579 29029 net.cpp:156] Memory required for data: 230565888
I0625 13:37:53.485584 29029 layer_factory.hpp:77] Creating layer loss
I0625 13:37:53.485591 29029 net.cpp:91] Creating Layer loss
I0625 13:37:53.485595 29029 net.cpp:425] loss <- conv1_1_D_conv1_1_D_0_split_0
I0625 13:37:53.485600 29029 net.cpp:425] loss <- label_data_1_split_0
I0625 13:37:53.485610 29029 net.cpp:399] loss -> loss
I0625 13:37:53.485621 29029 layer_factory.hpp:77] Creating layer loss
I0625 13:37:53.486917 29029 net.cpp:141] Setting up loss
I0625 13:37:53.486933 29029 net.cpp:148] Top shape: (1)
I0625 13:37:53.486939 29029 net.cpp:151]     with loss weight 1
I0625 13:37:53.486953 29029 net.cpp:156] Memory required for data: 230565892
I0625 13:37:53.486964 29029 layer_factory.hpp:77] Creating layer accuracy
I0625 13:37:53.486974 29029 net.cpp:91] Creating Layer accuracy
I0625 13:37:53.486979 29029 net.cpp:425] accuracy <- conv1_1_D_conv1_1_D_0_split_1
I0625 13:37:53.486984 29029 net.cpp:425] accuracy <- label_data_1_split_1
I0625 13:37:53.486991 29029 net.cpp:399] accuracy -> accuracy
I0625 13:37:53.487001 29029 net.cpp:141] Setting up accuracy
I0625 13:37:53.487006 29029 net.cpp:148] Top shape: (1)
I0625 13:37:53.487010 29029 net.cpp:156] Memory required for data: 230565896
I0625 13:37:53.487012 29029 net.cpp:219] accuracy does not need backward computation.
I0625 13:37:53.487016 29029 net.cpp:217] loss needs backward computation.
I0625 13:37:53.487020 29029 net.cpp:217] conv1_1_D_conv1_1_D_0_split needs backward computation.
I0625 13:37:53.487023 29029 net.cpp:217] conv1_1_D needs backward computation.
I0625 13:37:53.487027 29029 net.cpp:217] relu1_2_D needs backward computation.
I0625 13:37:53.487030 29029 net.cpp:217] scale1_2_D needs backward computation.
I0625 13:37:53.487033 29029 net.cpp:217] bn1_2_D needs backward computation.
I0625 13:37:53.487036 29029 net.cpp:217] conv1_2_D needs backward computation.
I0625 13:37:53.487040 29029 net.cpp:217] upsample1 needs backward computation.
I0625 13:37:53.487043 29029 net.cpp:217] relu2_1_D needs backward computation.
I0625 13:37:53.487046 29029 net.cpp:217] scale2_1_D needs backward computation.
I0625 13:37:53.487049 29029 net.cpp:217] bn2_1_D needs backward computation.
I0625 13:37:53.487052 29029 net.cpp:217] conv2_1_D needs backward computation.
I0625 13:37:53.487056 29029 net.cpp:217] relu2_2_D needs backward computation.
I0625 13:37:53.487059 29029 net.cpp:217] scale2_2_D needs backward computation.
I0625 13:37:53.487062 29029 net.cpp:217] bn2_2_D needs backward computation.
I0625 13:37:53.487066 29029 net.cpp:217] conv2_2_D needs backward computation.
I0625 13:37:53.487068 29029 net.cpp:217] upsample2 needs backward computation.
I0625 13:37:53.487072 29029 net.cpp:217] relu3_1_D needs backward computation.
I0625 13:37:53.487076 29029 net.cpp:217] scale3_1_D needs backward computation.
I0625 13:37:53.487078 29029 net.cpp:217] bn3_1_D needs backward computation.
I0625 13:37:53.487082 29029 net.cpp:217] conv3_1_D needs backward computation.
I0625 13:37:53.487084 29029 net.cpp:217] relu3_2_D needs backward computation.
I0625 13:37:53.487087 29029 net.cpp:217] scale3_2_D needs backward computation.
I0625 13:37:53.487090 29029 net.cpp:217] bn3_2_D needs backward computation.
I0625 13:37:53.487093 29029 net.cpp:217] conv3_2_D needs backward computation.
I0625 13:37:53.487097 29029 net.cpp:217] upsample3 needs backward computation.
I0625 13:37:53.487102 29029 net.cpp:217] relu4_1_D needs backward computation.
I0625 13:37:53.487105 29029 net.cpp:217] scale4_1_D needs backward computation.
I0625 13:37:53.487108 29029 net.cpp:217] bn4_1_D needs backward computation.
I0625 13:37:53.487112 29029 net.cpp:217] conv4_1_D needs backward computation.
I0625 13:37:53.487114 29029 net.cpp:217] relu4_2_D needs backward computation.
I0625 13:37:53.487118 29029 net.cpp:217] scale4_2_D needs backward computation.
I0625 13:37:53.487133 29029 net.cpp:217] bn4_2_D needs backward computation.
I0625 13:37:53.487138 29029 net.cpp:217] conv4_2_D needs backward computation.
I0625 13:37:53.487141 29029 net.cpp:217] upsample4 needs backward computation.
I0625 13:37:53.487145 29029 net.cpp:217] relu5_1_D needs backward computation.
I0625 13:37:53.487156 29029 net.cpp:217] scale5_1_D needs backward computation.
I0625 13:37:53.487160 29029 net.cpp:217] bn5_1_D needs backward computation.
I0625 13:37:53.487164 29029 net.cpp:217] conv5_1_D needs backward computation.
I0625 13:37:53.487167 29029 net.cpp:217] relu5_2_D needs backward computation.
I0625 13:37:53.487170 29029 net.cpp:217] scale5_2_D needs backward computation.
I0625 13:37:53.487174 29029 net.cpp:217] bn5_2_D needs backward computation.
I0625 13:37:53.487179 29029 net.cpp:217] conv5_2_D needs backward computation.
I0625 13:37:53.487185 29029 net.cpp:217] upsample5 needs backward computation.
I0625 13:37:53.487190 29029 net.cpp:217] pool5 needs backward computation.
I0625 13:37:53.487195 29029 net.cpp:217] relu5_2 needs backward computation.
I0625 13:37:53.487205 29029 net.cpp:217] scale5_2 needs backward computation.
I0625 13:37:53.487210 29029 net.cpp:217] bn5_2 needs backward computation.
I0625 13:37:53.487215 29029 net.cpp:217] conv5_2 needs backward computation.
I0625 13:37:53.487220 29029 net.cpp:217] relu5_1 needs backward computation.
I0625 13:37:53.487224 29029 net.cpp:217] scale5_1 needs backward computation.
I0625 13:37:53.487228 29029 net.cpp:217] bn5_1 needs backward computation.
I0625 13:37:53.487233 29029 net.cpp:217] conv5_1 needs backward computation.
I0625 13:37:53.487238 29029 net.cpp:217] pool4 needs backward computation.
I0625 13:37:53.487244 29029 net.cpp:217] relu4_2 needs backward computation.
I0625 13:37:53.487249 29029 net.cpp:217] scale4_2 needs backward computation.
I0625 13:37:53.487254 29029 net.cpp:217] bn4_2 needs backward computation.
I0625 13:37:53.487257 29029 net.cpp:217] conv4_2 needs backward computation.
I0625 13:37:53.487262 29029 net.cpp:217] relu4_1 needs backward computation.
I0625 13:37:53.487264 29029 net.cpp:217] scale4_1 needs backward computation.
I0625 13:37:53.487267 29029 net.cpp:217] bn4_1 needs backward computation.
I0625 13:37:53.487270 29029 net.cpp:217] conv4_1 needs backward computation.
I0625 13:37:53.487273 29029 net.cpp:217] pool3 needs backward computation.
I0625 13:37:53.487277 29029 net.cpp:217] relu3_2 needs backward computation.
I0625 13:37:53.487280 29029 net.cpp:217] scale3_2 needs backward computation.
I0625 13:37:53.487283 29029 net.cpp:217] bn3_2 needs backward computation.
I0625 13:37:53.487287 29029 net.cpp:217] conv3_2 needs backward computation.
I0625 13:37:53.487289 29029 net.cpp:217] relu3_1 needs backward computation.
I0625 13:37:53.487293 29029 net.cpp:217] scale3_1 needs backward computation.
I0625 13:37:53.487295 29029 net.cpp:217] bn3_1 needs backward computation.
I0625 13:37:53.487298 29029 net.cpp:217] conv3_1 needs backward computation.
I0625 13:37:53.487301 29029 net.cpp:217] pool2 needs backward computation.
I0625 13:37:53.487305 29029 net.cpp:217] relu2_2 needs backward computation.
I0625 13:37:53.487308 29029 net.cpp:217] scale2_2 needs backward computation.
I0625 13:37:53.487313 29029 net.cpp:217] bn2_2 needs backward computation.
I0625 13:37:53.487315 29029 net.cpp:217] conv2_2 needs backward computation.
I0625 13:37:53.487318 29029 net.cpp:217] relu2_1 needs backward computation.
I0625 13:37:53.487321 29029 net.cpp:217] scale2_1 needs backward computation.
I0625 13:37:53.487324 29029 net.cpp:217] bn2_1 needs backward computation.
I0625 13:37:53.487329 29029 net.cpp:217] conv2_1 needs backward computation.
I0625 13:37:53.487331 29029 net.cpp:217] pool1 needs backward computation.
I0625 13:37:53.487334 29029 net.cpp:217] relu1_2 needs backward computation.
I0625 13:37:53.487337 29029 net.cpp:217] scale1_2 needs backward computation.
I0625 13:37:53.487340 29029 net.cpp:217] bn1_2 needs backward computation.
I0625 13:37:53.487344 29029 net.cpp:217] conv1_2 needs backward computation.
I0625 13:37:53.487346 29029 net.cpp:217] relu1_1 needs backward computation.
I0625 13:37:53.487360 29029 net.cpp:217] scale1_1 needs backward computation.
I0625 13:37:53.487362 29029 net.cpp:217] bn1_1 needs backward computation.
I0625 13:37:53.487366 29029 net.cpp:217] conv1_1 needs backward computation.
I0625 13:37:53.487370 29029 net.cpp:219] label_data_1_split does not need backward computation.
I0625 13:37:53.487375 29029 net.cpp:219] data does not need backward computation.
I0625 13:37:53.487377 29029 net.cpp:261] This network produces output accuracy
I0625 13:37:53.487381 29029 net.cpp:261] This network produces output loss
I0625 13:37:53.487426 29029 net.cpp:274] Network initialization done.
I0625 13:37:53.487743 29029 solver.cpp:60] Solver scaffolding done.
I0625 13:37:53.494009 29029 caffe.cpp:219] Starting Optimization
I0625 13:37:53.494022 29029 solver.cpp:279] Solving segnet
I0625 13:37:53.494025 29029 solver.cpp:280] Learning Rate Policy: step
I0625 13:37:53.498879 29029 solver.cpp:337] Iteration 0, Testing net (#0)
I0625 13:37:53.964040 29029 solver.cpp:404]     Test net output #0: accuracy = 0.368623
I0625 13:37:53.964069 29029 solver.cpp:404]     Test net output #1: loss = 0.907907 (* 1 = 0.907907 loss)
I0625 13:37:54.961030 29029 solver.cpp:228] Iteration 0, loss = 0.905813
I0625 13:37:54.961055 29029 solver.cpp:244]     Train net output #0: accuracy = 0.36919
I0625 13:37:54.961063 29029 solver.cpp:244]     Train net output #1: loss = 0.905813 (* 1 = 0.905813 loss)
I0625 13:37:54.961079 29029 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0625 13:38:13.813066 29029 solver.cpp:228] Iteration 20, loss = 0.255357
I0625 13:38:13.813093 29029 solver.cpp:244]     Train net output #0: accuracy = 0.968251
I0625 13:38:13.813102 29029 solver.cpp:244]     Train net output #1: loss = 0.255357 (* 1 = 0.255357 loss)
I0625 13:38:13.813107 29029 sgd_solver.cpp:106] Iteration 20, lr = 0.01
I0625 13:38:33.031180 29029 solver.cpp:228] Iteration 40, loss = 0.148087
I0625 13:38:33.031253 29029 solver.cpp:244]     Train net output #0: accuracy = 0.967994
I0625 13:38:33.031262 29029 solver.cpp:244]     Train net output #1: loss = 0.148087 (* 1 = 0.148087 loss)
I0625 13:38:33.031267 29029 sgd_solver.cpp:106] Iteration 40, lr = 0.01
I0625 13:38:52.214843 29029 solver.cpp:228] Iteration 60, loss = 0.129526
I0625 13:38:52.214869 29029 solver.cpp:244]     Train net output #0: accuracy = 0.97055
I0625 13:38:52.214877 29029 solver.cpp:244]     Train net output #1: loss = 0.129526 (* 1 = 0.129526 loss)
I0625 13:38:52.214882 29029 sgd_solver.cpp:106] Iteration 60, lr = 0.01
I0625 13:39:11.473778 29029 solver.cpp:228] Iteration 80, loss = 0.130329
I0625 13:39:11.473865 29029 solver.cpp:244]     Train net output #0: accuracy = 0.967913
I0625 13:39:11.473875 29029 solver.cpp:244]     Train net output #1: loss = 0.130329 (* 1 = 0.130329 loss)
I0625 13:39:11.473880 29029 sgd_solver.cpp:106] Iteration 80, lr = 0.01
I0625 13:39:30.209439 29029 solver.cpp:337] Iteration 100, Testing net (#0)
I0625 13:39:30.619518 29029 solver.cpp:404]     Test net output #0: accuracy = 0.968101
I0625 13:39:30.619551 29029 solver.cpp:404]     Test net output #1: loss = 0.122592 (* 1 = 0.122592 loss)
I0625 13:39:31.168406 29029 solver.cpp:228] Iteration 100, loss = 0.118687
I0625 13:39:31.168431 29029 solver.cpp:244]     Train net output #0: accuracy = 0.969089
I0625 13:39:31.168438 29029 solver.cpp:244]     Train net output #1: loss = 0.118687 (* 1 = 0.118687 loss)
I0625 13:39:31.168443 29029 sgd_solver.cpp:106] Iteration 100, lr = 0.01
I0625 13:39:50.509346 29029 solver.cpp:228] Iteration 120, loss = 0.107632
I0625 13:39:50.509481 29029 solver.cpp:244]     Train net output #0: accuracy = 0.967956
I0625 13:39:50.509492 29029 solver.cpp:244]     Train net output #1: loss = 0.107632 (* 1 = 0.107632 loss)
I0625 13:39:50.509498 29029 sgd_solver.cpp:106] Iteration 120, lr = 0.01
I0625 13:40:09.852839 29029 solver.cpp:228] Iteration 140, loss = 0.0925134
I0625 13:40:09.852865 29029 solver.cpp:244]     Train net output #0: accuracy = 0.971373
I0625 13:40:09.852874 29029 solver.cpp:244]     Train net output #1: loss = 0.0925134 (* 1 = 0.0925134 loss)
I0625 13:40:09.852879 29029 sgd_solver.cpp:106] Iteration 140, lr = 0.01
I0625 13:40:29.196876 29029 solver.cpp:228] Iteration 160, loss = 0.0913169
I0625 13:40:29.196996 29029 solver.cpp:244]     Train net output #0: accuracy = 0.969641
I0625 13:40:29.197008 29029 solver.cpp:244]     Train net output #1: loss = 0.0913169 (* 1 = 0.0913169 loss)
I0625 13:40:29.197013 29029 sgd_solver.cpp:106] Iteration 160, lr = 0.01
I0625 13:40:48.618640 29029 solver.cpp:228] Iteration 180, loss = 0.0783201
I0625 13:40:48.618666 29029 solver.cpp:244]     Train net output #0: accuracy = 0.970882
I0625 13:40:48.618674 29029 solver.cpp:244]     Train net output #1: loss = 0.0783201 (* 1 = 0.0783201 loss)
I0625 13:40:48.618680 29029 sgd_solver.cpp:106] Iteration 180, lr = 0.01
I0625 13:41:07.469193 29029 solver.cpp:337] Iteration 200, Testing net (#0)
I0625 13:41:07.883586 29029 solver.cpp:404]     Test net output #0: accuracy = 0.968578
I0625 13:41:07.883610 29029 solver.cpp:404]     Test net output #1: loss = 0.0706572 (* 1 = 0.0706572 loss)
I0625 13:41:08.437549 29029 solver.cpp:228] Iteration 200, loss = 0.0774035
I0625 13:41:08.437575 29029 solver.cpp:244]     Train net output #0: accuracy = 0.968821
I0625 13:41:08.437583 29029 solver.cpp:244]     Train net output #1: loss = 0.0774035 (* 1 = 0.0774035 loss)
I0625 13:41:08.437588 29029 sgd_solver.cpp:106] Iteration 200, lr = 0.01
I0625 13:41:27.832449 29029 solver.cpp:228] Iteration 220, loss = 0.0659719
I0625 13:41:27.832475 29029 solver.cpp:244]     Train net output #0: accuracy = 0.968736
I0625 13:41:27.832484 29029 solver.cpp:244]     Train net output #1: loss = 0.0659719 (* 1 = 0.0659719 loss)
I0625 13:41:27.832489 29029 sgd_solver.cpp:106] Iteration 220, lr = 0.01
I0625 13:41:47.214234 29029 solver.cpp:228] Iteration 240, loss = 0.0672657
I0625 13:41:47.214329 29029 solver.cpp:244]     Train net output #0: accuracy = 0.96861
I0625 13:41:47.214339 29029 solver.cpp:244]     Train net output #1: loss = 0.0672657 (* 1 = 0.0672657 loss)
I0625 13:41:47.214344 29029 sgd_solver.cpp:106] Iteration 240, lr = 0.01
I0625 13:42:06.562861 29029 solver.cpp:228] Iteration 260, loss = 0.0607727
I0625 13:42:06.562887 29029 solver.cpp:244]     Train net output #0: accuracy = 0.96897
I0625 13:42:06.562896 29029 solver.cpp:244]     Train net output #1: loss = 0.0607727 (* 1 = 0.0607727 loss)
I0625 13:42:06.562901 29029 sgd_solver.cpp:106] Iteration 260, lr = 0.01
I0625 13:42:25.914638 29029 solver.cpp:228] Iteration 280, loss = 0.0659933
I0625 13:42:25.914742 29029 solver.cpp:244]     Train net output #0: accuracy = 0.966654
I0625 13:42:25.914752 29029 solver.cpp:244]     Train net output #1: loss = 0.0659933 (* 1 = 0.0659933 loss)
I0625 13:42:25.914757 29029 sgd_solver.cpp:106] Iteration 280, lr = 0.01
I0625 13:42:44.794082 29029 solver.cpp:337] Iteration 300, Testing net (#0)
I0625 13:42:45.204870 29029 solver.cpp:404]     Test net output #0: accuracy = 0.971209
I0625 13:42:45.204905 29029 solver.cpp:404]     Test net output #1: loss = 0.0579702 (* 1 = 0.0579702 loss)
I0625 13:42:45.754463 29029 solver.cpp:228] Iteration 300, loss = 0.0685448
I0625 13:42:45.754488 29029 solver.cpp:244]     Train net output #0: accuracy = 0.969316
I0625 13:42:45.754508 29029 solver.cpp:244]     Train net output #1: loss = 0.0685448 (* 1 = 0.0685448 loss)
I0625 13:42:45.754513 29029 sgd_solver.cpp:106] Iteration 300, lr = 0.01
I0625 13:43:05.120605 29029 solver.cpp:228] Iteration 320, loss = 0.0528958
I0625 13:43:05.120707 29029 solver.cpp:244]     Train net output #0: accuracy = 0.971034
I0625 13:43:05.120715 29029 solver.cpp:244]     Train net output #1: loss = 0.0528958 (* 1 = 0.0528958 loss)
I0625 13:43:05.120720 29029 sgd_solver.cpp:106] Iteration 320, lr = 0.01
I0625 13:43:24.468062 29029 solver.cpp:228] Iteration 340, loss = 0.0567471
I0625 13:43:24.468089 29029 solver.cpp:244]     Train net output #0: accuracy = 0.966567
I0625 13:43:24.468098 29029 solver.cpp:244]     Train net output #1: loss = 0.0567471 (* 1 = 0.0567471 loss)
I0625 13:43:24.468103 29029 sgd_solver.cpp:106] Iteration 340, lr = 0.01
I0625 13:43:43.833931 29029 solver.cpp:228] Iteration 360, loss = 0.0623775
I0625 13:43:43.834058 29029 solver.cpp:244]     Train net output #0: accuracy = 0.966715
I0625 13:43:43.834069 29029 solver.cpp:244]     Train net output #1: loss = 0.0623775 (* 1 = 0.0623775 loss)
I0625 13:43:43.834074 29029 sgd_solver.cpp:106] Iteration 360, lr = 0.01
I0625 13:44:03.170240 29029 solver.cpp:228] Iteration 380, loss = 0.0510303
I0625 13:44:03.170266 29029 solver.cpp:244]     Train net output #0: accuracy = 0.970678
I0625 13:44:03.170274 29029 solver.cpp:244]     Train net output #1: loss = 0.0510303 (* 1 = 0.0510303 loss)
I0625 13:44:03.170279 29029 sgd_solver.cpp:106] Iteration 380, lr = 0.01
I0625 13:44:21.958711 29029 solver.cpp:337] Iteration 400, Testing net (#0)
I0625 13:44:22.369086 29029 solver.cpp:404]     Test net output #0: accuracy = 0.969127
I0625 13:44:22.369110 29029 solver.cpp:404]     Test net output #1: loss = 0.0494814 (* 1 = 0.0494814 loss)
I0625 13:44:22.918215 29029 solver.cpp:228] Iteration 400, loss = 0.0508445
I0625 13:44:22.918239 29029 solver.cpp:244]     Train net output #0: accuracy = 0.967873
I0625 13:44:22.918247 29029 solver.cpp:244]     Train net output #1: loss = 0.0508445 (* 1 = 0.0508445 loss)
I0625 13:44:22.918252 29029 sgd_solver.cpp:106] Iteration 400, lr = 0.01
I0625 13:44:42.296702 29029 solver.cpp:228] Iteration 420, loss = 0.0439214
I0625 13:44:42.296728 29029 solver.cpp:244]     Train net output #0: accuracy = 0.983672
I0625 13:44:42.296736 29029 solver.cpp:244]     Train net output #1: loss = 0.0439214 (* 1 = 0.0439214 loss)
I0625 13:44:42.296741 29029 sgd_solver.cpp:106] Iteration 420, lr = 0.01
I0625 13:45:01.636457 29029 solver.cpp:228] Iteration 440, loss = 0.0565041
I0625 13:45:01.636564 29029 solver.cpp:244]     Train net output #0: accuracy = 0.977911
I0625 13:45:01.636574 29029 solver.cpp:244]     Train net output #1: loss = 0.0565041 (* 1 = 0.0565041 loss)
I0625 13:45:01.636579 29029 sgd_solver.cpp:106] Iteration 440, lr = 0.01
I0625 13:45:20.977670 29029 solver.cpp:228] Iteration 460, loss = 0.049771
I0625 13:45:20.977695 29029 solver.cpp:244]     Train net output #0: accuracy = 0.979842
I0625 13:45:20.977704 29029 solver.cpp:244]     Train net output #1: loss = 0.049771 (* 1 = 0.049771 loss)
I0625 13:45:20.977708 29029 sgd_solver.cpp:106] Iteration 460, lr = 0.01
I0625 13:45:40.316179 29029 solver.cpp:228] Iteration 480, loss = 0.0483106
I0625 13:45:40.316269 29029 solver.cpp:244]     Train net output #0: accuracy = 0.981748
I0625 13:45:40.316279 29029 solver.cpp:244]     Train net output #1: loss = 0.0483106 (* 1 = 0.0483106 loss)
I0625 13:45:40.316284 29029 sgd_solver.cpp:106] Iteration 480, lr = 0.01
I0625 13:45:59.108862 29029 solver.cpp:454] Snapshotting to binary proto file data/models/segnet_iter_500.caffemodel
I0625 13:45:59.171546 29029 sgd_solver.cpp:273] Snapshotting solver state to binary proto file data/models/segnet_iter_500.solverstate
I0625 13:45:59.194581 29029 solver.cpp:337] Iteration 500, Testing net (#0)
I0625 13:45:59.607795 29029 solver.cpp:404]     Test net output #0: accuracy = 0.98314
I0625 13:45:59.607818 29029 solver.cpp:404]     Test net output #1: loss = 0.0459597 (* 1 = 0.0459597 loss)
I0625 13:46:00.160560 29029 solver.cpp:228] Iteration 500, loss = 0.0499743
I0625 13:46:00.160584 29029 solver.cpp:244]     Train net output #0: accuracy = 0.979133
I0625 13:46:00.160593 29029 solver.cpp:244]     Train net output #1: loss = 0.0499743 (* 1 = 0.0499743 loss)
I0625 13:46:00.160598 29029 sgd_solver.cpp:106] Iteration 500, lr = 0.01
I0625 13:46:19.530869 29029 solver.cpp:228] Iteration 520, loss = 0.0517816
I0625 13:46:19.530968 29029 solver.cpp:244]     Train net output #0: accuracy = 0.980528
I0625 13:46:19.530978 29029 solver.cpp:244]     Train net output #1: loss = 0.0517816 (* 1 = 0.0517816 loss)
I0625 13:46:19.530983 29029 sgd_solver.cpp:106] Iteration 520, lr = 0.01
I0625 13:46:38.983870 29029 solver.cpp:228] Iteration 540, loss = 0.0405151
I0625 13:46:38.983906 29029 solver.cpp:244]     Train net output #0: accuracy = 0.986574
I0625 13:46:38.983914 29029 solver.cpp:244]     Train net output #1: loss = 0.0405151 (* 1 = 0.0405151 loss)
I0625 13:46:38.983919 29029 sgd_solver.cpp:106] Iteration 540, lr = 0.01
I0625 13:46:58.345018 29029 solver.cpp:228] Iteration 560, loss = 0.0496999
I0625 13:46:58.345124 29029 solver.cpp:244]     Train net output #0: accuracy = 0.981641
I0625 13:46:58.345134 29029 solver.cpp:244]     Train net output #1: loss = 0.0496999 (* 1 = 0.0496999 loss)
I0625 13:46:58.345139 29029 sgd_solver.cpp:106] Iteration 560, lr = 0.01
I0625 13:47:17.715026 29029 solver.cpp:228] Iteration 580, loss = 0.0486297
I0625 13:47:17.715052 29029 solver.cpp:244]     Train net output #0: accuracy = 0.981324
I0625 13:47:17.715060 29029 solver.cpp:244]     Train net output #1: loss = 0.0486297 (* 1 = 0.0486297 loss)
I0625 13:47:17.715065 29029 sgd_solver.cpp:106] Iteration 580, lr = 0.01
I0625 13:47:36.540452 29029 solver.cpp:337] Iteration 600, Testing net (#0)
I0625 13:47:36.951270 29029 solver.cpp:404]     Test net output #0: accuracy = 0.980992
I0625 13:47:36.951294 29029 solver.cpp:404]     Test net output #1: loss = 0.0540156 (* 1 = 0.0540156 loss)
I0625 13:47:37.502800 29029 solver.cpp:228] Iteration 600, loss = 0.046772
I0625 13:47:37.502833 29029 solver.cpp:244]     Train net output #0: accuracy = 0.982505
I0625 13:47:37.502841 29029 solver.cpp:244]     Train net output #1: loss = 0.046772 (* 1 = 0.046772 loss)
I0625 13:47:37.502846 29029 sgd_solver.cpp:106] Iteration 600, lr = 0.01
I0625 13:47:56.876714 29029 solver.cpp:228] Iteration 620, loss = 0.0521577
I0625 13:47:56.876739 29029 solver.cpp:244]     Train net output #0: accuracy = 0.978998
I0625 13:47:56.876746 29029 solver.cpp:244]     Train net output #1: loss = 0.0521577 (* 1 = 0.0521577 loss)
I0625 13:47:56.876751 29029 sgd_solver.cpp:106] Iteration 620, lr = 0.01
I0625 13:48:16.246584 29029 solver.cpp:228] Iteration 640, loss = 0.0507194
I0625 13:48:16.246676 29029 solver.cpp:244]     Train net output #0: accuracy = 0.981088
I0625 13:48:16.246686 29029 solver.cpp:244]     Train net output #1: loss = 0.0507194 (* 1 = 0.0507194 loss)
I0625 13:48:16.246692 29029 sgd_solver.cpp:106] Iteration 640, lr = 0.01
I0625 13:48:35.664444 29029 solver.cpp:228] Iteration 660, loss = 0.0438943
I0625 13:48:35.664479 29029 solver.cpp:244]     Train net output #0: accuracy = 0.983792
I0625 13:48:35.664487 29029 solver.cpp:244]     Train net output #1: loss = 0.0438943 (* 1 = 0.0438943 loss)
I0625 13:48:35.664492 29029 sgd_solver.cpp:106] Iteration 660, lr = 0.01
I0625 13:48:55.037911 29029 solver.cpp:228] Iteration 680, loss = 0.0414905
I0625 13:48:55.038012 29029 solver.cpp:244]     Train net output #0: accuracy = 0.984628
I0625 13:48:55.038020 29029 solver.cpp:244]     Train net output #1: loss = 0.0414905 (* 1 = 0.0414905 loss)
I0625 13:48:55.038025 29029 sgd_solver.cpp:106] Iteration 680, lr = 0.01
I0625 13:49:13.862318 29029 solver.cpp:337] Iteration 700, Testing net (#0)
I0625 13:49:14.273931 29029 solver.cpp:404]     Test net output #0: accuracy = 0.979654
I0625 13:49:14.273964 29029 solver.cpp:404]     Test net output #1: loss = 0.0522253 (* 1 = 0.0522253 loss)
I0625 13:49:14.822989 29029 solver.cpp:228] Iteration 700, loss = 0.0442415
I0625 13:49:14.823014 29029 solver.cpp:244]     Train net output #0: accuracy = 0.9841
I0625 13:49:14.823022 29029 solver.cpp:244]     Train net output #1: loss = 0.0442415 (* 1 = 0.0442415 loss)
I0625 13:49:14.823026 29029 sgd_solver.cpp:106] Iteration 700, lr = 0.01
I0625 13:49:34.187726 29029 solver.cpp:228] Iteration 720, loss = 0.0400213
I0625 13:49:34.187826 29029 solver.cpp:244]     Train net output #0: accuracy = 0.984539
I0625 13:49:34.187835 29029 solver.cpp:244]     Train net output #1: loss = 0.0400213 (* 1 = 0.0400213 loss)
I0625 13:49:34.187841 29029 sgd_solver.cpp:106] Iteration 720, lr = 0.01
I0625 13:49:53.551177 29029 solver.cpp:228] Iteration 740, loss = 0.0420714
I0625 13:49:53.551203 29029 solver.cpp:244]     Train net output #0: accuracy = 0.984295
I0625 13:49:53.551209 29029 solver.cpp:244]     Train net output #1: loss = 0.0420714 (* 1 = 0.0420714 loss)
I0625 13:49:53.551214 29029 sgd_solver.cpp:106] Iteration 740, lr = 0.01
I0625 13:50:12.913893 29029 solver.cpp:228] Iteration 760, loss = 0.0371714
I0625 13:50:12.914023 29029 solver.cpp:244]     Train net output #0: accuracy = 0.986503
I0625 13:50:12.914033 29029 solver.cpp:244]     Train net output #1: loss = 0.0371714 (* 1 = 0.0371714 loss)
I0625 13:50:12.914038 29029 sgd_solver.cpp:106] Iteration 760, lr = 0.01
I0625 13:50:32.330845 29029 solver.cpp:228] Iteration 780, loss = 0.0408129
I0625 13:50:32.330880 29029 solver.cpp:244]     Train net output #0: accuracy = 0.984779
I0625 13:50:32.330888 29029 solver.cpp:244]     Train net output #1: loss = 0.0408129 (* 1 = 0.0408129 loss)
I0625 13:50:32.330893 29029 sgd_solver.cpp:106] Iteration 780, lr = 0.01
I0625 13:50:51.167515 29029 solver.cpp:337] Iteration 800, Testing net (#0)
I0625 13:50:51.577699 29029 solver.cpp:404]     Test net output #0: accuracy = 0.9811
I0625 13:50:51.577734 29029 solver.cpp:404]     Test net output #1: loss = 0.0556239 (* 1 = 0.0556239 loss)
I0625 13:50:52.130074 29029 solver.cpp:228] Iteration 800, loss = 0.0397311
I0625 13:50:52.130097 29029 solver.cpp:244]     Train net output #0: accuracy = 0.984533
I0625 13:50:52.130105 29029 solver.cpp:244]     Train net output #1: loss = 0.0397311 (* 1 = 0.0397311 loss)
I0625 13:50:52.130110 29029 sgd_solver.cpp:106] Iteration 800, lr = 0.01
I0625 13:51:11.498214 29029 solver.cpp:228] Iteration 820, loss = 0.0396903
I0625 13:51:11.498237 29029 solver.cpp:244]     Train net output #0: accuracy = 0.985367
I0625 13:51:11.498245 29029 solver.cpp:244]     Train net output #1: loss = 0.0396903 (* 1 = 0.0396903 loss)
I0625 13:51:11.498250 29029 sgd_solver.cpp:106] Iteration 820, lr = 0.01
I0625 13:51:30.868741 29029 solver.cpp:228] Iteration 840, loss = 0.0371544
I0625 13:51:30.868837 29029 solver.cpp:244]     Train net output #0: accuracy = 0.986894
I0625 13:51:30.868847 29029 solver.cpp:244]     Train net output #1: loss = 0.0371544 (* 1 = 0.0371544 loss)
I0625 13:51:30.868852 29029 sgd_solver.cpp:106] Iteration 840, lr = 0.01
I0625 13:51:50.238625 29029 solver.cpp:228] Iteration 860, loss = 0.0416014
I0625 13:51:50.238649 29029 solver.cpp:244]     Train net output #0: accuracy = 0.984375
I0625 13:51:50.238657 29029 solver.cpp:244]     Train net output #1: loss = 0.0416014 (* 1 = 0.0416014 loss)
I0625 13:51:50.238662 29029 sgd_solver.cpp:106] Iteration 860, lr = 0.01
I0625 13:52:09.611004 29029 solver.cpp:228] Iteration 880, loss = 0.0409915
I0625 13:52:09.611107 29029 solver.cpp:244]     Train net output #0: accuracy = 0.983812
I0625 13:52:09.611117 29029 solver.cpp:244]     Train net output #1: loss = 0.0409915 (* 1 = 0.0409915 loss)
I0625 13:52:09.611122 29029 sgd_solver.cpp:106] Iteration 880, lr = 0.01
I0625 13:52:28.440322 29029 solver.cpp:337] Iteration 900, Testing net (#0)
I0625 13:52:28.851536 29029 solver.cpp:404]     Test net output #0: accuracy = 0.981975
I0625 13:52:28.851570 29029 solver.cpp:404]     Test net output #1: loss = 0.0510095 (* 1 = 0.0510095 loss)
I0625 13:52:29.401518 29029 solver.cpp:228] Iteration 900, loss = 0.0351226
I0625 13:52:29.401542 29029 solver.cpp:244]     Train net output #0: accuracy = 0.987319
I0625 13:52:29.401551 29029 solver.cpp:244]     Train net output #1: loss = 0.0351226 (* 1 = 0.0351226 loss)
I0625 13:52:29.401556 29029 sgd_solver.cpp:106] Iteration 900, lr = 0.01
I0625 13:52:48.835163 29029 solver.cpp:228] Iteration 920, loss = 0.0365374
I0625 13:52:48.835259 29029 solver.cpp:244]     Train net output #0: accuracy = 0.98568
I0625 13:52:48.835270 29029 solver.cpp:244]     Train net output #1: loss = 0.0365374 (* 1 = 0.0365374 loss)
I0625 13:52:48.835275 29029 sgd_solver.cpp:106] Iteration 920, lr = 0.01
I0625 13:53:08.205255 29029 solver.cpp:228] Iteration 940, loss = 0.0340107
I0625 13:53:08.205281 29029 solver.cpp:244]     Train net output #0: accuracy = 0.98795
I0625 13:53:08.205287 29029 solver.cpp:244]     Train net output #1: loss = 0.0340107 (* 1 = 0.0340107 loss)
I0625 13:53:08.205292 29029 sgd_solver.cpp:106] Iteration 940, lr = 0.01
I0625 13:53:27.579490 29029 solver.cpp:228] Iteration 960, loss = 0.036539
I0625 13:53:27.579596 29029 solver.cpp:244]     Train net output #0: accuracy = 0.986182
I0625 13:53:27.579605 29029 solver.cpp:244]     Train net output #1: loss = 0.036539 (* 1 = 0.036539 loss)
I0625 13:53:27.579610 29029 sgd_solver.cpp:106] Iteration 960, lr = 0.01
I0625 13:53:46.954308 29029 solver.cpp:228] Iteration 980, loss = 0.0333569
I0625 13:53:46.954339 29029 solver.cpp:244]     Train net output #0: accuracy = 0.987963
I0625 13:53:46.954358 29029 solver.cpp:244]     Train net output #1: loss = 0.0333569 (* 1 = 0.0333569 loss)
I0625 13:53:46.954363 29029 sgd_solver.cpp:106] Iteration 980, lr = 0.01
I0625 13:54:05.774657 29029 solver.cpp:454] Snapshotting to binary proto file data/models/segnet_iter_1000.caffemodel
I0625 13:54:05.821521 29029 sgd_solver.cpp:273] Snapshotting solver state to binary proto file data/models/segnet_iter_1000.solverstate
I0625 13:54:05.844161 29029 solver.cpp:337] Iteration 1000, Testing net (#0)
I0625 13:54:06.257387 29029 solver.cpp:404]     Test net output #0: accuracy = 0.984085
I0625 13:54:06.257422 29029 solver.cpp:404]     Test net output #1: loss = 0.0459141 (* 1 = 0.0459141 loss)
I0625 13:54:06.806957 29029 solver.cpp:228] Iteration 1000, loss = 0.0384089
I0625 13:54:06.806982 29029 solver.cpp:244]     Train net output #0: accuracy = 0.985098
I0625 13:54:06.806990 29029 solver.cpp:244]     Train net output #1: loss = 0.0384089 (* 1 = 0.0384089 loss)
I0625 13:54:06.806995 29029 sgd_solver.cpp:106] Iteration 1000, lr = 0.01
I0625 13:54:26.184881 29029 solver.cpp:228] Iteration 1020, loss = 0.0403054
I0625 13:54:26.184907 29029 solver.cpp:244]     Train net output #0: accuracy = 0.984648
I0625 13:54:26.184916 29029 solver.cpp:244]     Train net output #1: loss = 0.0403054 (* 1 = 0.0403054 loss)
I0625 13:54:26.184921 29029 sgd_solver.cpp:106] Iteration 1020, lr = 0.01
I0625 13:54:45.618451 29029 solver.cpp:228] Iteration 1040, loss = 0.0362113
I0625 13:54:45.618542 29029 solver.cpp:244]     Train net output #0: accuracy = 0.986381
I0625 13:54:45.618552 29029 solver.cpp:244]     Train net output #1: loss = 0.0362113 (* 1 = 0.0362113 loss)
I0625 13:54:45.618557 29029 sgd_solver.cpp:106] Iteration 1040, lr = 0.01
I0625 13:55:04.986932 29029 solver.cpp:228] Iteration 1060, loss = 0.0382486
I0625 13:55:04.986958 29029 solver.cpp:244]     Train net output #0: accuracy = 0.985672
I0625 13:55:04.986966 29029 solver.cpp:244]     Train net output #1: loss = 0.0382486 (* 1 = 0.0382486 loss)
I0625 13:55:04.986971 29029 sgd_solver.cpp:106] Iteration 1060, lr = 0.01
I0625 13:55:24.364361 29029 solver.cpp:228] Iteration 1080, loss = 0.0367497
I0625 13:55:24.364464 29029 solver.cpp:244]     Train net output #0: accuracy = 0.986948
I0625 13:55:24.364472 29029 solver.cpp:244]     Train net output #1: loss = 0.0367497 (* 1 = 0.0367497 loss)
I0625 13:55:24.364477 29029 sgd_solver.cpp:106] Iteration 1080, lr = 0.01
I0625 13:55:43.191992 29029 solver.cpp:337] Iteration 1100, Testing net (#0)
I0625 13:55:43.603210 29029 solver.cpp:404]     Test net output #0: accuracy = 0.980659
I0625 13:55:43.603235 29029 solver.cpp:404]     Test net output #1: loss = 0.0488474 (* 1 = 0.0488474 loss)
I0625 13:55:44.156363 29029 solver.cpp:228] Iteration 1100, loss = 0.0359676
I0625 13:55:44.156389 29029 solver.cpp:244]     Train net output #0: accuracy = 0.985883
I0625 13:55:44.156396 29029 solver.cpp:244]     Train net output #1: loss = 0.0359676 (* 1 = 0.0359676 loss)
I0625 13:55:44.156402 29029 sgd_solver.cpp:106] Iteration 1100, lr = 0.01
I0625 13:56:03.523618 29029 solver.cpp:228] Iteration 1120, loss = 0.0384512
I0625 13:56:03.523717 29029 solver.cpp:244]     Train net output #0: accuracy = 0.985897
I0625 13:56:03.523727 29029 solver.cpp:244]     Train net output #1: loss = 0.0384512 (* 1 = 0.0384512 loss)
I0625 13:56:03.523732 29029 sgd_solver.cpp:106] Iteration 1120, lr = 0.01
I0625 13:56:22.890103 29029 solver.cpp:228] Iteration 1140, loss = 0.0317472
I0625 13:56:22.890127 29029 solver.cpp:244]     Train net output #0: accuracy = 0.988022
I0625 13:56:22.890136 29029 solver.cpp:244]     Train net output #1: loss = 0.0317472 (* 1 = 0.0317472 loss)
I0625 13:56:22.890141 29029 sgd_solver.cpp:106] Iteration 1140, lr = 0.01
I0625 13:56:42.276367 29029 solver.cpp:228] Iteration 1160, loss = 0.0341732
I0625 13:56:42.276494 29029 solver.cpp:244]     Train net output #0: accuracy = 0.986944
I0625 13:56:42.276505 29029 solver.cpp:244]     Train net output #1: loss = 0.0341732 (* 1 = 0.0341732 loss)
I0625 13:56:42.276510 29029 sgd_solver.cpp:106] Iteration 1160, lr = 0.01
I0625 13:57:01.656831 29029 solver.cpp:228] Iteration 1180, loss = 0.0342589
I0625 13:57:01.656855 29029 solver.cpp:244]     Train net output #0: accuracy = 0.987442
I0625 13:57:01.656863 29029 solver.cpp:244]     Train net output #1: loss = 0.0342589 (* 1 = 0.0342589 loss)
I0625 13:57:01.656868 29029 sgd_solver.cpp:106] Iteration 1180, lr = 0.01
I0625 13:57:20.481053 29029 solver.cpp:337] Iteration 1200, Testing net (#0)
I0625 13:57:20.891788 29029 solver.cpp:404]     Test net output #0: accuracy = 0.983866
I0625 13:57:20.891811 29029 solver.cpp:404]     Test net output #1: loss = 0.0422594 (* 1 = 0.0422594 loss)
I0625 13:57:21.442898 29029 solver.cpp:228] Iteration 1200, loss = 0.0376127
I0625 13:57:21.442924 29029 solver.cpp:244]     Train net output #0: accuracy = 0.985281
I0625 13:57:21.442931 29029 solver.cpp:244]     Train net output #1: loss = 0.0376127 (* 1 = 0.0376127 loss)
I0625 13:57:21.442936 29029 sgd_solver.cpp:106] Iteration 1200, lr = 0.01
I0625 13:57:40.816886 29029 solver.cpp:228] Iteration 1220, loss = 0.0349788
I0625 13:57:40.816910 29029 solver.cpp:244]     Train net output #0: accuracy = 0.986947
I0625 13:57:40.816920 29029 solver.cpp:244]     Train net output #1: loss = 0.0349788 (* 1 = 0.0349788 loss)
I0625 13:57:40.816923 29029 sgd_solver.cpp:106] Iteration 1220, lr = 0.01
I0625 13:58:00.196077 29029 solver.cpp:228] Iteration 1240, loss = 0.0339191
I0625 13:58:00.196182 29029 solver.cpp:244]     Train net output #0: accuracy = 0.986871
I0625 13:58:00.196192 29029 solver.cpp:244]     Train net output #1: loss = 0.0339191 (* 1 = 0.0339191 loss)
I0625 13:58:00.196197 29029 sgd_solver.cpp:106] Iteration 1240, lr = 0.01
I0625 13:58:19.561241 29029 solver.cpp:228] Iteration 1260, loss = 0.0322803
I0625 13:58:19.561267 29029 solver.cpp:244]     Train net output #0: accuracy = 0.986936
I0625 13:58:19.561275 29029 solver.cpp:244]     Train net output #1: loss = 0.0322803 (* 1 = 0.0322803 loss)
I0625 13:58:19.561280 29029 sgd_solver.cpp:106] Iteration 1260, lr = 0.01
I0625 13:58:38.981475 29029 solver.cpp:228] Iteration 1280, loss = 0.0376285
I0625 13:58:38.981580 29029 solver.cpp:244]     Train net output #0: accuracy = 0.985218
I0625 13:58:38.981590 29029 solver.cpp:244]     Train net output #1: loss = 0.0376285 (* 1 = 0.0376285 loss)
I0625 13:58:38.981595 29029 sgd_solver.cpp:106] Iteration 1280, lr = 0.01
I0625 13:58:57.807592 29029 solver.cpp:337] Iteration 1300, Testing net (#0)
I0625 13:58:58.218471 29029 solver.cpp:404]     Test net output #0: accuracy = 0.98315
I0625 13:58:58.218507 29029 solver.cpp:404]     Test net output #1: loss = 0.0461404 (* 1 = 0.0461404 loss)
I0625 13:58:58.767829 29029 solver.cpp:228] Iteration 1300, loss = 0.0303919
I0625 13:58:58.767854 29029 solver.cpp:244]     Train net output #0: accuracy = 0.988701
I0625 13:58:58.767863 29029 solver.cpp:244]     Train net output #1: loss = 0.0303919 (* 1 = 0.0303919 loss)
I0625 13:58:58.767868 29029 sgd_solver.cpp:106] Iteration 1300, lr = 0.01
I0625 13:59:18.140242 29029 solver.cpp:228] Iteration 1320, loss = 0.0322311
I0625 13:59:18.140346 29029 solver.cpp:244]     Train net output #0: accuracy = 0.98792
I0625 13:59:18.140354 29029 solver.cpp:244]     Train net output #1: loss = 0.0322311 (* 1 = 0.0322311 loss)
I0625 13:59:18.140359 29029 sgd_solver.cpp:106] Iteration 1320, lr = 0.01
I0625 13:59:37.510514 29029 solver.cpp:228] Iteration 1340, loss = 0.0353368
I0625 13:59:37.510550 29029 solver.cpp:244]     Train net output #0: accuracy = 0.98906
I0625 13:59:37.510558 29029 solver.cpp:244]     Train net output #1: loss = 0.0353368 (* 1 = 0.0353368 loss)
I0625 13:59:37.510563 29029 sgd_solver.cpp:106] Iteration 1340, lr = 0.01
I0625 13:59:56.878469 29029 solver.cpp:228] Iteration 1360, loss = 0.0355727
I0625 13:59:56.878583 29029 solver.cpp:244]     Train net output #0: accuracy = 0.986376
I0625 13:59:56.878593 29029 solver.cpp:244]     Train net output #1: loss = 0.0355727 (* 1 = 0.0355727 loss)
I0625 13:59:56.878598 29029 sgd_solver.cpp:106] Iteration 1360, lr = 0.01
I0625 14:00:16.254257 29029 solver.cpp:228] Iteration 1380, loss = 0.031735
I0625 14:00:16.254282 29029 solver.cpp:244]     Train net output #0: accuracy = 0.988618
I0625 14:00:16.254290 29029 solver.cpp:244]     Train net output #1: loss = 0.031735 (* 1 = 0.031735 loss)
I0625 14:00:16.254295 29029 sgd_solver.cpp:106] Iteration 1380, lr = 0.01
I0625 14:00:35.131876 29029 solver.cpp:337] Iteration 1400, Testing net (#0)
I0625 14:00:35.543218 29029 solver.cpp:404]     Test net output #0: accuracy = 0.986293
I0625 14:00:35.543243 29029 solver.cpp:404]     Test net output #1: loss = 0.0334698 (* 1 = 0.0334698 loss)
I0625 14:00:36.096174 29029 solver.cpp:228] Iteration 1400, loss = 0.029971
I0625 14:00:36.096200 29029 solver.cpp:244]     Train net output #0: accuracy = 0.989042
I0625 14:00:36.096209 29029 solver.cpp:244]     Train net output #1: loss = 0.029971 (* 1 = 0.029971 loss)
I0625 14:00:36.096213 29029 sgd_solver.cpp:106] Iteration 1400, lr = 0.01
I0625 14:00:55.464308 29029 solver.cpp:228] Iteration 1420, loss = 0.0305532
I0625 14:00:55.464334 29029 solver.cpp:244]     Train net output #0: accuracy = 0.989262
I0625 14:00:55.464341 29029 solver.cpp:244]     Train net output #1: loss = 0.0305532 (* 1 = 0.0305532 loss)
I0625 14:00:55.464345 29029 sgd_solver.cpp:106] Iteration 1420, lr = 0.01
I0625 14:01:14.832388 29029 solver.cpp:228] Iteration 1440, loss = 0.0343007
I0625 14:01:14.832495 29029 solver.cpp:244]     Train net output #0: accuracy = 0.986773
I0625 14:01:14.832505 29029 solver.cpp:244]     Train net output #1: loss = 0.0343007 (* 1 = 0.0343007 loss)
I0625 14:01:14.832510 29029 sgd_solver.cpp:106] Iteration 1440, lr = 0.01
I0625 14:01:34.207095 29029 solver.cpp:228] Iteration 1460, loss = 0.0279545
I0625 14:01:34.207121 29029 solver.cpp:244]     Train net output #0: accuracy = 0.989436
I0625 14:01:34.207129 29029 solver.cpp:244]     Train net output #1: loss = 0.0279545 (* 1 = 0.0279545 loss)
I0625 14:01:34.207134 29029 sgd_solver.cpp:106] Iteration 1460, lr = 0.01
I0625 14:01:53.571059 29029 solver.cpp:228] Iteration 1480, loss = 0.0293445
I0625 14:01:53.571152 29029 solver.cpp:244]     Train net output #0: accuracy = 0.988444
I0625 14:01:53.571162 29029 solver.cpp:244]     Train net output #1: loss = 0.0293445 (* 1 = 0.0293445 loss)
I0625 14:01:53.571167 29029 sgd_solver.cpp:106] Iteration 1480, lr = 0.01
I0625 14:02:12.387164 29029 solver.cpp:454] Snapshotting to binary proto file data/models/segnet_iter_1500.caffemodel
I0625 14:02:12.434233 29029 sgd_solver.cpp:273] Snapshotting solver state to binary proto file data/models/segnet_iter_1500.solverstate
I0625 14:02:12.457221 29029 solver.cpp:337] Iteration 1500, Testing net (#0)
I0625 14:02:12.870141 29029 solver.cpp:404]     Test net output #0: accuracy = 0.981151
I0625 14:02:12.870177 29029 solver.cpp:404]     Test net output #1: loss = 0.0487923 (* 1 = 0.0487923 loss)
I0625 14:02:13.426528 29029 solver.cpp:228] Iteration 1500, loss = 0.029801
I0625 14:02:13.426553 29029 solver.cpp:244]     Train net output #0: accuracy = 0.988894
I0625 14:02:13.426561 29029 solver.cpp:244]     Train net output #1: loss = 0.029801 (* 1 = 0.029801 loss)
I0625 14:02:13.426565 29029 sgd_solver.cpp:106] Iteration 1500, lr = 0.01
I0625 14:02:32.818852 29029 solver.cpp:228] Iteration 1520, loss = 0.0312436
I0625 14:02:32.818960 29029 solver.cpp:244]     Train net output #0: accuracy = 0.987739
I0625 14:02:32.818970 29029 solver.cpp:244]     Train net output #1: loss = 0.0312436 (* 1 = 0.0312436 loss)
I0625 14:02:32.818976 29029 sgd_solver.cpp:106] Iteration 1520, lr = 0.01
I0625 14:02:52.202637 29029 solver.cpp:228] Iteration 1540, loss = 0.0287377
I0625 14:02:52.202663 29029 solver.cpp:244]     Train net output #0: accuracy = 0.98933
I0625 14:02:52.202682 29029 solver.cpp:244]     Train net output #1: loss = 0.0287377 (* 1 = 0.0287377 loss)
I0625 14:02:52.202687 29029 sgd_solver.cpp:106] Iteration 1540, lr = 0.01
I0625 14:03:11.568572 29029 solver.cpp:228] Iteration 1560, loss = 0.0292915
I0625 14:03:11.568693 29029 solver.cpp:244]     Train net output #0: accuracy = 0.988596
I0625 14:03:11.568703 29029 solver.cpp:244]     Train net output #1: loss = 0.0292915 (* 1 = 0.0292915 loss)
I0625 14:03:11.568708 29029 sgd_solver.cpp:106] Iteration 1560, lr = 0.01
I0625 14:03:30.930655 29029 solver.cpp:228] Iteration 1580, loss = 0.027467
I0625 14:03:30.930680 29029 solver.cpp:244]     Train net output #0: accuracy = 0.98936
I0625 14:03:30.930688 29029 solver.cpp:244]     Train net output #1: loss = 0.027467 (* 1 = 0.027467 loss)
I0625 14:03:30.930692 29029 sgd_solver.cpp:106] Iteration 1580, lr = 0.01
I0625 14:03:49.753059 29029 solver.cpp:337] Iteration 1600, Testing net (#0)
I0625 14:03:50.164125 29029 solver.cpp:404]     Test net output #0: accuracy = 0.984853
I0625 14:03:50.164149 29029 solver.cpp:404]     Test net output #1: loss = 0.0468121 (* 1 = 0.0468121 loss)
I0625 14:03:50.713917 29029 solver.cpp:228] Iteration 1600, loss = 0.0281684
I0625 14:03:50.713943 29029 solver.cpp:244]     Train net output #0: accuracy = 0.989208
I0625 14:03:50.713950 29029 solver.cpp:244]     Train net output #1: loss = 0.0281684 (* 1 = 0.0281684 loss)
I0625 14:03:50.713955 29029 sgd_solver.cpp:106] Iteration 1600, lr = 0.01
I0625 14:04:10.088218 29029 solver.cpp:228] Iteration 1620, loss = 0.0297613
I0625 14:04:10.088243 29029 solver.cpp:244]     Train net output #0: accuracy = 0.988873
I0625 14:04:10.088250 29029 solver.cpp:244]     Train net output #1: loss = 0.0297613 (* 1 = 0.0297613 loss)
I0625 14:04:10.088255 29029 sgd_solver.cpp:106] Iteration 1620, lr = 0.01
I0625 14:04:29.460206 29029 solver.cpp:228] Iteration 1640, loss = 0.0272679
I0625 14:04:29.460299 29029 solver.cpp:244]     Train net output #0: accuracy = 0.989307
I0625 14:04:29.460307 29029 solver.cpp:244]     Train net output #1: loss = 0.0272679 (* 1 = 0.0272679 loss)
I0625 14:04:29.460312 29029 sgd_solver.cpp:106] Iteration 1640, lr = 0.01
I0625 14:04:48.917472 29029 solver.cpp:228] Iteration 1660, loss = 0.028619
I0625 14:04:48.917498 29029 solver.cpp:244]     Train net output #0: accuracy = 0.989248
I0625 14:04:48.917506 29029 solver.cpp:244]     Train net output #1: loss = 0.028619 (* 1 = 0.028619 loss)
I0625 14:04:48.917511 29029 sgd_solver.cpp:106] Iteration 1660, lr = 0.01
I0625 14:05:08.285054 29029 solver.cpp:228] Iteration 1680, loss = 0.0267578
I0625 14:05:08.285151 29029 solver.cpp:244]     Train net output #0: accuracy = 0.989662
I0625 14:05:08.285161 29029 solver.cpp:244]     Train net output #1: loss = 0.0267578 (* 1 = 0.0267578 loss)
I0625 14:05:08.285166 29029 sgd_solver.cpp:106] Iteration 1680, lr = 0.01
I0625 14:05:27.103814 29029 solver.cpp:337] Iteration 1700, Testing net (#0)
I0625 14:05:27.514576 29029 solver.cpp:404]     Test net output #0: accuracy = 0.98504
I0625 14:05:27.514600 29029 solver.cpp:404]     Test net output #1: loss = 0.0408453 (* 1 = 0.0408453 loss)
I0625 14:05:28.065486 29029 solver.cpp:228] Iteration 1700, loss = 0.0297558
I0625 14:05:28.065511 29029 solver.cpp:244]     Train net output #0: accuracy = 0.987797
I0625 14:05:28.065518 29029 solver.cpp:244]     Train net output #1: loss = 0.0297558 (* 1 = 0.0297558 loss)
I0625 14:05:28.065523 29029 sgd_solver.cpp:106] Iteration 1700, lr = 0.01
I0625 14:05:47.444078 29029 solver.cpp:228] Iteration 1720, loss = 0.027945
I0625 14:05:47.444172 29029 solver.cpp:244]     Train net output #0: accuracy = 0.989652
I0625 14:05:47.444182 29029 solver.cpp:244]     Train net output #1: loss = 0.027945 (* 1 = 0.027945 loss)
I0625 14:05:47.444187 29029 sgd_solver.cpp:106] Iteration 1720, lr = 0.01
I0625 14:06:06.810601 29029 solver.cpp:228] Iteration 1740, loss = 0.027011
I0625 14:06:06.810626 29029 solver.cpp:244]     Train net output #0: accuracy = 0.990136
I0625 14:06:06.810634 29029 solver.cpp:244]     Train net output #1: loss = 0.027011 (* 1 = 0.027011 loss)
I0625 14:06:06.810639 29029 sgd_solver.cpp:106] Iteration 1740, lr = 0.01
I0625 14:06:26.178908 29029 solver.cpp:228] Iteration 1760, loss = 0.0279745
I0625 14:06:26.179031 29029 solver.cpp:244]     Train net output #0: accuracy = 0.989586
I0625 14:06:26.179041 29029 solver.cpp:244]     Train net output #1: loss = 0.0279745 (* 1 = 0.0279745 loss)
I0625 14:06:26.179046 29029 sgd_solver.cpp:106] Iteration 1760, lr = 0.01
I0625 14:06:45.598588 29029 solver.cpp:228] Iteration 1780, loss = 0.0250461
I0625 14:06:45.598614 29029 solver.cpp:244]     Train net output #0: accuracy = 0.990819
I0625 14:06:45.598623 29029 solver.cpp:244]     Train net output #1: loss = 0.0250461 (* 1 = 0.0250461 loss)
I0625 14:06:45.598628 29029 sgd_solver.cpp:106] Iteration 1780, lr = 0.01
I0625 14:07:04.417776 29029 solver.cpp:337] Iteration 1800, Testing net (#0)
I0625 14:07:04.828435 29029 solver.cpp:404]     Test net output #0: accuracy = 0.982931
I0625 14:07:04.828460 29029 solver.cpp:404]     Test net output #1: loss = 0.0538108 (* 1 = 0.0538108 loss)
I0625 14:07:05.381162 29029 solver.cpp:228] Iteration 1800, loss = 0.0284691
I0625 14:07:05.381197 29029 solver.cpp:244]     Train net output #0: accuracy = 0.988917
I0625 14:07:05.381206 29029 solver.cpp:244]     Train net output #1: loss = 0.0284691 (* 1 = 0.0284691 loss)
I0625 14:07:05.381222 29029 sgd_solver.cpp:106] Iteration 1800, lr = 0.01
I0625 14:07:24.760128 29029 solver.cpp:228] Iteration 1820, loss = 0.026787
I0625 14:07:24.760154 29029 solver.cpp:244]     Train net output #0: accuracy = 0.989987
I0625 14:07:24.760160 29029 solver.cpp:244]     Train net output #1: loss = 0.026787 (* 1 = 0.026787 loss)
I0625 14:07:24.760165 29029 sgd_solver.cpp:106] Iteration 1820, lr = 0.01
I0625 14:07:44.124415 29029 solver.cpp:228] Iteration 1840, loss = 0.0274831
I0625 14:07:44.124508 29029 solver.cpp:244]     Train net output #0: accuracy = 0.989229
I0625 14:07:44.124518 29029 solver.cpp:244]     Train net output #1: loss = 0.0274831 (* 1 = 0.0274831 loss)
I0625 14:07:44.124523 29029 sgd_solver.cpp:106] Iteration 1840, lr = 0.01
I0625 14:08:03.491899 29029 solver.cpp:228] Iteration 1860, loss = 0.0256712
I0625 14:08:03.491933 29029 solver.cpp:244]     Train net output #0: accuracy = 0.990413
I0625 14:08:03.491941 29029 solver.cpp:244]     Train net output #1: loss = 0.0256712 (* 1 = 0.0256712 loss)
I0625 14:08:03.491945 29029 sgd_solver.cpp:106] Iteration 1860, lr = 0.01
I0625 14:08:22.856765 29029 solver.cpp:228] Iteration 1880, loss = 0.0257296
I0625 14:08:22.856868 29029 solver.cpp:244]     Train net output #0: accuracy = 0.990493
I0625 14:08:22.856878 29029 solver.cpp:244]     Train net output #1: loss = 0.0257296 (* 1 = 0.0257296 loss)
I0625 14:08:22.856891 29029 sgd_solver.cpp:106] Iteration 1880, lr = 0.01
I0625 14:08:41.729567 29029 solver.cpp:337] Iteration 1900, Testing net (#0)
I0625 14:08:42.140403 29029 solver.cpp:404]     Test net output #0: accuracy = 0.98334
I0625 14:08:42.140439 29029 solver.cpp:404]     Test net output #1: loss = 0.0502502 (* 1 = 0.0502502 loss)
I0625 14:08:42.689960 29029 solver.cpp:228] Iteration 1900, loss = 0.0266582
I0625 14:08:42.689985 29029 solver.cpp:244]     Train net output #0: accuracy = 0.989541
I0625 14:08:42.689993 29029 solver.cpp:244]     Train net output #1: loss = 0.0266582 (* 1 = 0.0266582 loss)
I0625 14:08:42.689998 29029 sgd_solver.cpp:106] Iteration 1900, lr = 0.01
I0625 14:09:02.066589 29029 solver.cpp:228] Iteration 1920, loss = 0.0233857
I0625 14:09:02.066696 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991474
I0625 14:09:02.066706 29029 solver.cpp:244]     Train net output #1: loss = 0.0233857 (* 1 = 0.0233857 loss)
I0625 14:09:02.066711 29029 sgd_solver.cpp:106] Iteration 1920, lr = 0.01
I0625 14:09:21.439527 29029 solver.cpp:228] Iteration 1940, loss = 0.0270438
I0625 14:09:21.439564 29029 solver.cpp:244]     Train net output #0: accuracy = 0.989286
I0625 14:09:21.439571 29029 solver.cpp:244]     Train net output #1: loss = 0.0270438 (* 1 = 0.0270438 loss)
I0625 14:09:21.439577 29029 sgd_solver.cpp:106] Iteration 1940, lr = 0.01
I0625 14:09:40.803783 29029 solver.cpp:228] Iteration 1960, loss = 0.0253355
I0625 14:09:40.803910 29029 solver.cpp:244]     Train net output #0: accuracy = 0.990132
I0625 14:09:40.803920 29029 solver.cpp:244]     Train net output #1: loss = 0.0253355 (* 1 = 0.0253355 loss)
I0625 14:09:40.803925 29029 sgd_solver.cpp:106] Iteration 1960, lr = 0.01
I0625 14:10:00.182859 29029 solver.cpp:228] Iteration 1980, loss = 0.0253062
I0625 14:10:00.182886 29029 solver.cpp:244]     Train net output #0: accuracy = 0.990598
I0625 14:10:00.182894 29029 solver.cpp:244]     Train net output #1: loss = 0.0253062 (* 1 = 0.0253062 loss)
I0625 14:10:00.182898 29029 sgd_solver.cpp:106] Iteration 1980, lr = 0.01
I0625 14:10:19.007359 29029 solver.cpp:454] Snapshotting to binary proto file data/models/segnet_iter_2000.caffemodel
I0625 14:10:19.054065 29029 sgd_solver.cpp:273] Snapshotting solver state to binary proto file data/models/segnet_iter_2000.solverstate
I0625 14:10:19.077096 29029 solver.cpp:337] Iteration 2000, Testing net (#0)
I0625 14:10:19.490396 29029 solver.cpp:404]     Test net output #0: accuracy = 0.985648
I0625 14:10:19.490430 29029 solver.cpp:404]     Test net output #1: loss = 0.0392843 (* 1 = 0.0392843 loss)
I0625 14:10:20.040288 29029 solver.cpp:228] Iteration 2000, loss = 0.0267029
I0625 14:10:20.040313 29029 solver.cpp:244]     Train net output #0: accuracy = 0.989816
I0625 14:10:20.040321 29029 solver.cpp:244]     Train net output #1: loss = 0.0267029 (* 1 = 0.0267029 loss)
I0625 14:10:20.040325 29029 sgd_solver.cpp:106] Iteration 2000, lr = 0.001
I0625 14:10:39.455221 29029 solver.cpp:228] Iteration 2020, loss = 0.0252123
I0625 14:10:39.455245 29029 solver.cpp:244]     Train net output #0: accuracy = 0.990359
I0625 14:10:39.455252 29029 solver.cpp:244]     Train net output #1: loss = 0.0252123 (* 1 = 0.0252123 loss)
I0625 14:10:39.455257 29029 sgd_solver.cpp:106] Iteration 2020, lr = 0.001
I0625 14:10:58.826853 29029 solver.cpp:228] Iteration 2040, loss = 0.0256086
I0625 14:10:58.826953 29029 solver.cpp:244]     Train net output #0: accuracy = 0.99003
I0625 14:10:58.826963 29029 solver.cpp:244]     Train net output #1: loss = 0.0256086 (* 1 = 0.0256086 loss)
I0625 14:10:58.826969 29029 sgd_solver.cpp:106] Iteration 2040, lr = 0.001
I0625 14:11:18.200139 29029 solver.cpp:228] Iteration 2060, loss = 0.0247679
I0625 14:11:18.200175 29029 solver.cpp:244]     Train net output #0: accuracy = 0.990319
I0625 14:11:18.200182 29029 solver.cpp:244]     Train net output #1: loss = 0.0247679 (* 1 = 0.0247679 loss)
I0625 14:11:18.200186 29029 sgd_solver.cpp:106] Iteration 2060, lr = 0.001
I0625 14:11:37.574476 29029 solver.cpp:228] Iteration 2080, loss = 0.0241696
I0625 14:11:37.574581 29029 solver.cpp:244]     Train net output #0: accuracy = 0.990944
I0625 14:11:37.574590 29029 solver.cpp:244]     Train net output #1: loss = 0.0241696 (* 1 = 0.0241696 loss)
I0625 14:11:37.574595 29029 sgd_solver.cpp:106] Iteration 2080, lr = 0.001
I0625 14:11:56.398480 29029 solver.cpp:337] Iteration 2100, Testing net (#0)
I0625 14:11:56.809744 29029 solver.cpp:404]     Test net output #0: accuracy = 0.987165
I0625 14:11:56.809778 29029 solver.cpp:404]     Test net output #1: loss = 0.040597 (* 1 = 0.040597 loss)
I0625 14:11:57.361755 29029 solver.cpp:228] Iteration 2100, loss = 0.0247091
I0625 14:11:57.361793 29029 solver.cpp:244]     Train net output #0: accuracy = 0.990214
I0625 14:11:57.361800 29029 solver.cpp:244]     Train net output #1: loss = 0.0247091 (* 1 = 0.0247091 loss)
I0625 14:11:57.361805 29029 sgd_solver.cpp:106] Iteration 2100, lr = 0.001
I0625 14:12:16.729643 29029 solver.cpp:228] Iteration 2120, loss = 0.0249157
I0625 14:12:16.729751 29029 solver.cpp:244]     Train net output #0: accuracy = 0.990648
I0625 14:12:16.729761 29029 solver.cpp:244]     Train net output #1: loss = 0.0249157 (* 1 = 0.0249157 loss)
I0625 14:12:16.729766 29029 sgd_solver.cpp:106] Iteration 2120, lr = 0.001
I0625 14:12:36.209305 29029 solver.cpp:228] Iteration 2140, loss = 0.0266774
I0625 14:12:36.209341 29029 solver.cpp:244]     Train net output #0: accuracy = 0.989922
I0625 14:12:36.209347 29029 solver.cpp:244]     Train net output #1: loss = 0.0266774 (* 1 = 0.0266774 loss)
I0625 14:12:36.209352 29029 sgd_solver.cpp:106] Iteration 2140, lr = 0.001
I0625 14:12:55.576110 29029 solver.cpp:228] Iteration 2160, loss = 0.0238743
I0625 14:12:55.576231 29029 solver.cpp:244]     Train net output #0: accuracy = 0.99152
I0625 14:12:55.576242 29029 solver.cpp:244]     Train net output #1: loss = 0.0238743 (* 1 = 0.0238743 loss)
I0625 14:12:55.576246 29029 sgd_solver.cpp:106] Iteration 2160, lr = 0.001
I0625 14:13:14.954798 29029 solver.cpp:228] Iteration 2180, loss = 0.0215338
I0625 14:13:14.954834 29029 solver.cpp:244]     Train net output #0: accuracy = 0.992279
I0625 14:13:14.954843 29029 solver.cpp:244]     Train net output #1: loss = 0.0215338 (* 1 = 0.0215338 loss)
I0625 14:13:14.954848 29029 sgd_solver.cpp:106] Iteration 2180, lr = 0.001
I0625 14:13:33.776536 29029 solver.cpp:337] Iteration 2200, Testing net (#0)
I0625 14:13:34.187654 29029 solver.cpp:404]     Test net output #0: accuracy = 0.987641
I0625 14:13:34.187687 29029 solver.cpp:404]     Test net output #1: loss = 0.0364485 (* 1 = 0.0364485 loss)
I0625 14:13:34.739513 29029 solver.cpp:228] Iteration 2200, loss = 0.0232773
I0625 14:13:34.739539 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991129
I0625 14:13:34.739547 29029 solver.cpp:244]     Train net output #1: loss = 0.0232773 (* 1 = 0.0232773 loss)
I0625 14:13:34.739552 29029 sgd_solver.cpp:106] Iteration 2200, lr = 0.001
I0625 14:13:54.119614 29029 solver.cpp:228] Iteration 2220, loss = 0.0228344
I0625 14:13:54.119639 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991039
I0625 14:13:54.119647 29029 solver.cpp:244]     Train net output #1: loss = 0.0228344 (* 1 = 0.0228344 loss)
I0625 14:13:54.119652 29029 sgd_solver.cpp:106] Iteration 2220, lr = 0.001
I0625 14:14:13.492804 29029 solver.cpp:228] Iteration 2240, loss = 0.0251538
I0625 14:14:13.492903 29029 solver.cpp:244]     Train net output #0: accuracy = 0.990465
I0625 14:14:13.492913 29029 solver.cpp:244]     Train net output #1: loss = 0.0251538 (* 1 = 0.0251538 loss)
I0625 14:14:13.492918 29029 sgd_solver.cpp:106] Iteration 2240, lr = 0.001
I0625 14:14:32.912461 29029 solver.cpp:228] Iteration 2260, loss = 0.0242731
I0625 14:14:32.912487 29029 solver.cpp:244]     Train net output #0: accuracy = 0.990834
I0625 14:14:32.912504 29029 solver.cpp:244]     Train net output #1: loss = 0.0242731 (* 1 = 0.0242731 loss)
I0625 14:14:32.912509 29029 sgd_solver.cpp:106] Iteration 2260, lr = 0.001
I0625 14:14:52.327113 29029 solver.cpp:228] Iteration 2280, loss = 0.0239388
I0625 14:14:52.327211 29029 solver.cpp:244]     Train net output #0: accuracy = 0.990807
I0625 14:14:52.327221 29029 solver.cpp:244]     Train net output #1: loss = 0.0239388 (* 1 = 0.0239388 loss)
I0625 14:14:52.327226 29029 sgd_solver.cpp:106] Iteration 2280, lr = 0.001
I0625 14:15:11.171696 29029 solver.cpp:337] Iteration 2300, Testing net (#0)
I0625 14:15:11.582852 29029 solver.cpp:404]     Test net output #0: accuracy = 0.982539
I0625 14:15:11.582876 29029 solver.cpp:404]     Test net output #1: loss = 0.0640934 (* 1 = 0.0640934 loss)
I0625 14:15:12.135083 29029 solver.cpp:228] Iteration 2300, loss = 0.027397
I0625 14:15:12.135108 29029 solver.cpp:244]     Train net output #0: accuracy = 0.989885
I0625 14:15:12.135116 29029 solver.cpp:244]     Train net output #1: loss = 0.027397 (* 1 = 0.027397 loss)
I0625 14:15:12.135121 29029 sgd_solver.cpp:106] Iteration 2300, lr = 0.001
I0625 14:15:31.516998 29029 solver.cpp:228] Iteration 2320, loss = 0.0243937
I0625 14:15:31.517093 29029 solver.cpp:244]     Train net output #0: accuracy = 0.99066
I0625 14:15:31.517103 29029 solver.cpp:244]     Train net output #1: loss = 0.0243937 (* 1 = 0.0243937 loss)
I0625 14:15:31.517107 29029 sgd_solver.cpp:106] Iteration 2320, lr = 0.001
I0625 14:15:50.908929 29029 solver.cpp:228] Iteration 2340, loss = 0.0212509
I0625 14:15:50.908954 29029 solver.cpp:244]     Train net output #0: accuracy = 0.992192
I0625 14:15:50.908962 29029 solver.cpp:244]     Train net output #1: loss = 0.0212509 (* 1 = 0.0212509 loss)
I0625 14:15:50.908967 29029 sgd_solver.cpp:106] Iteration 2340, lr = 0.001
I0625 14:16:10.279320 29029 solver.cpp:228] Iteration 2360, loss = 0.0229883
I0625 14:16:10.279453 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991475
I0625 14:16:10.279464 29029 solver.cpp:244]     Train net output #1: loss = 0.0229883 (* 1 = 0.0229883 loss)
I0625 14:16:10.279469 29029 sgd_solver.cpp:106] Iteration 2360, lr = 0.001
I0625 14:16:29.654860 29029 solver.cpp:228] Iteration 2380, loss = 0.0222028
I0625 14:16:29.654884 29029 solver.cpp:244]     Train net output #0: accuracy = 0.992078
I0625 14:16:29.654891 29029 solver.cpp:244]     Train net output #1: loss = 0.0222028 (* 1 = 0.0222028 loss)
I0625 14:16:29.654896 29029 sgd_solver.cpp:106] Iteration 2380, lr = 0.001
I0625 14:16:48.529904 29029 solver.cpp:337] Iteration 2400, Testing net (#0)
I0625 14:16:48.941015 29029 solver.cpp:404]     Test net output #0: accuracy = 0.984108
I0625 14:16:48.941051 29029 solver.cpp:404]     Test net output #1: loss = 0.0471031 (* 1 = 0.0471031 loss)
I0625 14:16:49.493842 29029 solver.cpp:228] Iteration 2400, loss = 0.0260822
I0625 14:16:49.493866 29029 solver.cpp:244]     Train net output #0: accuracy = 0.990163
I0625 14:16:49.493873 29029 solver.cpp:244]     Train net output #1: loss = 0.0260822 (* 1 = 0.0260822 loss)
I0625 14:16:49.493878 29029 sgd_solver.cpp:106] Iteration 2400, lr = 0.001
I0625 14:17:08.856681 29029 solver.cpp:228] Iteration 2420, loss = 0.0239229
I0625 14:17:08.856706 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991286
I0625 14:17:08.856714 29029 solver.cpp:244]     Train net output #1: loss = 0.0239229 (* 1 = 0.0239229 loss)
I0625 14:17:08.856719 29029 sgd_solver.cpp:106] Iteration 2420, lr = 0.001
I0625 14:17:28.219125 29029 solver.cpp:228] Iteration 2440, loss = 0.0299593
I0625 14:17:28.219226 29029 solver.cpp:244]     Train net output #0: accuracy = 0.988864
I0625 14:17:28.219236 29029 solver.cpp:244]     Train net output #1: loss = 0.0299593 (* 1 = 0.0299593 loss)
I0625 14:17:28.219241 29029 sgd_solver.cpp:106] Iteration 2440, lr = 0.001
I0625 14:17:47.600018 29029 solver.cpp:228] Iteration 2460, loss = 0.022618
I0625 14:17:47.600044 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991683
I0625 14:17:47.600051 29029 solver.cpp:244]     Train net output #1: loss = 0.022618 (* 1 = 0.022618 loss)
I0625 14:17:47.600055 29029 sgd_solver.cpp:106] Iteration 2460, lr = 0.001
I0625 14:18:06.982465 29029 solver.cpp:228] Iteration 2480, loss = 0.022698
I0625 14:18:06.982585 29029 solver.cpp:244]     Train net output #0: accuracy = 0.99144
I0625 14:18:06.982643 29029 solver.cpp:244]     Train net output #1: loss = 0.022698 (* 1 = 0.022698 loss)
I0625 14:18:06.982656 29029 sgd_solver.cpp:106] Iteration 2480, lr = 0.001
I0625 14:18:25.825892 29029 solver.cpp:454] Snapshotting to binary proto file data/models/segnet_iter_2500.caffemodel
I0625 14:18:25.873083 29029 sgd_solver.cpp:273] Snapshotting solver state to binary proto file data/models/segnet_iter_2500.solverstate
I0625 14:18:25.896505 29029 solver.cpp:337] Iteration 2500, Testing net (#0)
I0625 14:18:26.310032 29029 solver.cpp:404]     Test net output #0: accuracy = 0.987002
I0625 14:18:26.310056 29029 solver.cpp:404]     Test net output #1: loss = 0.0384456 (* 1 = 0.0384456 loss)
I0625 14:18:26.864465 29029 solver.cpp:228] Iteration 2500, loss = 0.0231437
I0625 14:18:26.864490 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991783
I0625 14:18:26.864497 29029 solver.cpp:244]     Train net output #1: loss = 0.0231437 (* 1 = 0.0231437 loss)
I0625 14:18:26.864501 29029 sgd_solver.cpp:106] Iteration 2500, lr = 0.001
I0625 14:18:46.280074 29029 solver.cpp:228] Iteration 2520, loss = 0.0227301
I0625 14:18:46.280179 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991193
I0625 14:18:46.280190 29029 solver.cpp:244]     Train net output #1: loss = 0.0227301 (* 1 = 0.0227301 loss)
I0625 14:18:46.280194 29029 sgd_solver.cpp:106] Iteration 2520, lr = 0.001
I0625 14:19:05.665976 29029 solver.cpp:228] Iteration 2540, loss = 0.0218103
I0625 14:19:05.665999 29029 solver.cpp:244]     Train net output #0: accuracy = 0.992065
I0625 14:19:05.666007 29029 solver.cpp:244]     Train net output #1: loss = 0.0218103 (* 1 = 0.0218103 loss)
I0625 14:19:05.666012 29029 sgd_solver.cpp:106] Iteration 2540, lr = 0.001
I0625 14:19:25.047682 29029 solver.cpp:228] Iteration 2560, loss = 0.0231129
I0625 14:19:25.047796 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991564
I0625 14:19:25.047807 29029 solver.cpp:244]     Train net output #1: loss = 0.0231129 (* 1 = 0.0231129 loss)
I0625 14:19:25.047812 29029 sgd_solver.cpp:106] Iteration 2560, lr = 0.001
I0625 14:19:44.433123 29029 solver.cpp:228] Iteration 2580, loss = 0.0206637
I0625 14:19:44.433159 29029 solver.cpp:244]     Train net output #0: accuracy = 0.99254
I0625 14:19:44.433167 29029 solver.cpp:244]     Train net output #1: loss = 0.0206637 (* 1 = 0.0206637 loss)
I0625 14:19:44.433172 29029 sgd_solver.cpp:106] Iteration 2580, lr = 0.001
I0625 14:20:03.268744 29029 solver.cpp:337] Iteration 2600, Testing net (#0)
I0625 14:20:03.679894 29029 solver.cpp:404]     Test net output #0: accuracy = 0.983842
I0625 14:20:03.679929 29029 solver.cpp:404]     Test net output #1: loss = 0.0514879 (* 1 = 0.0514879 loss)
I0625 14:20:04.234575 29029 solver.cpp:228] Iteration 2600, loss = 0.0249188
I0625 14:20:04.234597 29029 solver.cpp:244]     Train net output #0: accuracy = 0.990838
I0625 14:20:04.234606 29029 solver.cpp:244]     Train net output #1: loss = 0.0249188 (* 1 = 0.0249188 loss)
I0625 14:20:04.234611 29029 sgd_solver.cpp:106] Iteration 2600, lr = 0.001
I0625 14:20:23.623883 29029 solver.cpp:228] Iteration 2620, loss = 0.028146
I0625 14:20:23.623906 29029 solver.cpp:244]     Train net output #0: accuracy = 0.989485
I0625 14:20:23.623914 29029 solver.cpp:244]     Train net output #1: loss = 0.028146 (* 1 = 0.028146 loss)
I0625 14:20:23.623919 29029 sgd_solver.cpp:106] Iteration 2620, lr = 0.001
I0625 14:20:43.050763 29029 solver.cpp:228] Iteration 2640, loss = 0.0224349
I0625 14:20:43.050866 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991464
I0625 14:20:43.050876 29029 solver.cpp:244]     Train net output #1: loss = 0.0224349 (* 1 = 0.0224349 loss)
I0625 14:20:43.050880 29029 sgd_solver.cpp:106] Iteration 2640, lr = 0.001
I0625 14:21:02.436540 29029 solver.cpp:228] Iteration 2660, loss = 0.0226516
I0625 14:21:02.436565 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991363
I0625 14:21:02.436573 29029 solver.cpp:244]     Train net output #1: loss = 0.0226516 (* 1 = 0.0226516 loss)
I0625 14:21:02.436578 29029 sgd_solver.cpp:106] Iteration 2660, lr = 0.001
I0625 14:21:21.830315 29029 solver.cpp:228] Iteration 2680, loss = 0.0231143
I0625 14:21:21.830407 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991536
I0625 14:21:21.830418 29029 solver.cpp:244]     Train net output #1: loss = 0.0231143 (* 1 = 0.0231143 loss)
I0625 14:21:21.830423 29029 sgd_solver.cpp:106] Iteration 2680, lr = 0.001
I0625 14:21:40.664885 29029 solver.cpp:337] Iteration 2700, Testing net (#0)
I0625 14:21:41.076653 29029 solver.cpp:404]     Test net output #0: accuracy = 0.986542
I0625 14:21:41.076686 29029 solver.cpp:404]     Test net output #1: loss = 0.0356269 (* 1 = 0.0356269 loss)
I0625 14:21:41.628048 29029 solver.cpp:228] Iteration 2700, loss = 0.0234849
I0625 14:21:41.628073 29029 solver.cpp:244]     Train net output #0: accuracy = 0.990665
I0625 14:21:41.628082 29029 solver.cpp:244]     Train net output #1: loss = 0.0234849 (* 1 = 0.0234849 loss)
I0625 14:21:41.628085 29029 sgd_solver.cpp:106] Iteration 2700, lr = 0.001
I0625 14:22:01.014456 29029 solver.cpp:228] Iteration 2720, loss = 0.0214329
I0625 14:22:01.014587 29029 solver.cpp:244]     Train net output #0: accuracy = 0.992433
I0625 14:22:01.014597 29029 solver.cpp:244]     Train net output #1: loss = 0.0214329 (* 1 = 0.0214329 loss)
I0625 14:22:01.014603 29029 sgd_solver.cpp:106] Iteration 2720, lr = 0.001
I0625 14:22:20.396375 29029 solver.cpp:228] Iteration 2740, loss = 0.0209745
I0625 14:22:20.396400 29029 solver.cpp:244]     Train net output #0: accuracy = 0.99244
I0625 14:22:20.396409 29029 solver.cpp:244]     Train net output #1: loss = 0.0209745 (* 1 = 0.0209745 loss)
I0625 14:22:20.396414 29029 sgd_solver.cpp:106] Iteration 2740, lr = 0.001
I0625 14:22:39.815513 29029 solver.cpp:228] Iteration 2760, loss = 0.0223413
I0625 14:22:39.815616 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991627
I0625 14:22:39.815626 29029 solver.cpp:244]     Train net output #1: loss = 0.0223413 (* 1 = 0.0223413 loss)
I0625 14:22:39.815631 29029 sgd_solver.cpp:106] Iteration 2760, lr = 0.001
I0625 14:22:59.223531 29029 solver.cpp:228] Iteration 2780, loss = 0.0223392
I0625 14:22:59.223556 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991839
I0625 14:22:59.223563 29029 solver.cpp:244]     Train net output #1: loss = 0.0223392 (* 1 = 0.0223392 loss)
I0625 14:22:59.223569 29029 sgd_solver.cpp:106] Iteration 2780, lr = 0.001
I0625 14:23:18.063500 29029 solver.cpp:337] Iteration 2800, Testing net (#0)
I0625 14:23:18.475242 29029 solver.cpp:404]     Test net output #0: accuracy = 0.985508
I0625 14:23:18.475266 29029 solver.cpp:404]     Test net output #1: loss = 0.0516131 (* 1 = 0.0516131 loss)
I0625 14:23:19.027992 29029 solver.cpp:228] Iteration 2800, loss = 0.0247666
I0625 14:23:19.028028 29029 solver.cpp:244]     Train net output #0: accuracy = 0.99064
I0625 14:23:19.028036 29029 solver.cpp:244]     Train net output #1: loss = 0.0247666 (* 1 = 0.0247666 loss)
I0625 14:23:19.028040 29029 sgd_solver.cpp:106] Iteration 2800, lr = 0.001
I0625 14:23:38.415477 29029 solver.cpp:228] Iteration 2820, loss = 0.0226283
I0625 14:23:38.415501 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991589
I0625 14:23:38.415509 29029 solver.cpp:244]     Train net output #1: loss = 0.0226283 (* 1 = 0.0226283 loss)
I0625 14:23:38.415514 29029 sgd_solver.cpp:106] Iteration 2820, lr = 0.001
I0625 14:23:57.817642 29029 solver.cpp:228] Iteration 2840, loss = 0.0238659
I0625 14:23:57.817744 29029 solver.cpp:244]     Train net output #0: accuracy = 0.990951
I0625 14:23:57.817754 29029 solver.cpp:244]     Train net output #1: loss = 0.0238659 (* 1 = 0.0238659 loss)
I0625 14:23:57.817759 29029 sgd_solver.cpp:106] Iteration 2840, lr = 0.001
I0625 14:24:17.201417 29029 solver.cpp:228] Iteration 2860, loss = 0.0228797
I0625 14:24:17.201442 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991687
I0625 14:24:17.201449 29029 solver.cpp:244]     Train net output #1: loss = 0.0228797 (* 1 = 0.0228797 loss)
I0625 14:24:17.201454 29029 sgd_solver.cpp:106] Iteration 2860, lr = 0.001
I0625 14:24:36.603694 29029 solver.cpp:228] Iteration 2880, loss = 0.0225953
I0625 14:24:36.603801 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991919
I0625 14:24:36.603811 29029 solver.cpp:244]     Train net output #1: loss = 0.0225953 (* 1 = 0.0225953 loss)
I0625 14:24:36.603816 29029 sgd_solver.cpp:106] Iteration 2880, lr = 0.001
I0625 14:24:55.448799 29029 solver.cpp:337] Iteration 2900, Testing net (#0)
I0625 14:24:55.859894 29029 solver.cpp:404]     Test net output #0: accuracy = 0.985086
I0625 14:24:55.859917 29029 solver.cpp:404]     Test net output #1: loss = 0.0438299 (* 1 = 0.0438299 loss)
I0625 14:24:56.410104 29029 solver.cpp:228] Iteration 2900, loss = 0.0257278
I0625 14:24:56.410127 29029 solver.cpp:244]     Train net output #0: accuracy = 0.989789
I0625 14:24:56.410146 29029 solver.cpp:244]     Train net output #1: loss = 0.0257278 (* 1 = 0.0257278 loss)
I0625 14:24:56.410151 29029 sgd_solver.cpp:106] Iteration 2900, lr = 0.001
I0625 14:25:15.825904 29029 solver.cpp:228] Iteration 2920, loss = 0.0262451
I0625 14:25:15.826027 29029 solver.cpp:244]     Train net output #0: accuracy = 0.989645
I0625 14:25:15.826038 29029 solver.cpp:244]     Train net output #1: loss = 0.0262451 (* 1 = 0.0262451 loss)
I0625 14:25:15.826043 29029 sgd_solver.cpp:106] Iteration 2920, lr = 0.001
I0625 14:25:35.222887 29029 solver.cpp:228] Iteration 2940, loss = 0.0213217
I0625 14:25:35.222913 29029 solver.cpp:244]     Train net output #0: accuracy = 0.992369
I0625 14:25:35.222920 29029 solver.cpp:244]     Train net output #1: loss = 0.0213217 (* 1 = 0.0213217 loss)
I0625 14:25:35.222925 29029 sgd_solver.cpp:106] Iteration 2940, lr = 0.001
I0625 14:25:54.650270 29029 solver.cpp:228] Iteration 2960, loss = 0.0218524
I0625 14:25:54.650367 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991918
I0625 14:25:54.650375 29029 solver.cpp:244]     Train net output #1: loss = 0.0218524 (* 1 = 0.0218524 loss)
I0625 14:25:54.650380 29029 sgd_solver.cpp:106] Iteration 2960, lr = 0.001
I0625 14:26:14.052800 29029 solver.cpp:228] Iteration 2980, loss = 0.0206564
I0625 14:26:14.052827 29029 solver.cpp:244]     Train net output #0: accuracy = 0.992258
I0625 14:26:14.052835 29029 solver.cpp:244]     Train net output #1: loss = 0.0206564 (* 1 = 0.0206564 loss)
I0625 14:26:14.052839 29029 sgd_solver.cpp:106] Iteration 2980, lr = 0.001
I0625 14:26:32.936439 29029 solver.cpp:454] Snapshotting to binary proto file data/models/segnet_iter_3000.caffemodel
I0625 14:26:32.988478 29029 sgd_solver.cpp:273] Snapshotting solver state to binary proto file data/models/segnet_iter_3000.solverstate
I0625 14:26:33.016252 29029 solver.cpp:337] Iteration 3000, Testing net (#0)
I0625 14:26:33.433873 29029 solver.cpp:404]     Test net output #0: accuracy = 0.983602
I0625 14:26:33.433897 29029 solver.cpp:404]     Test net output #1: loss = 0.0482183 (* 1 = 0.0482183 loss)
I0625 14:26:33.996043 29029 solver.cpp:228] Iteration 3000, loss = 0.0225961
I0625 14:26:33.996068 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991636
I0625 14:26:33.996076 29029 solver.cpp:244]     Train net output #1: loss = 0.0225961 (* 1 = 0.0225961 loss)
I0625 14:26:33.996080 29029 sgd_solver.cpp:106] Iteration 3000, lr = 0.001
I0625 14:26:53.376400 29029 solver.cpp:228] Iteration 3020, loss = 0.0245332
I0625 14:26:53.376426 29029 solver.cpp:244]     Train net output #0: accuracy = 0.990933
I0625 14:26:53.376433 29029 solver.cpp:244]     Train net output #1: loss = 0.0245332 (* 1 = 0.0245332 loss)
I0625 14:26:53.376437 29029 sgd_solver.cpp:106] Iteration 3020, lr = 0.001
I0625 14:27:12.754150 29029 solver.cpp:228] Iteration 3040, loss = 0.0211891
I0625 14:27:12.754252 29029 solver.cpp:244]     Train net output #0: accuracy = 0.992156
I0625 14:27:12.754262 29029 solver.cpp:244]     Train net output #1: loss = 0.0211891 (* 1 = 0.0211891 loss)
I0625 14:27:12.754267 29029 sgd_solver.cpp:106] Iteration 3040, lr = 0.001
I0625 14:27:32.142077 29029 solver.cpp:228] Iteration 3060, loss = 0.0219597
I0625 14:27:32.142114 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991873
I0625 14:27:32.142122 29029 solver.cpp:244]     Train net output #1: loss = 0.0219597 (* 1 = 0.0219597 loss)
I0625 14:27:32.142127 29029 sgd_solver.cpp:106] Iteration 3060, lr = 0.001
I0625 14:27:51.532814 29029 solver.cpp:228] Iteration 3080, loss = 0.0235381
I0625 14:27:51.532910 29029 solver.cpp:244]     Train net output #0: accuracy = 0.990967
I0625 14:27:51.532920 29029 solver.cpp:244]     Train net output #1: loss = 0.0235381 (* 1 = 0.0235381 loss)
I0625 14:27:51.532925 29029 sgd_solver.cpp:106] Iteration 3080, lr = 0.001
I0625 14:28:10.367300 29029 solver.cpp:337] Iteration 3100, Testing net (#0)
I0625 14:28:10.778607 29029 solver.cpp:404]     Test net output #0: accuracy = 0.986252
I0625 14:28:10.778630 29029 solver.cpp:404]     Test net output #1: loss = 0.0488846 (* 1 = 0.0488846 loss)
I0625 14:28:11.329915 29029 solver.cpp:228] Iteration 3100, loss = 0.0231635
I0625 14:28:11.329939 29029 solver.cpp:244]     Train net output #0: accuracy = 0.990646
I0625 14:28:11.329946 29029 solver.cpp:244]     Train net output #1: loss = 0.0231635 (* 1 = 0.0231635 loss)
I0625 14:28:11.329952 29029 sgd_solver.cpp:106] Iteration 3100, lr = 0.001
I0625 14:28:30.715443 29029 solver.cpp:228] Iteration 3120, loss = 0.0238354
I0625 14:28:30.715558 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991155
I0625 14:28:30.715567 29029 solver.cpp:244]     Train net output #1: loss = 0.0238354 (* 1 = 0.0238354 loss)
I0625 14:28:30.715574 29029 sgd_solver.cpp:106] Iteration 3120, lr = 0.001
I0625 14:28:50.174970 29029 solver.cpp:228] Iteration 3140, loss = 0.0229291
I0625 14:28:50.174995 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991605
I0625 14:28:50.175004 29029 solver.cpp:244]     Train net output #1: loss = 0.0229291 (* 1 = 0.0229291 loss)
I0625 14:28:50.175007 29029 sgd_solver.cpp:106] Iteration 3140, lr = 0.001
I0625 14:29:09.597452 29029 solver.cpp:228] Iteration 3160, loss = 0.023455
I0625 14:29:09.597555 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991022
I0625 14:29:09.597565 29029 solver.cpp:244]     Train net output #1: loss = 0.023455 (* 1 = 0.023455 loss)
I0625 14:29:09.597570 29029 sgd_solver.cpp:106] Iteration 3160, lr = 0.001
I0625 14:29:28.997403 29029 solver.cpp:228] Iteration 3180, loss = 0.0244564
I0625 14:29:28.997439 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991041
I0625 14:29:28.997447 29029 solver.cpp:244]     Train net output #1: loss = 0.0244564 (* 1 = 0.0244564 loss)
I0625 14:29:28.997452 29029 sgd_solver.cpp:106] Iteration 3180, lr = 0.001
I0625 14:29:47.850543 29029 solver.cpp:337] Iteration 3200, Testing net (#0)
I0625 14:29:48.263592 29029 solver.cpp:404]     Test net output #0: accuracy = 0.987726
I0625 14:29:48.263614 29029 solver.cpp:404]     Test net output #1: loss = 0.0295277 (* 1 = 0.0295277 loss)
I0625 14:29:48.814009 29029 solver.cpp:228] Iteration 3200, loss = 0.0239911
I0625 14:29:48.814035 29029 solver.cpp:244]     Train net output #0: accuracy = 0.990849
I0625 14:29:48.814043 29029 solver.cpp:244]     Train net output #1: loss = 0.0239911 (* 1 = 0.0239911 loss)
I0625 14:29:48.814048 29029 sgd_solver.cpp:106] Iteration 3200, lr = 0.001
I0625 14:30:08.187674 29029 solver.cpp:228] Iteration 3220, loss = 0.0225445
I0625 14:30:08.187708 29029 solver.cpp:244]     Train net output #0: accuracy = 0.99158
I0625 14:30:08.187716 29029 solver.cpp:244]     Train net output #1: loss = 0.0225445 (* 1 = 0.0225445 loss)
I0625 14:30:08.187721 29029 sgd_solver.cpp:106] Iteration 3220, lr = 0.001
I0625 14:30:27.567885 29029 solver.cpp:228] Iteration 3240, loss = 0.0226421
I0625 14:30:27.567991 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991487
I0625 14:30:27.568001 29029 solver.cpp:244]     Train net output #1: loss = 0.0226421 (* 1 = 0.0226421 loss)
I0625 14:30:27.568006 29029 sgd_solver.cpp:106] Iteration 3240, lr = 0.001
I0625 14:30:47.044090 29029 solver.cpp:228] Iteration 3260, loss = 0.0229106
I0625 14:30:47.044116 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991241
I0625 14:30:47.044124 29029 solver.cpp:244]     Train net output #1: loss = 0.0229106 (* 1 = 0.0229106 loss)
I0625 14:30:47.044128 29029 sgd_solver.cpp:106] Iteration 3260, lr = 0.001
I0625 14:31:06.462949 29029 solver.cpp:228] Iteration 3280, loss = 0.0210416
I0625 14:31:06.463057 29029 solver.cpp:244]     Train net output #0: accuracy = 0.99219
I0625 14:31:06.463068 29029 solver.cpp:244]     Train net output #1: loss = 0.0210416 (* 1 = 0.0210416 loss)
I0625 14:31:06.463073 29029 sgd_solver.cpp:106] Iteration 3280, lr = 0.001
I0625 14:31:25.306540 29029 solver.cpp:337] Iteration 3300, Testing net (#0)
I0625 14:31:25.717375 29029 solver.cpp:404]     Test net output #0: accuracy = 0.987743
I0625 14:31:25.717409 29029 solver.cpp:404]     Test net output #1: loss = 0.0374043 (* 1 = 0.0374043 loss)
I0625 14:31:26.267084 29029 solver.cpp:228] Iteration 3300, loss = 0.0227186
I0625 14:31:26.267109 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991524
I0625 14:31:26.267117 29029 solver.cpp:244]     Train net output #1: loss = 0.0227186 (* 1 = 0.0227186 loss)
I0625 14:31:26.267122 29029 sgd_solver.cpp:106] Iteration 3300, lr = 0.001
I0625 14:31:45.651535 29029 solver.cpp:228] Iteration 3320, loss = 0.0248371
I0625 14:31:45.651671 29029 solver.cpp:244]     Train net output #0: accuracy = 0.990642
I0625 14:31:45.651681 29029 solver.cpp:244]     Train net output #1: loss = 0.0248371 (* 1 = 0.0248371 loss)
I0625 14:31:45.651686 29029 sgd_solver.cpp:106] Iteration 3320, lr = 0.001
I0625 14:32:05.042237 29029 solver.cpp:228] Iteration 3340, loss = 0.0249698
I0625 14:32:05.042273 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991037
I0625 14:32:05.042282 29029 solver.cpp:244]     Train net output #1: loss = 0.0249698 (* 1 = 0.0249698 loss)
I0625 14:32:05.042286 29029 sgd_solver.cpp:106] Iteration 3340, lr = 0.001
I0625 14:32:24.426235 29029 solver.cpp:228] Iteration 3360, loss = 0.0217275
I0625 14:32:24.426349 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991796
I0625 14:32:24.426359 29029 solver.cpp:244]     Train net output #1: loss = 0.0217275 (* 1 = 0.0217275 loss)
I0625 14:32:24.426364 29029 sgd_solver.cpp:106] Iteration 3360, lr = 0.001
I0625 14:32:43.913590 29029 solver.cpp:228] Iteration 3380, loss = 0.0227435
I0625 14:32:43.913616 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991516
I0625 14:32:43.913624 29029 solver.cpp:244]     Train net output #1: loss = 0.0227435 (* 1 = 0.0227435 loss)
I0625 14:32:43.913628 29029 sgd_solver.cpp:106] Iteration 3380, lr = 0.001
I0625 14:33:02.744215 29029 solver.cpp:337] Iteration 3400, Testing net (#0)
I0625 14:33:03.155045 29029 solver.cpp:404]     Test net output #0: accuracy = 0.980162
I0625 14:33:03.155078 29029 solver.cpp:404]     Test net output #1: loss = 0.0697422 (* 1 = 0.0697422 loss)
I0625 14:33:03.709974 29029 solver.cpp:228] Iteration 3400, loss = 0.0242163
I0625 14:33:03.710008 29029 solver.cpp:244]     Train net output #0: accuracy = 0.990779
I0625 14:33:03.710016 29029 solver.cpp:244]     Train net output #1: loss = 0.0242163 (* 1 = 0.0242163 loss)
I0625 14:33:03.710021 29029 sgd_solver.cpp:106] Iteration 3400, lr = 0.001
I0625 14:33:23.108903 29029 solver.cpp:228] Iteration 3420, loss = 0.0213934
I0625 14:33:23.108937 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991857
I0625 14:33:23.108945 29029 solver.cpp:244]     Train net output #1: loss = 0.0213934 (* 1 = 0.0213934 loss)
I0625 14:33:23.108950 29029 sgd_solver.cpp:106] Iteration 3420, lr = 0.001
I0625 14:33:42.479822 29029 solver.cpp:228] Iteration 3440, loss = 0.0226103
I0625 14:33:42.479939 29029 solver.cpp:244]     Train net output #0: accuracy = 0.99166
I0625 14:33:42.479948 29029 solver.cpp:244]     Train net output #1: loss = 0.0226103 (* 1 = 0.0226103 loss)
I0625 14:33:42.479954 29029 sgd_solver.cpp:106] Iteration 3440, lr = 0.001
I0625 14:34:01.859812 29029 solver.cpp:228] Iteration 3460, loss = 0.0229372
I0625 14:34:01.859835 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991842
I0625 14:34:01.859843 29029 solver.cpp:244]     Train net output #1: loss = 0.0229372 (* 1 = 0.0229372 loss)
I0625 14:34:01.859848 29029 sgd_solver.cpp:106] Iteration 3460, lr = 0.001
I0625 14:34:21.252399 29029 solver.cpp:228] Iteration 3480, loss = 0.022604
I0625 14:34:21.252492 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991526
I0625 14:34:21.252501 29029 solver.cpp:244]     Train net output #1: loss = 0.022604 (* 1 = 0.022604 loss)
I0625 14:34:21.252506 29029 sgd_solver.cpp:106] Iteration 3480, lr = 0.001
I0625 14:34:40.151674 29029 solver.cpp:454] Snapshotting to binary proto file data/models/segnet_iter_3500.caffemodel
I0625 14:34:40.197682 29029 sgd_solver.cpp:273] Snapshotting solver state to binary proto file data/models/segnet_iter_3500.solverstate
I0625 14:34:40.220403 29029 solver.cpp:337] Iteration 3500, Testing net (#0)
I0625 14:34:40.633121 29029 solver.cpp:404]     Test net output #0: accuracy = 0.985542
I0625 14:34:40.633155 29029 solver.cpp:404]     Test net output #1: loss = 0.0448908 (* 1 = 0.0448908 loss)
I0625 14:34:41.186301 29029 solver.cpp:228] Iteration 3500, loss = 0.0224326
I0625 14:34:41.186327 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991591
I0625 14:34:41.186336 29029 solver.cpp:244]     Train net output #1: loss = 0.0224326 (* 1 = 0.0224326 loss)
I0625 14:34:41.186341 29029 sgd_solver.cpp:106] Iteration 3500, lr = 0.001
I0625 14:35:00.555862 29029 solver.cpp:228] Iteration 3520, loss = 0.019925
I0625 14:35:00.555984 29029 solver.cpp:244]     Train net output #0: accuracy = 0.992564
I0625 14:35:00.555994 29029 solver.cpp:244]     Train net output #1: loss = 0.019925 (* 1 = 0.019925 loss)
I0625 14:35:00.555999 29029 sgd_solver.cpp:106] Iteration 3520, lr = 0.001
I0625 14:35:19.947754 29029 solver.cpp:228] Iteration 3540, loss = 0.0219836
I0625 14:35:19.947779 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991504
I0625 14:35:19.947788 29029 solver.cpp:244]     Train net output #1: loss = 0.0219836 (* 1 = 0.0219836 loss)
I0625 14:35:19.947793 29029 sgd_solver.cpp:106] Iteration 3540, lr = 0.001
I0625 14:35:39.319854 29029 solver.cpp:228] Iteration 3560, loss = 0.0231006
I0625 14:35:39.319955 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991552
I0625 14:35:39.319964 29029 solver.cpp:244]     Train net output #1: loss = 0.0231006 (* 1 = 0.0231006 loss)
I0625 14:35:39.319969 29029 sgd_solver.cpp:106] Iteration 3560, lr = 0.001
I0625 14:35:58.711029 29029 solver.cpp:228] Iteration 3580, loss = 0.0236904
I0625 14:35:58.711055 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991327
I0625 14:35:58.711063 29029 solver.cpp:244]     Train net output #1: loss = 0.0236904 (* 1 = 0.0236904 loss)
I0625 14:35:58.711068 29029 sgd_solver.cpp:106] Iteration 3580, lr = 0.001
I0625 14:36:17.565454 29029 solver.cpp:337] Iteration 3600, Testing net (#0)
I0625 14:36:17.976369 29029 solver.cpp:404]     Test net output #0: accuracy = 0.979254
I0625 14:36:17.976393 29029 solver.cpp:404]     Test net output #1: loss = 0.0631177 (* 1 = 0.0631177 loss)
I0625 14:36:18.526391 29029 solver.cpp:228] Iteration 3600, loss = 0.0225558
I0625 14:36:18.526414 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991621
I0625 14:36:18.526433 29029 solver.cpp:244]     Train net output #1: loss = 0.0225558 (* 1 = 0.0225558 loss)
I0625 14:36:18.526437 29029 sgd_solver.cpp:106] Iteration 3600, lr = 0.001
I0625 14:36:38.023460 29029 solver.cpp:228] Iteration 3620, loss = 0.0238244
I0625 14:36:38.023486 29029 solver.cpp:244]     Train net output #0: accuracy = 0.99099
I0625 14:36:38.023494 29029 solver.cpp:244]     Train net output #1: loss = 0.0238244 (* 1 = 0.0238244 loss)
I0625 14:36:38.023499 29029 sgd_solver.cpp:106] Iteration 3620, lr = 0.001
I0625 14:36:57.394748 29029 solver.cpp:228] Iteration 3640, loss = 0.0222272
I0625 14:36:57.394927 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991628
I0625 14:36:57.394937 29029 solver.cpp:244]     Train net output #1: loss = 0.0222272 (* 1 = 0.0222272 loss)
I0625 14:36:57.394942 29029 sgd_solver.cpp:106] Iteration 3640, lr = 0.001
I0625 14:37:16.796479 29029 solver.cpp:228] Iteration 3660, loss = 0.0217219
I0625 14:37:16.796504 29029 solver.cpp:244]     Train net output #0: accuracy = 0.99228
I0625 14:37:16.796512 29029 solver.cpp:244]     Train net output #1: loss = 0.0217219 (* 1 = 0.0217219 loss)
I0625 14:37:16.796519 29029 sgd_solver.cpp:106] Iteration 3660, lr = 0.001
I0625 14:37:36.220966 29029 solver.cpp:228] Iteration 3680, loss = 0.0212102
I0625 14:37:36.221058 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991993
I0625 14:37:36.221068 29029 solver.cpp:244]     Train net output #1: loss = 0.0212102 (* 1 = 0.0212102 loss)
I0625 14:37:36.221073 29029 sgd_solver.cpp:106] Iteration 3680, lr = 0.001
I0625 14:37:55.077038 29029 solver.cpp:337] Iteration 3700, Testing net (#0)
I0625 14:37:55.488278 29029 solver.cpp:404]     Test net output #0: accuracy = 0.98504
I0625 14:37:55.488302 29029 solver.cpp:404]     Test net output #1: loss = 0.0451406 (* 1 = 0.0451406 loss)
I0625 14:37:56.039866 29029 solver.cpp:228] Iteration 3700, loss = 0.0220328
I0625 14:37:56.039891 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991412
I0625 14:37:56.039899 29029 solver.cpp:244]     Train net output #1: loss = 0.0220328 (* 1 = 0.0220328 loss)
I0625 14:37:56.039904 29029 sgd_solver.cpp:106] Iteration 3700, lr = 0.001
I0625 14:38:15.441608 29029 solver.cpp:228] Iteration 3720, loss = 0.0208797
I0625 14:38:15.441751 29029 solver.cpp:244]     Train net output #0: accuracy = 0.992422
I0625 14:38:15.441761 29029 solver.cpp:244]     Train net output #1: loss = 0.0208797 (* 1 = 0.0208797 loss)
I0625 14:38:15.441766 29029 sgd_solver.cpp:106] Iteration 3720, lr = 0.001
I0625 14:38:34.897064 29029 solver.cpp:228] Iteration 3740, loss = 0.0212249
I0625 14:38:34.897100 29029 solver.cpp:244]     Train net output #0: accuracy = 0.992343
I0625 14:38:34.897109 29029 solver.cpp:244]     Train net output #1: loss = 0.0212249 (* 1 = 0.0212249 loss)
I0625 14:38:34.897114 29029 sgd_solver.cpp:106] Iteration 3740, lr = 0.001
I0625 14:38:54.298554 29029 solver.cpp:228] Iteration 3760, loss = 0.0214512
I0625 14:38:54.298645 29029 solver.cpp:244]     Train net output #0: accuracy = 0.992031
I0625 14:38:54.298655 29029 solver.cpp:244]     Train net output #1: loss = 0.0214512 (* 1 = 0.0214512 loss)
I0625 14:38:54.298660 29029 sgd_solver.cpp:106] Iteration 3760, lr = 0.001
I0625 14:39:13.683374 29029 solver.cpp:228] Iteration 3780, loss = 0.0258299
I0625 14:39:13.683410 29029 solver.cpp:244]     Train net output #0: accuracy = 0.990238
I0625 14:39:13.683418 29029 solver.cpp:244]     Train net output #1: loss = 0.0258299 (* 1 = 0.0258299 loss)
I0625 14:39:13.683423 29029 sgd_solver.cpp:106] Iteration 3780, lr = 0.001
I0625 14:39:32.542423 29029 solver.cpp:337] Iteration 3800, Testing net (#0)
I0625 14:39:32.953285 29029 solver.cpp:404]     Test net output #0: accuracy = 0.987306
I0625 14:39:32.953310 29029 solver.cpp:404]     Test net output #1: loss = 0.0349561 (* 1 = 0.0349561 loss)
I0625 14:39:33.505188 29029 solver.cpp:228] Iteration 3800, loss = 0.0209144
I0625 14:39:33.505213 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991955
I0625 14:39:33.505220 29029 solver.cpp:244]     Train net output #1: loss = 0.0209144 (* 1 = 0.0209144 loss)
I0625 14:39:33.505225 29029 sgd_solver.cpp:106] Iteration 3800, lr = 0.001
I0625 14:39:52.885941 29029 solver.cpp:228] Iteration 3820, loss = 0.0216062
I0625 14:39:52.885967 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991924
I0625 14:39:52.885975 29029 solver.cpp:244]     Train net output #1: loss = 0.0216062 (* 1 = 0.0216062 loss)
I0625 14:39:52.885980 29029 sgd_solver.cpp:106] Iteration 3820, lr = 0.001
I0625 14:40:12.269953 29029 solver.cpp:228] Iteration 3840, loss = 0.0219083
I0625 14:40:12.270046 29029 solver.cpp:244]     Train net output #0: accuracy = 0.992033
I0625 14:40:12.270056 29029 solver.cpp:244]     Train net output #1: loss = 0.0219083 (* 1 = 0.0219083 loss)
I0625 14:40:12.270061 29029 sgd_solver.cpp:106] Iteration 3840, lr = 0.001
I0625 14:40:31.680315 29029 solver.cpp:228] Iteration 3860, loss = 0.022482
I0625 14:40:31.680351 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991558
I0625 14:40:31.680359 29029 solver.cpp:244]     Train net output #1: loss = 0.022482 (* 1 = 0.022482 loss)
I0625 14:40:31.680364 29029 sgd_solver.cpp:106] Iteration 3860, lr = 0.001
I0625 14:40:51.085505 29029 solver.cpp:228] Iteration 3880, loss = 0.0223798
I0625 14:40:51.085602 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991165
I0625 14:40:51.085611 29029 solver.cpp:244]     Train net output #1: loss = 0.0223798 (* 1 = 0.0223798 loss)
I0625 14:40:51.085616 29029 sgd_solver.cpp:106] Iteration 3880, lr = 0.001
I0625 14:41:09.908813 29029 solver.cpp:337] Iteration 3900, Testing net (#0)
I0625 14:41:10.319716 29029 solver.cpp:404]     Test net output #0: accuracy = 0.984552
I0625 14:41:10.319751 29029 solver.cpp:404]     Test net output #1: loss = 0.0485467 (* 1 = 0.0485467 loss)
I0625 14:41:10.869081 29029 solver.cpp:228] Iteration 3900, loss = 0.0229922
I0625 14:41:10.869105 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991827
I0625 14:41:10.869112 29029 solver.cpp:244]     Train net output #1: loss = 0.0229922 (* 1 = 0.0229922 loss)
I0625 14:41:10.869117 29029 sgd_solver.cpp:106] Iteration 3900, lr = 0.001
I0625 14:41:30.269631 29029 solver.cpp:228] Iteration 3920, loss = 0.0231107
I0625 14:41:30.269762 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991458
I0625 14:41:30.269772 29029 solver.cpp:244]     Train net output #1: loss = 0.0231107 (* 1 = 0.0231107 loss)
I0625 14:41:30.269776 29029 sgd_solver.cpp:106] Iteration 3920, lr = 0.001
I0625 14:41:49.639354 29029 solver.cpp:228] Iteration 3940, loss = 0.022225
I0625 14:41:49.639390 29029 solver.cpp:244]     Train net output #0: accuracy = 0.992057
I0625 14:41:49.639399 29029 solver.cpp:244]     Train net output #1: loss = 0.022225 (* 1 = 0.022225 loss)
I0625 14:41:49.639403 29029 sgd_solver.cpp:106] Iteration 3940, lr = 0.001
I0625 14:42:09.011637 29029 solver.cpp:228] Iteration 3960, loss = 0.0207322
I0625 14:42:09.011740 29029 solver.cpp:244]     Train net output #0: accuracy = 0.992313
I0625 14:42:09.011749 29029 solver.cpp:244]     Train net output #1: loss = 0.0207322 (* 1 = 0.0207322 loss)
I0625 14:42:09.011754 29029 sgd_solver.cpp:106] Iteration 3960, lr = 0.001
I0625 14:42:28.381028 29029 solver.cpp:228] Iteration 3980, loss = 0.0231615
I0625 14:42:28.381065 29029 solver.cpp:244]     Train net output #0: accuracy = 0.991399
I0625 14:42:28.381073 29029 solver.cpp:244]     Train net output #1: loss = 0.0231615 (* 1 = 0.0231615 loss)
I0625 14:42:28.381078 29029 sgd_solver.cpp:106] Iteration 3980, lr = 0.001
I0625 14:42:47.315790 29029 solver.cpp:454] Snapshotting to binary proto file data/models/segnet_iter_4000.caffemodel
I0625 14:42:47.362454 29029 sgd_solver.cpp:273] Snapshotting solver state to binary proto file data/models/segnet_iter_4000.solverstate
I0625 14:42:47.828971 29029 solver.cpp:317] Iteration 4000, loss = 0.020941
I0625 14:42:47.829006 29029 solver.cpp:337] Iteration 4000, Testing net (#0)
I0625 14:42:48.242491 29029 solver.cpp:404]     Test net output #0: accuracy = 0.985992
I0625 14:42:48.242514 29029 solver.cpp:404]     Test net output #1: loss = 0.0490062 (* 1 = 0.0490062 loss)
I0625 14:42:48.242518 29029 solver.cpp:322] Optimization Done.
I0625 14:42:48.242522 29029 caffe.cpp:222] Optimization Done.
