I0523 15:13:59.882629 21880 caffe.cpp:185] Using GPUs 0
I0523 15:13:59.885561 21880 caffe.cpp:190] GPU 0: GeForce GTX TITAN X
I0523 15:14:00.017076 21880 solver.cpp:48] Initializing solver from parameters: 
test_iter: 16
test_interval: 100
base_lr: 0.01
display: 20
max_iter: 10000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 3000
snapshot: 500
snapshot_prefix: "data/models/segnet"
solver_mode: GPU
device_id: 0
net: "models/segnet/train_val.prototxt"
test_initialization: true
I0523 15:14:00.017184 21880 solver.cpp:91] Creating training net from net file: models/segnet/train_val.prototxt
I0523 15:14:00.018520 21880 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0523 15:14:00.018899 21880 net.cpp:49] Initializing net from parameters: 
name: "segnet"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "DenseImageData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    crop_size: 224
    mean_value: 100
  }
  dense_image_data_param {
    source: "data/train.txt"
    batch_size: 32
    shuffle: true
    new_height: 224
    new_width: 310
    is_color: false
    root_folder: "data/raw/train/"
  }
}
layer {
  name: "conv1_1"
  type: "Convolution"
  bottom: "data"
  top: "conv1_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn1_1"
  type: "BatchNorm"
  bottom: "conv1_1"
  top: "conv1_1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale1_1"
  type: "Scale"
  bottom: "conv1_1"
  top: "conv1_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1_1"
  type: "ReLU"
  bottom: "conv1_1"
  top: "conv1_1"
}
layer {
  name: "conv1_2"
  type: "Convolution"
  bottom: "conv1_1"
  top: "conv1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn1_2"
  type: "BatchNorm"
  bottom: "conv1_2"
  top: "conv1_2"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale1_2"
  type: "Scale"
  bottom: "conv1_2"
  top: "conv1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1_2"
  type: "ReLU"
  bottom: "conv1_2"
  top: "conv1_2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1_2"
  top: "pool1"
  top: "pool1_mask"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2"
  top: "pool2_mask"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn3_1"
  type: "BatchNorm"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale3_1"
  type: "Scale"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv3_2"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv3_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn3_2"
  type: "BatchNorm"
  bottom: "conv3_2"
  top: "conv3_2"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale3_2"
  type: "Scale"
  bottom: "conv3_2"
  top: "conv3_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_2"
  type: "ReLU"
  bottom: "conv3_2"
  top: "conv3_2"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3_2"
  top: "pool3"
  top: "pool3_mask"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4"
  top: "pool4_mask"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv5_1"
  type: "Convolution"
  bottom: "pool4"
  top: "conv5_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn5_1"
  type: "BatchNorm"
  bottom: "conv5_1"
  top: "conv5_1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale5_1"
  type: "Scale"
  bottom: "conv5_1"
  top: "conv5_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5_1"
  type: "ReLU"
  bottom: "conv5_1"
  top: "conv5_1"
}
layer {
  name: "conv5_2"
  type: "Convolution"
  bottom: "conv5_1"
  top: "conv5_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn5_2"
  type: "BatchNorm"
  bottom: "conv5_2"
  top: "conv5_2"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale5_2"
  type: "Scale"
  bottom: "conv5_2"
  top: "conv5_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5_2"
  type: "ReLU"
  bottom: "conv5_2"
  top: "conv5_2"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5_2"
  top: "pool5"
  top: "pool5_mask"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "upsample5"
  type: "Upsample"
  bottom: "pool5"
  bottom: "pool5_mask"
  top: "pool5_D"
  upsample_param {
    scale: 2
    upsample_h: 14
    upsample_w: 14
  }
}
layer {
  name: "conv5_2_D"
  type: "Convolution"
  bottom: "pool5_D"
  top: "conv5_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn5_2_D"
  type: "BatchNorm"
  bottom: "conv5_2_D"
  top: "conv5_2_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale5_2_D"
  type: "Scale"
  bottom: "conv5_2_D"
  top: "conv5_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5_2_D"
  type: "ReLU"
  bottom: "conv5_2_D"
  top: "conv5_2_D"
}
layer {
  name: "conv5_1_D"
  type: "Convolution"
  bottom: "conv5_2_D"
  top: "conv5_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn5_1_D"
  type: "BatchNorm"
  bottom: "conv5_1_D"
  top: "conv5_1_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale5_1_D"
  type: "Scale"
  bottom: "conv5_1_D"
  top: "conv5_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5_1_D"
  type: "ReLU"
  bottom: "conv5_1_D"
  top: "conv5_1_D"
}
layer {
  name: "upsample4"
  type: "Upsample"
  bottom: "conv5_1_D"
  bottom: "pool4_mask"
  top: "pool4_D"
  upsample_param {
    scale: 2
    upsample_h: 28
    upsample_w: 28
  }
}
layer {
  name: "conv4_2_D"
  type: "Convolution"
  bottom: "pool4_D"
  top: "conv4_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn4_2_D"
  type: "BatchNorm"
  bottom: "conv4_2_D"
  top: "conv4_2_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale4_2_D"
  type: "Scale"
  bottom: "conv4_2_D"
  top: "conv4_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4_2_D"
  type: "ReLU"
  bottom: "conv4_2_D"
  top: "conv4_2_D"
}
layer {
  name: "conv4_1_D"
  type: "Convolution"
  bottom: "conv4_2_D"
  top: "conv4_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn4_1_D"
  type: "BatchNorm"
  bottom: "conv4_1_D"
  top: "conv4_1_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale4_1_D"
  type: "Scale"
  bottom: "conv4_1_D"
  top: "conv4_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4_1_D"
  type: "ReLU"
  bottom: "conv4_1_D"
  top: "conv4_1_D"
}
layer {
  name: "upsample3"
  type: "Upsample"
  bottom: "conv4_1_D"
  bottom: "pool3_mask"
  top: "pool3_D"
  upsample_param {
    scale: 2
    upsample_h: 56
    upsample_w: 56
  }
}
layer {
  name: "conv3_2_D"
  type: "Convolution"
  bottom: "pool3_D"
  top: "conv3_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3_2_D"
  type: "BatchNorm"
  bottom: "conv3_2_D"
  top: "conv3_2_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale3_2_D"
  type: "Scale"
  bottom: "conv3_2_D"
  top: "conv3_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_2_D"
  type: "ReLU"
  bottom: "conv3_2_D"
  top: "conv3_2_D"
}
layer {
  name: "conv3_1_D"
  type: "Convolution"
  bottom: "conv3_2_D"
  top: "conv3_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn3_1_D"
  type: "BatchNorm"
  bottom: "conv3_1_D"
  top: "conv3_1_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale3_1_D"
  type: "Scale"
  bottom: "conv3_1_D"
  top: "conv3_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_1_D"
  type: "ReLU"
  bottom: "conv3_1_D"
  top: "conv3_1_D"
}
layer {
  name: "upsample2"
  type: "Upsample"
  bottom: "conv3_1_D"
  bottom: "pool2_mask"
  top: "pool2_D"
  upsample_param {
    scale: 2
    upsample_h: 112
    upsample_w: 112
  }
}
layer {
  name: "conv2_2_D"
  type: "Convolution"
  bottom: "pool2_D"
  top: "conv2_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn2_2_D"
  type: "BatchNorm"
  bottom: "conv2_2_D"
  top: "conv2_2_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale2_2_D"
  type: "Scale"
  bottom: "conv2_2_D"
  top: "conv2_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2_2_D"
  type: "ReLU"
  bottom: "conv2_2_D"
  top: "conv2_2_D"
}
layer {
  name: "conv2_1_D"
  type: "Convolution"
  bottom: "conv2_2_D"
  top: "conv2_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn2_1_D"
  type: "BatchNorm"
  bottom: "conv2_1_D"
  top: "conv2_1_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale2_1_D"
  type: "Scale"
  bottom: "conv2_1_D"
  top: "conv2_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2_1_D"
  type: "ReLU"
  bottom: "conv2_1_D"
  top: "conv2_1_D"
}
layer {
  name: "upsample1"
  type: "Upsample"
  bottom: "conv2_1_D"
  bottom: "pool1_mask"
  top: "pool1_D"
  upsample_param {
    scale: 2
    upsample_h: 224
    upsample_w: 224
  }
}
layer {
  name: "conv1_2_D"
  type: "Convolution"
  bottom: "pool1_D"
  top: "conv1_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn1_2_D"
  type: "BatchNorm"
  bottom: "conv1_2_D"
  top: "conv1_2_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale1_2_D"
  type: "Scale"
  bottom: "conv1_2_D"
  top: "conv1_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1_2_D"
  type: "ReLU"
  bottom: "conv1_2_D"
  top: "conv1_2_D"
}
layer {
  name: "conv1_1_D"
  type: "Convolution"
  bottom: "conv1_2_D"
  top: "conv1_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 2
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "conv1_1_D"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "conv1_1_D"
  bottom: "label"
  top: "accuracy"
}
I0523 15:14:00.019131 21880 layer_factory.hpp:77] Creating layer data
I0523 15:14:00.019157 21880 net.cpp:91] Creating Layer data
I0523 15:14:00.019162 21880 net.cpp:399] data -> data
I0523 15:14:00.019179 21880 net.cpp:399] data -> label
I0523 15:14:00.019461 21880 dense_image_data_layer.cpp:38] Opening file data/train.txt
I0523 15:14:00.021559 21880 dense_image_data_layer.cpp:48] Shuffling data
I0523 15:14:00.022032 21880 dense_image_data_layer.cpp:53] A total of 4930 examples.
I0523 15:14:00.032487 21880 dense_image_data_layer.cpp:92] output data size: 1,1,224,224
I0523 15:14:00.033586 21880 net.cpp:141] Setting up data
I0523 15:14:00.033619 21880 net.cpp:148] Top shape: 1 1 224 224 (50176)
I0523 15:14:00.033623 21880 net.cpp:148] Top shape: 1 1 224 224 (50176)
I0523 15:14:00.033625 21880 net.cpp:156] Memory required for data: 401408
I0523 15:14:00.033632 21880 layer_factory.hpp:77] Creating layer label_data_1_split
I0523 15:14:00.033646 21880 net.cpp:91] Creating Layer label_data_1_split
I0523 15:14:00.033650 21880 net.cpp:425] label_data_1_split <- label
I0523 15:14:00.033659 21880 net.cpp:399] label_data_1_split -> label_data_1_split_0
I0523 15:14:00.033668 21880 net.cpp:399] label_data_1_split -> label_data_1_split_1
I0523 15:14:00.033726 21880 net.cpp:141] Setting up label_data_1_split
I0523 15:14:00.033731 21880 net.cpp:148] Top shape: 1 1 224 224 (50176)
I0523 15:14:00.033745 21880 net.cpp:148] Top shape: 1 1 224 224 (50176)
I0523 15:14:00.033746 21880 net.cpp:156] Memory required for data: 802816
I0523 15:14:00.033749 21880 layer_factory.hpp:77] Creating layer conv1_1
I0523 15:14:00.033761 21880 net.cpp:91] Creating Layer conv1_1
I0523 15:14:00.033764 21880 net.cpp:425] conv1_1 <- data
I0523 15:14:00.033768 21880 net.cpp:399] conv1_1 -> conv1_1
I0523 15:14:00.191020 21880 net.cpp:141] Setting up conv1_1
I0523 15:14:00.191051 21880 net.cpp:148] Top shape: 1 32 224 224 (1605632)
I0523 15:14:00.191054 21880 net.cpp:156] Memory required for data: 7225344
I0523 15:14:00.191067 21880 layer_factory.hpp:77] Creating layer bn1_1
I0523 15:14:00.191082 21880 net.cpp:91] Creating Layer bn1_1
I0523 15:14:00.191087 21880 net.cpp:425] bn1_1 <- conv1_1
I0523 15:14:00.191092 21880 net.cpp:386] bn1_1 -> conv1_1 (in-place)
I0523 15:14:00.191273 21880 net.cpp:141] Setting up bn1_1
I0523 15:14:00.191280 21880 net.cpp:148] Top shape: 1 32 224 224 (1605632)
I0523 15:14:00.191293 21880 net.cpp:156] Memory required for data: 13647872
I0523 15:14:00.191303 21880 layer_factory.hpp:77] Creating layer scale1_1
I0523 15:14:00.191311 21880 net.cpp:91] Creating Layer scale1_1
I0523 15:14:00.191313 21880 net.cpp:425] scale1_1 <- conv1_1
I0523 15:14:00.191318 21880 net.cpp:386] scale1_1 -> conv1_1 (in-place)
I0523 15:14:00.191350 21880 layer_factory.hpp:77] Creating layer scale1_1
I0523 15:14:00.191507 21880 net.cpp:141] Setting up scale1_1
I0523 15:14:00.191514 21880 net.cpp:148] Top shape: 1 32 224 224 (1605632)
I0523 15:14:00.191525 21880 net.cpp:156] Memory required for data: 20070400
I0523 15:14:00.191531 21880 layer_factory.hpp:77] Creating layer relu1_1
I0523 15:14:00.191537 21880 net.cpp:91] Creating Layer relu1_1
I0523 15:14:00.191540 21880 net.cpp:425] relu1_1 <- conv1_1
I0523 15:14:00.191543 21880 net.cpp:386] relu1_1 -> conv1_1 (in-place)
I0523 15:14:00.191776 21880 net.cpp:141] Setting up relu1_1
I0523 15:14:00.191786 21880 net.cpp:148] Top shape: 1 32 224 224 (1605632)
I0523 15:14:00.191797 21880 net.cpp:156] Memory required for data: 26492928
I0523 15:14:00.191800 21880 layer_factory.hpp:77] Creating layer conv1_2
I0523 15:14:00.191809 21880 net.cpp:91] Creating Layer conv1_2
I0523 15:14:00.191812 21880 net.cpp:425] conv1_2 <- conv1_1
I0523 15:14:00.191815 21880 net.cpp:399] conv1_2 -> conv1_2
I0523 15:14:00.193084 21880 net.cpp:141] Setting up conv1_2
I0523 15:14:00.193105 21880 net.cpp:148] Top shape: 1 32 224 224 (1605632)
I0523 15:14:00.193109 21880 net.cpp:156] Memory required for data: 32915456
I0523 15:14:00.193112 21880 layer_factory.hpp:77] Creating layer bn1_2
I0523 15:14:00.193120 21880 net.cpp:91] Creating Layer bn1_2
I0523 15:14:00.193121 21880 net.cpp:425] bn1_2 <- conv1_2
I0523 15:14:00.193125 21880 net.cpp:386] bn1_2 -> conv1_2 (in-place)
I0523 15:14:00.193308 21880 net.cpp:141] Setting up bn1_2
I0523 15:14:00.193315 21880 net.cpp:148] Top shape: 1 32 224 224 (1605632)
I0523 15:14:00.193326 21880 net.cpp:156] Memory required for data: 39337984
I0523 15:14:00.193333 21880 layer_factory.hpp:77] Creating layer scale1_2
I0523 15:14:00.193341 21880 net.cpp:91] Creating Layer scale1_2
I0523 15:14:00.193357 21880 net.cpp:425] scale1_2 <- conv1_2
I0523 15:14:00.193361 21880 net.cpp:386] scale1_2 -> conv1_2 (in-place)
I0523 15:14:00.193392 21880 layer_factory.hpp:77] Creating layer scale1_2
I0523 15:14:00.193554 21880 net.cpp:141] Setting up scale1_2
I0523 15:14:00.193559 21880 net.cpp:148] Top shape: 1 32 224 224 (1605632)
I0523 15:14:00.193572 21880 net.cpp:156] Memory required for data: 45760512
I0523 15:14:00.193577 21880 layer_factory.hpp:77] Creating layer relu1_2
I0523 15:14:00.193583 21880 net.cpp:91] Creating Layer relu1_2
I0523 15:14:00.193584 21880 net.cpp:425] relu1_2 <- conv1_2
I0523 15:14:00.193588 21880 net.cpp:386] relu1_2 -> conv1_2 (in-place)
I0523 15:14:00.193730 21880 net.cpp:141] Setting up relu1_2
I0523 15:14:00.193735 21880 net.cpp:148] Top shape: 1 32 224 224 (1605632)
I0523 15:14:00.193748 21880 net.cpp:156] Memory required for data: 52183040
I0523 15:14:00.193750 21880 layer_factory.hpp:77] Creating layer pool1
I0523 15:14:00.193754 21880 layer_factory.cpp:90] cuDNN does not support multiple tops. Using Caffe's own pooling layer.
I0523 15:14:00.193758 21880 net.cpp:91] Creating Layer pool1
I0523 15:14:00.193761 21880 net.cpp:425] pool1 <- conv1_2
I0523 15:14:00.193764 21880 net.cpp:399] pool1 -> pool1
I0523 15:14:00.193771 21880 net.cpp:399] pool1 -> pool1_mask
I0523 15:14:00.193806 21880 net.cpp:141] Setting up pool1
I0523 15:14:00.193810 21880 net.cpp:148] Top shape: 1 32 112 112 (401408)
I0523 15:14:00.193814 21880 net.cpp:148] Top shape: 1 32 112 112 (401408)
I0523 15:14:00.193815 21880 net.cpp:156] Memory required for data: 55394304
I0523 15:14:00.193817 21880 layer_factory.hpp:77] Creating layer conv2_1
I0523 15:14:00.193825 21880 net.cpp:91] Creating Layer conv2_1
I0523 15:14:00.193826 21880 net.cpp:425] conv2_1 <- pool1
I0523 15:14:00.193830 21880 net.cpp:399] conv2_1 -> conv2_1
I0523 15:14:00.196733 21880 net.cpp:141] Setting up conv2_1
I0523 15:14:00.196759 21880 net.cpp:148] Top shape: 1 64 112 112 (802816)
I0523 15:14:00.196761 21880 net.cpp:156] Memory required for data: 58605568
I0523 15:14:00.196768 21880 layer_factory.hpp:77] Creating layer bn2_1
I0523 15:14:00.196776 21880 net.cpp:91] Creating Layer bn2_1
I0523 15:14:00.196779 21880 net.cpp:425] bn2_1 <- conv2_1
I0523 15:14:00.196784 21880 net.cpp:386] bn2_1 -> conv2_1 (in-place)
I0523 15:14:00.196949 21880 net.cpp:141] Setting up bn2_1
I0523 15:14:00.196954 21880 net.cpp:148] Top shape: 1 64 112 112 (802816)
I0523 15:14:00.196967 21880 net.cpp:156] Memory required for data: 61816832
I0523 15:14:00.196972 21880 layer_factory.hpp:77] Creating layer scale2_1
I0523 15:14:00.196979 21880 net.cpp:91] Creating Layer scale2_1
I0523 15:14:00.196981 21880 net.cpp:425] scale2_1 <- conv2_1
I0523 15:14:00.196986 21880 net.cpp:386] scale2_1 -> conv2_1 (in-place)
I0523 15:14:00.197017 21880 layer_factory.hpp:77] Creating layer scale2_1
I0523 15:14:00.197160 21880 net.cpp:141] Setting up scale2_1
I0523 15:14:00.197166 21880 net.cpp:148] Top shape: 1 64 112 112 (802816)
I0523 15:14:00.197180 21880 net.cpp:156] Memory required for data: 65028096
I0523 15:14:00.197188 21880 layer_factory.hpp:77] Creating layer relu2_1
I0523 15:14:00.197194 21880 net.cpp:91] Creating Layer relu2_1
I0523 15:14:00.197196 21880 net.cpp:425] relu2_1 <- conv2_1
I0523 15:14:00.197199 21880 net.cpp:386] relu2_1 -> conv2_1 (in-place)
I0523 15:14:00.198616 21880 net.cpp:141] Setting up relu2_1
I0523 15:14:00.198635 21880 net.cpp:148] Top shape: 1 64 112 112 (802816)
I0523 15:14:00.198638 21880 net.cpp:156] Memory required for data: 68239360
I0523 15:14:00.198640 21880 layer_factory.hpp:77] Creating layer conv2_2
I0523 15:14:00.198649 21880 net.cpp:91] Creating Layer conv2_2
I0523 15:14:00.198652 21880 net.cpp:425] conv2_2 <- conv2_1
I0523 15:14:00.198657 21880 net.cpp:399] conv2_2 -> conv2_2
I0523 15:14:00.200326 21880 net.cpp:141] Setting up conv2_2
I0523 15:14:00.200335 21880 net.cpp:148] Top shape: 1 64 112 112 (802816)
I0523 15:14:00.200348 21880 net.cpp:156] Memory required for data: 71450624
I0523 15:14:00.200353 21880 layer_factory.hpp:77] Creating layer bn2_2
I0523 15:14:00.200373 21880 net.cpp:91] Creating Layer bn2_2
I0523 15:14:00.200376 21880 net.cpp:425] bn2_2 <- conv2_2
I0523 15:14:00.200381 21880 net.cpp:386] bn2_2 -> conv2_2 (in-place)
I0523 15:14:00.200530 21880 net.cpp:141] Setting up bn2_2
I0523 15:14:00.200534 21880 net.cpp:148] Top shape: 1 64 112 112 (802816)
I0523 15:14:00.200547 21880 net.cpp:156] Memory required for data: 74661888
I0523 15:14:00.200552 21880 layer_factory.hpp:77] Creating layer scale2_2
I0523 15:14:00.200558 21880 net.cpp:91] Creating Layer scale2_2
I0523 15:14:00.200561 21880 net.cpp:425] scale2_2 <- conv2_2
I0523 15:14:00.200563 21880 net.cpp:386] scale2_2 -> conv2_2 (in-place)
I0523 15:14:00.200592 21880 layer_factory.hpp:77] Creating layer scale2_2
I0523 15:14:00.200714 21880 net.cpp:141] Setting up scale2_2
I0523 15:14:00.200719 21880 net.cpp:148] Top shape: 1 64 112 112 (802816)
I0523 15:14:00.200731 21880 net.cpp:156] Memory required for data: 77873152
I0523 15:14:00.200736 21880 layer_factory.hpp:77] Creating layer relu2_2
I0523 15:14:00.200741 21880 net.cpp:91] Creating Layer relu2_2
I0523 15:14:00.200742 21880 net.cpp:425] relu2_2 <- conv2_2
I0523 15:14:00.200747 21880 net.cpp:386] relu2_2 -> conv2_2 (in-place)
I0523 15:14:00.200968 21880 net.cpp:141] Setting up relu2_2
I0523 15:14:00.200976 21880 net.cpp:148] Top shape: 1 64 112 112 (802816)
I0523 15:14:00.200989 21880 net.cpp:156] Memory required for data: 81084416
I0523 15:14:00.200991 21880 layer_factory.hpp:77] Creating layer pool2
I0523 15:14:00.200995 21880 layer_factory.cpp:90] cuDNN does not support multiple tops. Using Caffe's own pooling layer.
I0523 15:14:00.201000 21880 net.cpp:91] Creating Layer pool2
I0523 15:14:00.201002 21880 net.cpp:425] pool2 <- conv2_2
I0523 15:14:00.201006 21880 net.cpp:399] pool2 -> pool2
I0523 15:14:00.201011 21880 net.cpp:399] pool2 -> pool2_mask
I0523 15:14:00.201052 21880 net.cpp:141] Setting up pool2
I0523 15:14:00.201057 21880 net.cpp:148] Top shape: 1 64 56 56 (200704)
I0523 15:14:00.201071 21880 net.cpp:148] Top shape: 1 64 56 56 (200704)
I0523 15:14:00.201071 21880 net.cpp:156] Memory required for data: 82690048
I0523 15:14:00.201074 21880 layer_factory.hpp:77] Creating layer conv3_1
I0523 15:14:00.201081 21880 net.cpp:91] Creating Layer conv3_1
I0523 15:14:00.201082 21880 net.cpp:425] conv3_1 <- pool2
I0523 15:14:00.201086 21880 net.cpp:399] conv3_1 -> conv3_1
I0523 15:14:00.203989 21880 net.cpp:141] Setting up conv3_1
I0523 15:14:00.204010 21880 net.cpp:148] Top shape: 1 128 56 56 (401408)
I0523 15:14:00.204015 21880 net.cpp:156] Memory required for data: 84295680
I0523 15:14:00.204018 21880 layer_factory.hpp:77] Creating layer bn3_1
I0523 15:14:00.204023 21880 net.cpp:91] Creating Layer bn3_1
I0523 15:14:00.204026 21880 net.cpp:425] bn3_1 <- conv3_1
I0523 15:14:00.204031 21880 net.cpp:386] bn3_1 -> conv3_1 (in-place)
I0523 15:14:00.204447 21880 net.cpp:141] Setting up bn3_1
I0523 15:14:00.204454 21880 net.cpp:148] Top shape: 1 128 56 56 (401408)
I0523 15:14:00.204468 21880 net.cpp:156] Memory required for data: 85901312
I0523 15:14:00.204473 21880 layer_factory.hpp:77] Creating layer scale3_1
I0523 15:14:00.204478 21880 net.cpp:91] Creating Layer scale3_1
I0523 15:14:00.204481 21880 net.cpp:425] scale3_1 <- conv3_1
I0523 15:14:00.204488 21880 net.cpp:386] scale3_1 -> conv3_1 (in-place)
I0523 15:14:00.204516 21880 layer_factory.hpp:77] Creating layer scale3_1
I0523 15:14:00.204603 21880 net.cpp:141] Setting up scale3_1
I0523 15:14:00.204607 21880 net.cpp:148] Top shape: 1 128 56 56 (401408)
I0523 15:14:00.204619 21880 net.cpp:156] Memory required for data: 87506944
I0523 15:14:00.204623 21880 layer_factory.hpp:77] Creating layer relu3_1
I0523 15:14:00.204628 21880 net.cpp:91] Creating Layer relu3_1
I0523 15:14:00.204630 21880 net.cpp:425] relu3_1 <- conv3_1
I0523 15:14:00.204633 21880 net.cpp:386] relu3_1 -> conv3_1 (in-place)
I0523 15:14:00.204772 21880 net.cpp:141] Setting up relu3_1
I0523 15:14:00.204778 21880 net.cpp:148] Top shape: 1 128 56 56 (401408)
I0523 15:14:00.204790 21880 net.cpp:156] Memory required for data: 89112576
I0523 15:14:00.204802 21880 layer_factory.hpp:77] Creating layer conv3_2
I0523 15:14:00.204809 21880 net.cpp:91] Creating Layer conv3_2
I0523 15:14:00.204812 21880 net.cpp:425] conv3_2 <- conv3_1
I0523 15:14:00.204816 21880 net.cpp:399] conv3_2 -> conv3_2
I0523 15:14:00.209257 21880 net.cpp:141] Setting up conv3_2
I0523 15:14:00.209280 21880 net.cpp:148] Top shape: 1 128 56 56 (401408)
I0523 15:14:00.209282 21880 net.cpp:156] Memory required for data: 90718208
I0523 15:14:00.209287 21880 layer_factory.hpp:77] Creating layer bn3_2
I0523 15:14:00.209293 21880 net.cpp:91] Creating Layer bn3_2
I0523 15:14:00.209296 21880 net.cpp:425] bn3_2 <- conv3_2
I0523 15:14:00.209300 21880 net.cpp:386] bn3_2 -> conv3_2 (in-place)
I0523 15:14:00.209449 21880 net.cpp:141] Setting up bn3_2
I0523 15:14:00.209453 21880 net.cpp:148] Top shape: 1 128 56 56 (401408)
I0523 15:14:00.209465 21880 net.cpp:156] Memory required for data: 92323840
I0523 15:14:00.209475 21880 layer_factory.hpp:77] Creating layer scale3_2
I0523 15:14:00.209481 21880 net.cpp:91] Creating Layer scale3_2
I0523 15:14:00.209484 21880 net.cpp:425] scale3_2 <- conv3_2
I0523 15:14:00.209487 21880 net.cpp:386] scale3_2 -> conv3_2 (in-place)
I0523 15:14:00.209516 21880 layer_factory.hpp:77] Creating layer scale3_2
I0523 15:14:00.209606 21880 net.cpp:141] Setting up scale3_2
I0523 15:14:00.209610 21880 net.cpp:148] Top shape: 1 128 56 56 (401408)
I0523 15:14:00.209624 21880 net.cpp:156] Memory required for data: 93929472
I0523 15:14:00.209627 21880 layer_factory.hpp:77] Creating layer relu3_2
I0523 15:14:00.209632 21880 net.cpp:91] Creating Layer relu3_2
I0523 15:14:00.209635 21880 net.cpp:425] relu3_2 <- conv3_2
I0523 15:14:00.209638 21880 net.cpp:386] relu3_2 -> conv3_2 (in-place)
I0523 15:14:00.209780 21880 net.cpp:141] Setting up relu3_2
I0523 15:14:00.209787 21880 net.cpp:148] Top shape: 1 128 56 56 (401408)
I0523 15:14:00.209800 21880 net.cpp:156] Memory required for data: 95535104
I0523 15:14:00.209802 21880 layer_factory.hpp:77] Creating layer pool3
I0523 15:14:00.209806 21880 layer_factory.cpp:90] cuDNN does not support multiple tops. Using Caffe's own pooling layer.
I0523 15:14:00.209810 21880 net.cpp:91] Creating Layer pool3
I0523 15:14:00.209812 21880 net.cpp:425] pool3 <- conv3_2
I0523 15:14:00.209815 21880 net.cpp:399] pool3 -> pool3
I0523 15:14:00.209820 21880 net.cpp:399] pool3 -> pool3_mask
I0523 15:14:00.209852 21880 net.cpp:141] Setting up pool3
I0523 15:14:00.209867 21880 net.cpp:148] Top shape: 1 128 28 28 (100352)
I0523 15:14:00.209868 21880 net.cpp:148] Top shape: 1 128 28 28 (100352)
I0523 15:14:00.209870 21880 net.cpp:156] Memory required for data: 96337920
I0523 15:14:00.209883 21880 layer_factory.hpp:77] Creating layer conv4_1
I0523 15:14:00.209889 21880 net.cpp:91] Creating Layer conv4_1
I0523 15:14:00.209892 21880 net.cpp:425] conv4_1 <- pool3
I0523 15:14:00.209897 21880 net.cpp:399] conv4_1 -> conv4_1
I0523 15:14:00.217905 21880 net.cpp:141] Setting up conv4_1
I0523 15:14:00.217919 21880 net.cpp:148] Top shape: 1 256 28 28 (200704)
I0523 15:14:00.217921 21880 net.cpp:156] Memory required for data: 97140736
I0523 15:14:00.217936 21880 layer_factory.hpp:77] Creating layer bn4_1
I0523 15:14:00.217943 21880 net.cpp:91] Creating Layer bn4_1
I0523 15:14:00.217947 21880 net.cpp:425] bn4_1 <- conv4_1
I0523 15:14:00.217952 21880 net.cpp:386] bn4_1 -> conv4_1 (in-place)
I0523 15:14:00.218107 21880 net.cpp:141] Setting up bn4_1
I0523 15:14:00.218112 21880 net.cpp:148] Top shape: 1 256 28 28 (200704)
I0523 15:14:00.218114 21880 net.cpp:156] Memory required for data: 97943552
I0523 15:14:00.218119 21880 layer_factory.hpp:77] Creating layer scale4_1
I0523 15:14:00.218127 21880 net.cpp:91] Creating Layer scale4_1
I0523 15:14:00.218129 21880 net.cpp:425] scale4_1 <- conv4_1
I0523 15:14:00.218132 21880 net.cpp:386] scale4_1 -> conv4_1 (in-place)
I0523 15:14:00.218163 21880 layer_factory.hpp:77] Creating layer scale4_1
I0523 15:14:00.218246 21880 net.cpp:141] Setting up scale4_1
I0523 15:14:00.218251 21880 net.cpp:148] Top shape: 1 256 28 28 (200704)
I0523 15:14:00.218261 21880 net.cpp:156] Memory required for data: 98746368
I0523 15:14:00.218266 21880 layer_factory.hpp:77] Creating layer relu4_1
I0523 15:14:00.218274 21880 net.cpp:91] Creating Layer relu4_1
I0523 15:14:00.218276 21880 net.cpp:425] relu4_1 <- conv4_1
I0523 15:14:00.218281 21880 net.cpp:386] relu4_1 -> conv4_1 (in-place)
I0523 15:14:00.218508 21880 net.cpp:141] Setting up relu4_1
I0523 15:14:00.218518 21880 net.cpp:148] Top shape: 1 256 28 28 (200704)
I0523 15:14:00.218530 21880 net.cpp:156] Memory required for data: 99549184
I0523 15:14:00.218533 21880 layer_factory.hpp:77] Creating layer conv4_2
I0523 15:14:00.218541 21880 net.cpp:91] Creating Layer conv4_2
I0523 15:14:00.218544 21880 net.cpp:425] conv4_2 <- conv4_1
I0523 15:14:00.218549 21880 net.cpp:399] conv4_2 -> conv4_2
I0523 15:14:00.234134 21880 net.cpp:141] Setting up conv4_2
I0523 15:14:00.234163 21880 net.cpp:148] Top shape: 1 256 28 28 (200704)
I0523 15:14:00.234166 21880 net.cpp:156] Memory required for data: 100352000
I0523 15:14:00.234174 21880 layer_factory.hpp:77] Creating layer bn4_2
I0523 15:14:00.234184 21880 net.cpp:91] Creating Layer bn4_2
I0523 15:14:00.234189 21880 net.cpp:425] bn4_2 <- conv4_2
I0523 15:14:00.234194 21880 net.cpp:386] bn4_2 -> conv4_2 (in-place)
I0523 15:14:00.234386 21880 net.cpp:141] Setting up bn4_2
I0523 15:14:00.234392 21880 net.cpp:148] Top shape: 1 256 28 28 (200704)
I0523 15:14:00.234405 21880 net.cpp:156] Memory required for data: 101154816
I0523 15:14:00.234411 21880 layer_factory.hpp:77] Creating layer scale4_2
I0523 15:14:00.234421 21880 net.cpp:91] Creating Layer scale4_2
I0523 15:14:00.234424 21880 net.cpp:425] scale4_2 <- conv4_2
I0523 15:14:00.234428 21880 net.cpp:386] scale4_2 -> conv4_2 (in-place)
I0523 15:14:00.234493 21880 layer_factory.hpp:77] Creating layer scale4_2
I0523 15:14:00.234596 21880 net.cpp:141] Setting up scale4_2
I0523 15:14:00.234602 21880 net.cpp:148] Top shape: 1 256 28 28 (200704)
I0523 15:14:00.234616 21880 net.cpp:156] Memory required for data: 101957632
I0523 15:14:00.234619 21880 layer_factory.hpp:77] Creating layer relu4_2
I0523 15:14:00.234624 21880 net.cpp:91] Creating Layer relu4_2
I0523 15:14:00.234627 21880 net.cpp:425] relu4_2 <- conv4_2
I0523 15:14:00.234630 21880 net.cpp:386] relu4_2 -> conv4_2 (in-place)
I0523 15:14:00.234771 21880 net.cpp:141] Setting up relu4_2
I0523 15:14:00.234777 21880 net.cpp:148] Top shape: 1 256 28 28 (200704)
I0523 15:14:00.234791 21880 net.cpp:156] Memory required for data: 102760448
I0523 15:14:00.234792 21880 layer_factory.hpp:77] Creating layer pool4
I0523 15:14:00.234796 21880 layer_factory.cpp:90] cuDNN does not support multiple tops. Using Caffe's own pooling layer.
I0523 15:14:00.234799 21880 net.cpp:91] Creating Layer pool4
I0523 15:14:00.234802 21880 net.cpp:425] pool4 <- conv4_2
I0523 15:14:00.234805 21880 net.cpp:399] pool4 -> pool4
I0523 15:14:00.234812 21880 net.cpp:399] pool4 -> pool4_mask
I0523 15:14:00.234843 21880 net.cpp:141] Setting up pool4
I0523 15:14:00.234846 21880 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0523 15:14:00.234849 21880 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0523 15:14:00.234851 21880 net.cpp:156] Memory required for data: 103161856
I0523 15:14:00.234853 21880 layer_factory.hpp:77] Creating layer conv5_1
I0523 15:14:00.234860 21880 net.cpp:91] Creating Layer conv5_1
I0523 15:14:00.234863 21880 net.cpp:425] conv5_1 <- pool4
I0523 15:14:00.234868 21880 net.cpp:399] conv5_1 -> conv5_1
I0523 15:14:00.249790 21880 net.cpp:141] Setting up conv5_1
I0523 15:14:00.249819 21880 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0523 15:14:00.249821 21880 net.cpp:156] Memory required for data: 103362560
I0523 15:14:00.249827 21880 layer_factory.hpp:77] Creating layer bn5_1
I0523 15:14:00.249835 21880 net.cpp:91] Creating Layer bn5_1
I0523 15:14:00.249840 21880 net.cpp:425] bn5_1 <- conv5_1
I0523 15:14:00.249845 21880 net.cpp:386] bn5_1 -> conv5_1 (in-place)
I0523 15:14:00.250025 21880 net.cpp:141] Setting up bn5_1
I0523 15:14:00.250035 21880 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0523 15:14:00.250049 21880 net.cpp:156] Memory required for data: 103563264
I0523 15:14:00.250059 21880 layer_factory.hpp:77] Creating layer scale5_1
I0523 15:14:00.250069 21880 net.cpp:91] Creating Layer scale5_1
I0523 15:14:00.250074 21880 net.cpp:425] scale5_1 <- conv5_1
I0523 15:14:00.250080 21880 net.cpp:386] scale5_1 -> conv5_1 (in-place)
I0523 15:14:00.250129 21880 layer_factory.hpp:77] Creating layer scale5_1
I0523 15:14:00.250219 21880 net.cpp:141] Setting up scale5_1
I0523 15:14:00.250224 21880 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0523 15:14:00.250226 21880 net.cpp:156] Memory required for data: 103763968
I0523 15:14:00.250241 21880 layer_factory.hpp:77] Creating layer relu5_1
I0523 15:14:00.250247 21880 net.cpp:91] Creating Layer relu5_1
I0523 15:14:00.250248 21880 net.cpp:425] relu5_1 <- conv5_1
I0523 15:14:00.250252 21880 net.cpp:386] relu5_1 -> conv5_1 (in-place)
I0523 15:14:00.250393 21880 net.cpp:141] Setting up relu5_1
I0523 15:14:00.250399 21880 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0523 15:14:00.250411 21880 net.cpp:156] Memory required for data: 103964672
I0523 15:14:00.250414 21880 layer_factory.hpp:77] Creating layer conv5_2
I0523 15:14:00.250422 21880 net.cpp:91] Creating Layer conv5_2
I0523 15:14:00.250424 21880 net.cpp:425] conv5_2 <- conv5_1
I0523 15:14:00.250428 21880 net.cpp:399] conv5_2 -> conv5_2
I0523 15:14:00.265666 21880 net.cpp:141] Setting up conv5_2
I0523 15:14:00.265696 21880 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0523 15:14:00.265698 21880 net.cpp:156] Memory required for data: 104165376
I0523 15:14:00.265705 21880 layer_factory.hpp:77] Creating layer bn5_2
I0523 15:14:00.265717 21880 net.cpp:91] Creating Layer bn5_2
I0523 15:14:00.265720 21880 net.cpp:425] bn5_2 <- conv5_2
I0523 15:14:00.265727 21880 net.cpp:386] bn5_2 -> conv5_2 (in-place)
I0523 15:14:00.265890 21880 net.cpp:141] Setting up bn5_2
I0523 15:14:00.265897 21880 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0523 15:14:00.265908 21880 net.cpp:156] Memory required for data: 104366080
I0523 15:14:00.265914 21880 layer_factory.hpp:77] Creating layer scale5_2
I0523 15:14:00.265921 21880 net.cpp:91] Creating Layer scale5_2
I0523 15:14:00.265923 21880 net.cpp:425] scale5_2 <- conv5_2
I0523 15:14:00.265928 21880 net.cpp:386] scale5_2 -> conv5_2 (in-place)
I0523 15:14:00.265957 21880 layer_factory.hpp:77] Creating layer scale5_2
I0523 15:14:00.266049 21880 net.cpp:141] Setting up scale5_2
I0523 15:14:00.266053 21880 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0523 15:14:00.266065 21880 net.cpp:156] Memory required for data: 104566784
I0523 15:14:00.266070 21880 layer_factory.hpp:77] Creating layer relu5_2
I0523 15:14:00.266074 21880 net.cpp:91] Creating Layer relu5_2
I0523 15:14:00.266077 21880 net.cpp:425] relu5_2 <- conv5_2
I0523 15:14:00.266079 21880 net.cpp:386] relu5_2 -> conv5_2 (in-place)
I0523 15:14:00.266331 21880 net.cpp:141] Setting up relu5_2
I0523 15:14:00.266351 21880 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0523 15:14:00.266355 21880 net.cpp:156] Memory required for data: 104767488
I0523 15:14:00.266356 21880 layer_factory.hpp:77] Creating layer pool5
I0523 15:14:00.266360 21880 layer_factory.cpp:90] cuDNN does not support multiple tops. Using Caffe's own pooling layer.
I0523 15:14:00.266365 21880 net.cpp:91] Creating Layer pool5
I0523 15:14:00.266366 21880 net.cpp:425] pool5 <- conv5_2
I0523 15:14:00.266371 21880 net.cpp:399] pool5 -> pool5
I0523 15:14:00.266376 21880 net.cpp:399] pool5 -> pool5_mask
I0523 15:14:00.266412 21880 net.cpp:141] Setting up pool5
I0523 15:14:00.266417 21880 net.cpp:148] Top shape: 1 256 7 7 (12544)
I0523 15:14:00.266420 21880 net.cpp:148] Top shape: 1 256 7 7 (12544)
I0523 15:14:00.266422 21880 net.cpp:156] Memory required for data: 104867840
I0523 15:14:00.266424 21880 layer_factory.hpp:77] Creating layer upsample5
I0523 15:14:00.266430 21880 net.cpp:91] Creating Layer upsample5
I0523 15:14:00.266433 21880 net.cpp:425] upsample5 <- pool5
I0523 15:14:00.266435 21880 net.cpp:425] upsample5 <- pool5_mask
I0523 15:14:00.266438 21880 net.cpp:399] upsample5 -> pool5_D
I0523 15:14:00.266477 21880 net.cpp:141] Setting up upsample5
I0523 15:14:00.266496 21880 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0523 15:14:00.266500 21880 net.cpp:156] Memory required for data: 105068544
I0523 15:14:00.266504 21880 layer_factory.hpp:77] Creating layer conv5_2_D
I0523 15:14:00.266511 21880 net.cpp:91] Creating Layer conv5_2_D
I0523 15:14:00.266515 21880 net.cpp:425] conv5_2_D <- pool5_D
I0523 15:14:00.266518 21880 net.cpp:399] conv5_2_D -> conv5_2_D
I0523 15:14:00.281366 21880 net.cpp:141] Setting up conv5_2_D
I0523 15:14:00.281390 21880 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0523 15:14:00.281394 21880 net.cpp:156] Memory required for data: 105269248
I0523 15:14:00.281399 21880 layer_factory.hpp:77] Creating layer bn5_2_D
I0523 15:14:00.281407 21880 net.cpp:91] Creating Layer bn5_2_D
I0523 15:14:00.281410 21880 net.cpp:425] bn5_2_D <- conv5_2_D
I0523 15:14:00.281414 21880 net.cpp:386] bn5_2_D -> conv5_2_D (in-place)
I0523 15:14:00.281569 21880 net.cpp:141] Setting up bn5_2_D
I0523 15:14:00.281574 21880 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0523 15:14:00.281576 21880 net.cpp:156] Memory required for data: 105469952
I0523 15:14:00.281581 21880 layer_factory.hpp:77] Creating layer scale5_2_D
I0523 15:14:00.281587 21880 net.cpp:91] Creating Layer scale5_2_D
I0523 15:14:00.281589 21880 net.cpp:425] scale5_2_D <- conv5_2_D
I0523 15:14:00.281595 21880 net.cpp:386] scale5_2_D -> conv5_2_D (in-place)
I0523 15:14:00.281625 21880 layer_factory.hpp:77] Creating layer scale5_2_D
I0523 15:14:00.281707 21880 net.cpp:141] Setting up scale5_2_D
I0523 15:14:00.281711 21880 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0523 15:14:00.281714 21880 net.cpp:156] Memory required for data: 105670656
I0523 15:14:00.281728 21880 layer_factory.hpp:77] Creating layer relu5_2_D
I0523 15:14:00.281733 21880 net.cpp:91] Creating Layer relu5_2_D
I0523 15:14:00.281734 21880 net.cpp:425] relu5_2_D <- conv5_2_D
I0523 15:14:00.281738 21880 net.cpp:386] relu5_2_D -> conv5_2_D (in-place)
I0523 15:14:00.281875 21880 net.cpp:141] Setting up relu5_2_D
I0523 15:14:00.281883 21880 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0523 15:14:00.281884 21880 net.cpp:156] Memory required for data: 105871360
I0523 15:14:00.281886 21880 layer_factory.hpp:77] Creating layer conv5_1_D
I0523 15:14:00.281894 21880 net.cpp:91] Creating Layer conv5_1_D
I0523 15:14:00.281898 21880 net.cpp:425] conv5_1_D <- conv5_2_D
I0523 15:14:00.281901 21880 net.cpp:399] conv5_1_D -> conv5_1_D
I0523 15:14:00.297214 21880 net.cpp:141] Setting up conv5_1_D
I0523 15:14:00.297242 21880 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0523 15:14:00.297245 21880 net.cpp:156] Memory required for data: 106072064
I0523 15:14:00.297252 21880 layer_factory.hpp:77] Creating layer bn5_1_D
I0523 15:14:00.297260 21880 net.cpp:91] Creating Layer bn5_1_D
I0523 15:14:00.297265 21880 net.cpp:425] bn5_1_D <- conv5_1_D
I0523 15:14:00.297271 21880 net.cpp:386] bn5_1_D -> conv5_1_D (in-place)
I0523 15:14:00.297435 21880 net.cpp:141] Setting up bn5_1_D
I0523 15:14:00.297441 21880 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0523 15:14:00.297452 21880 net.cpp:156] Memory required for data: 106272768
I0523 15:14:00.297457 21880 layer_factory.hpp:77] Creating layer scale5_1_D
I0523 15:14:00.297464 21880 net.cpp:91] Creating Layer scale5_1_D
I0523 15:14:00.297466 21880 net.cpp:425] scale5_1_D <- conv5_1_D
I0523 15:14:00.297471 21880 net.cpp:386] scale5_1_D -> conv5_1_D (in-place)
I0523 15:14:00.297500 21880 layer_factory.hpp:77] Creating layer scale5_1_D
I0523 15:14:00.297591 21880 net.cpp:141] Setting up scale5_1_D
I0523 15:14:00.297597 21880 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0523 15:14:00.297610 21880 net.cpp:156] Memory required for data: 106473472
I0523 15:14:00.297613 21880 layer_factory.hpp:77] Creating layer relu5_1_D
I0523 15:14:00.297618 21880 net.cpp:91] Creating Layer relu5_1_D
I0523 15:14:00.297621 21880 net.cpp:425] relu5_1_D <- conv5_1_D
I0523 15:14:00.297624 21880 net.cpp:386] relu5_1_D -> conv5_1_D (in-place)
I0523 15:14:00.297767 21880 net.cpp:141] Setting up relu5_1_D
I0523 15:14:00.297785 21880 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0523 15:14:00.297797 21880 net.cpp:156] Memory required for data: 106674176
I0523 15:14:00.297801 21880 layer_factory.hpp:77] Creating layer upsample4
I0523 15:14:00.297806 21880 net.cpp:91] Creating Layer upsample4
I0523 15:14:00.297808 21880 net.cpp:425] upsample4 <- conv5_1_D
I0523 15:14:00.297812 21880 net.cpp:425] upsample4 <- pool4_mask
I0523 15:14:00.297816 21880 net.cpp:399] upsample4 -> pool4_D
I0523 15:14:00.297839 21880 net.cpp:141] Setting up upsample4
I0523 15:14:00.297842 21880 net.cpp:148] Top shape: 1 256 28 28 (200704)
I0523 15:14:00.297844 21880 net.cpp:156] Memory required for data: 107476992
I0523 15:14:00.297847 21880 layer_factory.hpp:77] Creating layer conv4_2_D
I0523 15:14:00.297854 21880 net.cpp:91] Creating Layer conv4_2_D
I0523 15:14:00.297857 21880 net.cpp:425] conv4_2_D <- pool4_D
I0523 15:14:00.297862 21880 net.cpp:399] conv4_2_D -> conv4_2_D
I0523 15:14:00.312733 21880 net.cpp:141] Setting up conv4_2_D
I0523 15:14:00.312760 21880 net.cpp:148] Top shape: 1 256 28 28 (200704)
I0523 15:14:00.312763 21880 net.cpp:156] Memory required for data: 108279808
I0523 15:14:00.312769 21880 layer_factory.hpp:77] Creating layer bn4_2_D
I0523 15:14:00.312778 21880 net.cpp:91] Creating Layer bn4_2_D
I0523 15:14:00.312783 21880 net.cpp:425] bn4_2_D <- conv4_2_D
I0523 15:14:00.312788 21880 net.cpp:386] bn4_2_D -> conv4_2_D (in-place)
I0523 15:14:00.312990 21880 net.cpp:141] Setting up bn4_2_D
I0523 15:14:00.312997 21880 net.cpp:148] Top shape: 1 256 28 28 (200704)
I0523 15:14:00.312999 21880 net.cpp:156] Memory required for data: 109082624
I0523 15:14:00.313005 21880 layer_factory.hpp:77] Creating layer scale4_2_D
I0523 15:14:00.313012 21880 net.cpp:91] Creating Layer scale4_2_D
I0523 15:14:00.313015 21880 net.cpp:425] scale4_2_D <- conv4_2_D
I0523 15:14:00.313019 21880 net.cpp:386] scale4_2_D -> conv4_2_D (in-place)
I0523 15:14:00.313051 21880 layer_factory.hpp:77] Creating layer scale4_2_D
I0523 15:14:00.313144 21880 net.cpp:141] Setting up scale4_2_D
I0523 15:14:00.313149 21880 net.cpp:148] Top shape: 1 256 28 28 (200704)
I0523 15:14:00.313153 21880 net.cpp:156] Memory required for data: 109885440
I0523 15:14:00.313156 21880 layer_factory.hpp:77] Creating layer relu4_2_D
I0523 15:14:00.313161 21880 net.cpp:91] Creating Layer relu4_2_D
I0523 15:14:00.313163 21880 net.cpp:425] relu4_2_D <- conv4_2_D
I0523 15:14:00.313166 21880 net.cpp:386] relu4_2_D -> conv4_2_D (in-place)
I0523 15:14:00.313419 21880 net.cpp:141] Setting up relu4_2_D
I0523 15:14:00.313428 21880 net.cpp:148] Top shape: 1 256 28 28 (200704)
I0523 15:14:00.313442 21880 net.cpp:156] Memory required for data: 110688256
I0523 15:14:00.313446 21880 layer_factory.hpp:77] Creating layer conv4_1_D
I0523 15:14:00.313453 21880 net.cpp:91] Creating Layer conv4_1_D
I0523 15:14:00.313457 21880 net.cpp:425] conv4_1_D <- conv4_2_D
I0523 15:14:00.313462 21880 net.cpp:399] conv4_1_D -> conv4_1_D
I0523 15:14:00.321692 21880 net.cpp:141] Setting up conv4_1_D
I0523 15:14:00.321717 21880 net.cpp:148] Top shape: 1 128 28 28 (100352)
I0523 15:14:00.321720 21880 net.cpp:156] Memory required for data: 111089664
I0523 15:14:00.321727 21880 layer_factory.hpp:77] Creating layer bn4_1_D
I0523 15:14:00.321734 21880 net.cpp:91] Creating Layer bn4_1_D
I0523 15:14:00.321738 21880 net.cpp:425] bn4_1_D <- conv4_1_D
I0523 15:14:00.321743 21880 net.cpp:386] bn4_1_D -> conv4_1_D (in-place)
I0523 15:14:00.321918 21880 net.cpp:141] Setting up bn4_1_D
I0523 15:14:00.321923 21880 net.cpp:148] Top shape: 1 128 28 28 (100352)
I0523 15:14:00.321935 21880 net.cpp:156] Memory required for data: 111491072
I0523 15:14:00.321940 21880 layer_factory.hpp:77] Creating layer scale4_1_D
I0523 15:14:00.321948 21880 net.cpp:91] Creating Layer scale4_1_D
I0523 15:14:00.321950 21880 net.cpp:425] scale4_1_D <- conv4_1_D
I0523 15:14:00.321954 21880 net.cpp:386] scale4_1_D -> conv4_1_D (in-place)
I0523 15:14:00.321985 21880 layer_factory.hpp:77] Creating layer scale4_1_D
I0523 15:14:00.322088 21880 net.cpp:141] Setting up scale4_1_D
I0523 15:14:00.322103 21880 net.cpp:148] Top shape: 1 128 28 28 (100352)
I0523 15:14:00.322115 21880 net.cpp:156] Memory required for data: 111892480
I0523 15:14:00.322119 21880 layer_factory.hpp:77] Creating layer relu4_1_D
I0523 15:14:00.322132 21880 net.cpp:91] Creating Layer relu4_1_D
I0523 15:14:00.322135 21880 net.cpp:425] relu4_1_D <- conv4_1_D
I0523 15:14:00.322139 21880 net.cpp:386] relu4_1_D -> conv4_1_D (in-place)
I0523 15:14:00.322285 21880 net.cpp:141] Setting up relu4_1_D
I0523 15:14:00.322291 21880 net.cpp:148] Top shape: 1 128 28 28 (100352)
I0523 15:14:00.322304 21880 net.cpp:156] Memory required for data: 112293888
I0523 15:14:00.322306 21880 layer_factory.hpp:77] Creating layer upsample3
I0523 15:14:00.322312 21880 net.cpp:91] Creating Layer upsample3
I0523 15:14:00.322315 21880 net.cpp:425] upsample3 <- conv4_1_D
I0523 15:14:00.322319 21880 net.cpp:425] upsample3 <- pool3_mask
I0523 15:14:00.322321 21880 net.cpp:399] upsample3 -> pool3_D
I0523 15:14:00.322345 21880 net.cpp:141] Setting up upsample3
I0523 15:14:00.322348 21880 net.cpp:148] Top shape: 1 128 56 56 (401408)
I0523 15:14:00.322350 21880 net.cpp:156] Memory required for data: 113899520
I0523 15:14:00.322352 21880 layer_factory.hpp:77] Creating layer conv3_2_D
I0523 15:14:00.322360 21880 net.cpp:91] Creating Layer conv3_2_D
I0523 15:14:00.322362 21880 net.cpp:425] conv3_2_D <- pool3_D
I0523 15:14:00.322367 21880 net.cpp:399] conv3_2_D -> conv3_2_D
I0523 15:14:00.326982 21880 net.cpp:141] Setting up conv3_2_D
I0523 15:14:00.327003 21880 net.cpp:148] Top shape: 1 128 56 56 (401408)
I0523 15:14:00.327006 21880 net.cpp:156] Memory required for data: 115505152
I0523 15:14:00.327011 21880 layer_factory.hpp:77] Creating layer bn3_2_D
I0523 15:14:00.327018 21880 net.cpp:91] Creating Layer bn3_2_D
I0523 15:14:00.327021 21880 net.cpp:425] bn3_2_D <- conv3_2_D
I0523 15:14:00.327025 21880 net.cpp:386] bn3_2_D -> conv3_2_D (in-place)
I0523 15:14:00.327186 21880 net.cpp:141] Setting up bn3_2_D
I0523 15:14:00.327191 21880 net.cpp:148] Top shape: 1 128 56 56 (401408)
I0523 15:14:00.327204 21880 net.cpp:156] Memory required for data: 117110784
I0523 15:14:00.327210 21880 layer_factory.hpp:77] Creating layer scale3_2_D
I0523 15:14:00.327216 21880 net.cpp:91] Creating Layer scale3_2_D
I0523 15:14:00.327219 21880 net.cpp:425] scale3_2_D <- conv3_2_D
I0523 15:14:00.327221 21880 net.cpp:386] scale3_2_D -> conv3_2_D (in-place)
I0523 15:14:00.327251 21880 layer_factory.hpp:77] Creating layer scale3_2_D
I0523 15:14:00.327369 21880 net.cpp:141] Setting up scale3_2_D
I0523 15:14:00.327374 21880 net.cpp:148] Top shape: 1 128 56 56 (401408)
I0523 15:14:00.327376 21880 net.cpp:156] Memory required for data: 118716416
I0523 15:14:00.327390 21880 layer_factory.hpp:77] Creating layer relu3_2_D
I0523 15:14:00.327394 21880 net.cpp:91] Creating Layer relu3_2_D
I0523 15:14:00.327396 21880 net.cpp:425] relu3_2_D <- conv3_2_D
I0523 15:14:00.327400 21880 net.cpp:386] relu3_2_D -> conv3_2_D (in-place)
I0523 15:14:00.327541 21880 net.cpp:141] Setting up relu3_2_D
I0523 15:14:00.327548 21880 net.cpp:148] Top shape: 1 128 56 56 (401408)
I0523 15:14:00.327559 21880 net.cpp:156] Memory required for data: 120322048
I0523 15:14:00.327563 21880 layer_factory.hpp:77] Creating layer conv3_1_D
I0523 15:14:00.327569 21880 net.cpp:91] Creating Layer conv3_1_D
I0523 15:14:00.327571 21880 net.cpp:425] conv3_1_D <- conv3_2_D
I0523 15:14:00.327576 21880 net.cpp:399] conv3_1_D -> conv3_1_D
I0523 15:14:00.330291 21880 net.cpp:141] Setting up conv3_1_D
I0523 15:14:00.330312 21880 net.cpp:148] Top shape: 1 64 56 56 (200704)
I0523 15:14:00.330314 21880 net.cpp:156] Memory required for data: 121124864
I0523 15:14:00.330318 21880 layer_factory.hpp:77] Creating layer bn3_1_D
I0523 15:14:00.330323 21880 net.cpp:91] Creating Layer bn3_1_D
I0523 15:14:00.330327 21880 net.cpp:425] bn3_1_D <- conv3_1_D
I0523 15:14:00.330330 21880 net.cpp:386] bn3_1_D -> conv3_1_D (in-place)
I0523 15:14:00.330513 21880 net.cpp:141] Setting up bn3_1_D
I0523 15:14:00.330530 21880 net.cpp:148] Top shape: 1 64 56 56 (200704)
I0523 15:14:00.330543 21880 net.cpp:156] Memory required for data: 121927680
I0523 15:14:00.330549 21880 layer_factory.hpp:77] Creating layer scale3_1_D
I0523 15:14:00.330555 21880 net.cpp:91] Creating Layer scale3_1_D
I0523 15:14:00.330559 21880 net.cpp:425] scale3_1_D <- conv3_1_D
I0523 15:14:00.330561 21880 net.cpp:386] scale3_1_D -> conv3_1_D (in-place)
I0523 15:14:00.330605 21880 layer_factory.hpp:77] Creating layer scale3_1_D
I0523 15:14:00.330719 21880 net.cpp:141] Setting up scale3_1_D
I0523 15:14:00.330724 21880 net.cpp:148] Top shape: 1 64 56 56 (200704)
I0523 15:14:00.330736 21880 net.cpp:156] Memory required for data: 122730496
I0523 15:14:00.330740 21880 layer_factory.hpp:77] Creating layer relu3_1_D
I0523 15:14:00.330744 21880 net.cpp:91] Creating Layer relu3_1_D
I0523 15:14:00.330746 21880 net.cpp:425] relu3_1_D <- conv3_1_D
I0523 15:14:00.330750 21880 net.cpp:386] relu3_1_D -> conv3_1_D (in-place)
I0523 15:14:00.330974 21880 net.cpp:141] Setting up relu3_1_D
I0523 15:14:00.330982 21880 net.cpp:148] Top shape: 1 64 56 56 (200704)
I0523 15:14:00.330996 21880 net.cpp:156] Memory required for data: 123533312
I0523 15:14:00.330997 21880 layer_factory.hpp:77] Creating layer upsample2
I0523 15:14:00.331003 21880 net.cpp:91] Creating Layer upsample2
I0523 15:14:00.331006 21880 net.cpp:425] upsample2 <- conv3_1_D
I0523 15:14:00.331009 21880 net.cpp:425] upsample2 <- pool2_mask
I0523 15:14:00.331013 21880 net.cpp:399] upsample2 -> pool2_D
I0523 15:14:00.331037 21880 net.cpp:141] Setting up upsample2
I0523 15:14:00.331040 21880 net.cpp:148] Top shape: 1 64 112 112 (802816)
I0523 15:14:00.331043 21880 net.cpp:156] Memory required for data: 126744576
I0523 15:14:00.331044 21880 layer_factory.hpp:77] Creating layer conv2_2_D
I0523 15:14:00.331051 21880 net.cpp:91] Creating Layer conv2_2_D
I0523 15:14:00.331054 21880 net.cpp:425] conv2_2_D <- pool2_D
I0523 15:14:00.331058 21880 net.cpp:399] conv2_2_D -> conv2_2_D
I0523 15:14:00.332729 21880 net.cpp:141] Setting up conv2_2_D
I0523 15:14:00.332739 21880 net.cpp:148] Top shape: 1 64 112 112 (802816)
I0523 15:14:00.332741 21880 net.cpp:156] Memory required for data: 129955840
I0523 15:14:00.332746 21880 layer_factory.hpp:77] Creating layer bn2_2_D
I0523 15:14:00.332751 21880 net.cpp:91] Creating Layer bn2_2_D
I0523 15:14:00.332754 21880 net.cpp:425] bn2_2_D <- conv2_2_D
I0523 15:14:00.332759 21880 net.cpp:386] bn2_2_D -> conv2_2_D (in-place)
I0523 15:14:00.332933 21880 net.cpp:141] Setting up bn2_2_D
I0523 15:14:00.332938 21880 net.cpp:148] Top shape: 1 64 112 112 (802816)
I0523 15:14:00.332950 21880 net.cpp:156] Memory required for data: 133167104
I0523 15:14:00.332955 21880 layer_factory.hpp:77] Creating layer scale2_2_D
I0523 15:14:00.332960 21880 net.cpp:91] Creating Layer scale2_2_D
I0523 15:14:00.332962 21880 net.cpp:425] scale2_2_D <- conv2_2_D
I0523 15:14:00.332965 21880 net.cpp:386] scale2_2_D -> conv2_2_D (in-place)
I0523 15:14:00.333005 21880 layer_factory.hpp:77] Creating layer scale2_2_D
I0523 15:14:00.333118 21880 net.cpp:141] Setting up scale2_2_D
I0523 15:14:00.333123 21880 net.cpp:148] Top shape: 1 64 112 112 (802816)
I0523 15:14:00.333125 21880 net.cpp:156] Memory required for data: 136378368
I0523 15:14:00.333129 21880 layer_factory.hpp:77] Creating layer relu2_2_D
I0523 15:14:00.333134 21880 net.cpp:91] Creating Layer relu2_2_D
I0523 15:14:00.333137 21880 net.cpp:425] relu2_2_D <- conv2_2_D
I0523 15:14:00.333139 21880 net.cpp:386] relu2_2_D -> conv2_2_D (in-place)
I0523 15:14:00.333277 21880 net.cpp:141] Setting up relu2_2_D
I0523 15:14:00.333283 21880 net.cpp:148] Top shape: 1 64 112 112 (802816)
I0523 15:14:00.333295 21880 net.cpp:156] Memory required for data: 139589632
I0523 15:14:00.333298 21880 layer_factory.hpp:77] Creating layer conv2_1_D
I0523 15:14:00.333305 21880 net.cpp:91] Creating Layer conv2_1_D
I0523 15:14:00.333308 21880 net.cpp:425] conv2_1_D <- conv2_2_D
I0523 15:14:00.333312 21880 net.cpp:399] conv2_1_D -> conv2_1_D
I0523 15:14:00.334434 21880 net.cpp:141] Setting up conv2_1_D
I0523 15:14:00.334442 21880 net.cpp:148] Top shape: 1 32 112 112 (401408)
I0523 15:14:00.334463 21880 net.cpp:156] Memory required for data: 141195264
I0523 15:14:00.334468 21880 layer_factory.hpp:77] Creating layer bn2_1_D
I0523 15:14:00.334481 21880 net.cpp:91] Creating Layer bn2_1_D
I0523 15:14:00.334486 21880 net.cpp:425] bn2_1_D <- conv2_1_D
I0523 15:14:00.334491 21880 net.cpp:386] bn2_1_D -> conv2_1_D (in-place)
I0523 15:14:00.334666 21880 net.cpp:141] Setting up bn2_1_D
I0523 15:14:00.334671 21880 net.cpp:148] Top shape: 1 32 112 112 (401408)
I0523 15:14:00.334684 21880 net.cpp:156] Memory required for data: 142800896
I0523 15:14:00.334689 21880 layer_factory.hpp:77] Creating layer scale2_1_D
I0523 15:14:00.334694 21880 net.cpp:91] Creating Layer scale2_1_D
I0523 15:14:00.334697 21880 net.cpp:425] scale2_1_D <- conv2_1_D
I0523 15:14:00.334699 21880 net.cpp:386] scale2_1_D -> conv2_1_D (in-place)
I0523 15:14:00.334739 21880 layer_factory.hpp:77] Creating layer scale2_1_D
I0523 15:14:00.334859 21880 net.cpp:141] Setting up scale2_1_D
I0523 15:14:00.334863 21880 net.cpp:148] Top shape: 1 32 112 112 (401408)
I0523 15:14:00.334877 21880 net.cpp:156] Memory required for data: 144406528
I0523 15:14:00.334879 21880 layer_factory.hpp:77] Creating layer relu2_1_D
I0523 15:14:00.334883 21880 net.cpp:91] Creating Layer relu2_1_D
I0523 15:14:00.334887 21880 net.cpp:425] relu2_1_D <- conv2_1_D
I0523 15:14:00.334889 21880 net.cpp:386] relu2_1_D -> conv2_1_D (in-place)
I0523 15:14:00.335031 21880 net.cpp:141] Setting up relu2_1_D
I0523 15:14:00.335036 21880 net.cpp:148] Top shape: 1 32 112 112 (401408)
I0523 15:14:00.335049 21880 net.cpp:156] Memory required for data: 146012160
I0523 15:14:00.335052 21880 layer_factory.hpp:77] Creating layer upsample1
I0523 15:14:00.335057 21880 net.cpp:91] Creating Layer upsample1
I0523 15:14:00.335058 21880 net.cpp:425] upsample1 <- conv2_1_D
I0523 15:14:00.335062 21880 net.cpp:425] upsample1 <- pool1_mask
I0523 15:14:00.335065 21880 net.cpp:399] upsample1 -> pool1_D
I0523 15:14:00.335099 21880 net.cpp:141] Setting up upsample1
I0523 15:14:00.335103 21880 net.cpp:148] Top shape: 1 32 224 224 (1605632)
I0523 15:14:00.335104 21880 net.cpp:156] Memory required for data: 152434688
I0523 15:14:00.335117 21880 layer_factory.hpp:77] Creating layer conv1_2_D
I0523 15:14:00.335124 21880 net.cpp:91] Creating Layer conv1_2_D
I0523 15:14:00.335125 21880 net.cpp:425] conv1_2_D <- pool1_D
I0523 15:14:00.335130 21880 net.cpp:399] conv1_2_D -> conv1_2_D
I0523 15:14:00.336138 21880 net.cpp:141] Setting up conv1_2_D
I0523 15:14:00.336148 21880 net.cpp:148] Top shape: 1 32 224 224 (1605632)
I0523 15:14:00.336159 21880 net.cpp:156] Memory required for data: 158857216
I0523 15:14:00.336163 21880 layer_factory.hpp:77] Creating layer bn1_2_D
I0523 15:14:00.336169 21880 net.cpp:91] Creating Layer bn1_2_D
I0523 15:14:00.336171 21880 net.cpp:425] bn1_2_D <- conv1_2_D
I0523 15:14:00.336175 21880 net.cpp:386] bn1_2_D -> conv1_2_D (in-place)
I0523 15:14:00.336377 21880 net.cpp:141] Setting up bn1_2_D
I0523 15:14:00.336382 21880 net.cpp:148] Top shape: 1 32 224 224 (1605632)
I0523 15:14:00.336385 21880 net.cpp:156] Memory required for data: 165279744
I0523 15:14:00.336400 21880 layer_factory.hpp:77] Creating layer scale1_2_D
I0523 15:14:00.336403 21880 net.cpp:91] Creating Layer scale1_2_D
I0523 15:14:00.336405 21880 net.cpp:425] scale1_2_D <- conv1_2_D
I0523 15:14:00.336410 21880 net.cpp:386] scale1_2_D -> conv1_2_D (in-place)
I0523 15:14:00.336449 21880 layer_factory.hpp:77] Creating layer scale1_2_D
I0523 15:14:00.336627 21880 net.cpp:141] Setting up scale1_2_D
I0523 15:14:00.336632 21880 net.cpp:148] Top shape: 1 32 224 224 (1605632)
I0523 15:14:00.336644 21880 net.cpp:156] Memory required for data: 171702272
I0523 15:14:00.336648 21880 layer_factory.hpp:77] Creating layer relu1_2_D
I0523 15:14:00.336653 21880 net.cpp:91] Creating Layer relu1_2_D
I0523 15:14:00.336655 21880 net.cpp:425] relu1_2_D <- conv1_2_D
I0523 15:14:00.336658 21880 net.cpp:386] relu1_2_D -> conv1_2_D (in-place)
I0523 15:14:00.336889 21880 net.cpp:141] Setting up relu1_2_D
I0523 15:14:00.336897 21880 net.cpp:148] Top shape: 1 32 224 224 (1605632)
I0523 15:14:00.336908 21880 net.cpp:156] Memory required for data: 178124800
I0523 15:14:00.336911 21880 layer_factory.hpp:77] Creating layer conv1_1_D
I0523 15:14:00.336920 21880 net.cpp:91] Creating Layer conv1_1_D
I0523 15:14:00.336921 21880 net.cpp:425] conv1_1_D <- conv1_2_D
I0523 15:14:00.336926 21880 net.cpp:399] conv1_1_D -> conv1_1_D
I0523 15:14:00.338037 21880 net.cpp:141] Setting up conv1_1_D
I0523 15:14:00.338055 21880 net.cpp:148] Top shape: 1 2 224 224 (100352)
I0523 15:14:00.338058 21880 net.cpp:156] Memory required for data: 178526208
I0523 15:14:00.338064 21880 layer_factory.hpp:77] Creating layer conv1_1_D_conv1_1_D_0_split
I0523 15:14:00.338070 21880 net.cpp:91] Creating Layer conv1_1_D_conv1_1_D_0_split
I0523 15:14:00.338073 21880 net.cpp:425] conv1_1_D_conv1_1_D_0_split <- conv1_1_D
I0523 15:14:00.338078 21880 net.cpp:399] conv1_1_D_conv1_1_D_0_split -> conv1_1_D_conv1_1_D_0_split_0
I0523 15:14:00.338083 21880 net.cpp:399] conv1_1_D_conv1_1_D_0_split -> conv1_1_D_conv1_1_D_0_split_1
I0523 15:14:00.338131 21880 net.cpp:141] Setting up conv1_1_D_conv1_1_D_0_split
I0523 15:14:00.338135 21880 net.cpp:148] Top shape: 1 2 224 224 (100352)
I0523 15:14:00.338148 21880 net.cpp:148] Top shape: 1 2 224 224 (100352)
I0523 15:14:00.338150 21880 net.cpp:156] Memory required for data: 179329024
I0523 15:14:00.338152 21880 layer_factory.hpp:77] Creating layer loss
I0523 15:14:00.338160 21880 net.cpp:91] Creating Layer loss
I0523 15:14:00.338163 21880 net.cpp:425] loss <- conv1_1_D_conv1_1_D_0_split_0
I0523 15:14:00.338166 21880 net.cpp:425] loss <- label_data_1_split_0
I0523 15:14:00.338171 21880 net.cpp:399] loss -> loss
I0523 15:14:00.338177 21880 layer_factory.hpp:77] Creating layer loss
I0523 15:14:00.338744 21880 net.cpp:141] Setting up loss
I0523 15:14:00.338752 21880 net.cpp:148] Top shape: (1)
I0523 15:14:00.338765 21880 net.cpp:151]     with loss weight 1
I0523 15:14:00.338779 21880 net.cpp:156] Memory required for data: 179329028
I0523 15:14:00.338781 21880 layer_factory.hpp:77] Creating layer accuracy
I0523 15:14:00.338789 21880 net.cpp:91] Creating Layer accuracy
I0523 15:14:00.338793 21880 net.cpp:425] accuracy <- conv1_1_D_conv1_1_D_0_split_1
I0523 15:14:00.338796 21880 net.cpp:425] accuracy <- label_data_1_split_1
I0523 15:14:00.338800 21880 net.cpp:399] accuracy -> accuracy
I0523 15:14:00.338806 21880 net.cpp:141] Setting up accuracy
I0523 15:14:00.338809 21880 net.cpp:148] Top shape: (1)
I0523 15:14:00.338811 21880 net.cpp:156] Memory required for data: 179329032
I0523 15:14:00.338814 21880 net.cpp:219] accuracy does not need backward computation.
I0523 15:14:00.338816 21880 net.cpp:217] loss needs backward computation.
I0523 15:14:00.338819 21880 net.cpp:217] conv1_1_D_conv1_1_D_0_split needs backward computation.
I0523 15:14:00.338821 21880 net.cpp:217] conv1_1_D needs backward computation.
I0523 15:14:00.338824 21880 net.cpp:217] relu1_2_D needs backward computation.
I0523 15:14:00.338825 21880 net.cpp:217] scale1_2_D needs backward computation.
I0523 15:14:00.338827 21880 net.cpp:217] bn1_2_D needs backward computation.
I0523 15:14:00.338829 21880 net.cpp:217] conv1_2_D needs backward computation.
I0523 15:14:00.338831 21880 net.cpp:217] upsample1 needs backward computation.
I0523 15:14:00.338834 21880 net.cpp:217] relu2_1_D needs backward computation.
I0523 15:14:00.338835 21880 net.cpp:217] scale2_1_D needs backward computation.
I0523 15:14:00.338837 21880 net.cpp:217] bn2_1_D needs backward computation.
I0523 15:14:00.338838 21880 net.cpp:217] conv2_1_D needs backward computation.
I0523 15:14:00.338841 21880 net.cpp:217] relu2_2_D needs backward computation.
I0523 15:14:00.338842 21880 net.cpp:217] scale2_2_D needs backward computation.
I0523 15:14:00.338845 21880 net.cpp:217] bn2_2_D needs backward computation.
I0523 15:14:00.338846 21880 net.cpp:217] conv2_2_D needs backward computation.
I0523 15:14:00.338848 21880 net.cpp:217] upsample2 needs backward computation.
I0523 15:14:00.338851 21880 net.cpp:217] relu3_1_D needs backward computation.
I0523 15:14:00.338861 21880 net.cpp:217] scale3_1_D needs backward computation.
I0523 15:14:00.338863 21880 net.cpp:217] bn3_1_D needs backward computation.
I0523 15:14:00.338865 21880 net.cpp:217] conv3_1_D needs backward computation.
I0523 15:14:00.338867 21880 net.cpp:217] relu3_2_D needs backward computation.
I0523 15:14:00.338871 21880 net.cpp:217] scale3_2_D needs backward computation.
I0523 15:14:00.338871 21880 net.cpp:217] bn3_2_D needs backward computation.
I0523 15:14:00.338873 21880 net.cpp:217] conv3_2_D needs backward computation.
I0523 15:14:00.338876 21880 net.cpp:217] upsample3 needs backward computation.
I0523 15:14:00.338878 21880 net.cpp:217] relu4_1_D needs backward computation.
I0523 15:14:00.338881 21880 net.cpp:217] scale4_1_D needs backward computation.
I0523 15:14:00.338882 21880 net.cpp:217] bn4_1_D needs backward computation.
I0523 15:14:00.338884 21880 net.cpp:217] conv4_1_D needs backward computation.
I0523 15:14:00.338897 21880 net.cpp:217] relu4_2_D needs backward computation.
I0523 15:14:00.338899 21880 net.cpp:217] scale4_2_D needs backward computation.
I0523 15:14:00.338901 21880 net.cpp:217] bn4_2_D needs backward computation.
I0523 15:14:00.338903 21880 net.cpp:217] conv4_2_D needs backward computation.
I0523 15:14:00.338906 21880 net.cpp:217] upsample4 needs backward computation.
I0523 15:14:00.338908 21880 net.cpp:217] relu5_1_D needs backward computation.
I0523 15:14:00.338910 21880 net.cpp:217] scale5_1_D needs backward computation.
I0523 15:14:00.338912 21880 net.cpp:217] bn5_1_D needs backward computation.
I0523 15:14:00.338914 21880 net.cpp:217] conv5_1_D needs backward computation.
I0523 15:14:00.338917 21880 net.cpp:217] relu5_2_D needs backward computation.
I0523 15:14:00.338919 21880 net.cpp:217] scale5_2_D needs backward computation.
I0523 15:14:00.338922 21880 net.cpp:217] bn5_2_D needs backward computation.
I0523 15:14:00.338923 21880 net.cpp:217] conv5_2_D needs backward computation.
I0523 15:14:00.338925 21880 net.cpp:217] upsample5 needs backward computation.
I0523 15:14:00.338928 21880 net.cpp:217] pool5 needs backward computation.
I0523 15:14:00.338940 21880 net.cpp:217] relu5_2 needs backward computation.
I0523 15:14:00.338943 21880 net.cpp:217] scale5_2 needs backward computation.
I0523 15:14:00.338945 21880 net.cpp:217] bn5_2 needs backward computation.
I0523 15:14:00.338948 21880 net.cpp:217] conv5_2 needs backward computation.
I0523 15:14:00.338950 21880 net.cpp:217] relu5_1 needs backward computation.
I0523 15:14:00.338953 21880 net.cpp:217] scale5_1 needs backward computation.
I0523 15:14:00.338954 21880 net.cpp:217] bn5_1 needs backward computation.
I0523 15:14:00.338956 21880 net.cpp:217] conv5_1 needs backward computation.
I0523 15:14:00.338958 21880 net.cpp:217] pool4 needs backward computation.
I0523 15:14:00.338961 21880 net.cpp:217] relu4_2 needs backward computation.
I0523 15:14:00.338963 21880 net.cpp:217] scale4_2 needs backward computation.
I0523 15:14:00.338965 21880 net.cpp:217] bn4_2 needs backward computation.
I0523 15:14:00.338968 21880 net.cpp:217] conv4_2 needs backward computation.
I0523 15:14:00.338969 21880 net.cpp:217] relu4_1 needs backward computation.
I0523 15:14:00.338971 21880 net.cpp:217] scale4_1 needs backward computation.
I0523 15:14:00.338973 21880 net.cpp:217] bn4_1 needs backward computation.
I0523 15:14:00.338975 21880 net.cpp:217] conv4_1 needs backward computation.
I0523 15:14:00.338978 21880 net.cpp:217] pool3 needs backward computation.
I0523 15:14:00.338980 21880 net.cpp:217] relu3_2 needs backward computation.
I0523 15:14:00.338982 21880 net.cpp:217] scale3_2 needs backward computation.
I0523 15:14:00.338984 21880 net.cpp:217] bn3_2 needs backward computation.
I0523 15:14:00.338986 21880 net.cpp:217] conv3_2 needs backward computation.
I0523 15:14:00.338989 21880 net.cpp:217] relu3_1 needs backward computation.
I0523 15:14:00.338990 21880 net.cpp:217] scale3_1 needs backward computation.
I0523 15:14:00.338992 21880 net.cpp:217] bn3_1 needs backward computation.
I0523 15:14:00.338994 21880 net.cpp:217] conv3_1 needs backward computation.
I0523 15:14:00.339000 21880 net.cpp:217] pool2 needs backward computation.
I0523 15:14:00.339002 21880 net.cpp:217] relu2_2 needs backward computation.
I0523 15:14:00.339004 21880 net.cpp:217] scale2_2 needs backward computation.
I0523 15:14:00.339006 21880 net.cpp:217] bn2_2 needs backward computation.
I0523 15:14:00.339009 21880 net.cpp:217] conv2_2 needs backward computation.
I0523 15:14:00.339010 21880 net.cpp:217] relu2_1 needs backward computation.
I0523 15:14:00.339012 21880 net.cpp:217] scale2_1 needs backward computation.
I0523 15:14:00.339015 21880 net.cpp:217] bn2_1 needs backward computation.
I0523 15:14:00.339016 21880 net.cpp:217] conv2_1 needs backward computation.
I0523 15:14:00.339020 21880 net.cpp:217] pool1 needs backward computation.
I0523 15:14:00.339021 21880 net.cpp:217] relu1_2 needs backward computation.
I0523 15:14:00.339023 21880 net.cpp:217] scale1_2 needs backward computation.
I0523 15:14:00.339025 21880 net.cpp:217] bn1_2 needs backward computation.
I0523 15:14:00.339027 21880 net.cpp:217] conv1_2 needs backward computation.
I0523 15:14:00.339030 21880 net.cpp:217] relu1_1 needs backward computation.
I0523 15:14:00.339031 21880 net.cpp:217] scale1_1 needs backward computation.
I0523 15:14:00.339033 21880 net.cpp:217] bn1_1 needs backward computation.
I0523 15:14:00.339035 21880 net.cpp:217] conv1_1 needs backward computation.
I0523 15:14:00.339038 21880 net.cpp:219] label_data_1_split does not need backward computation.
I0523 15:14:00.339041 21880 net.cpp:219] data does not need backward computation.
I0523 15:14:00.339043 21880 net.cpp:261] This network produces output accuracy
I0523 15:14:00.339046 21880 net.cpp:261] This network produces output loss
I0523 15:14:00.339076 21880 net.cpp:274] Network initialization done.
I0523 15:14:00.340481 21880 solver.cpp:181] Creating test net (#0) specified by net file: models/segnet/train_val.prototxt
I0523 15:14:00.340589 21880 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0523 15:14:00.340955 21880 net.cpp:49] Initializing net from parameters: 
name: "segnet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "DenseImageData"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    crop_size: 224
    mean_value: 100
  }
  dense_image_data_param {
    source: "data/val.txt"
    batch_size: 4
    shuffle: true
    new_height: 224
    new_width: 310
    is_color: false
    root_folder: "data/raw/train/"
  }
}
layer {
  name: "conv1_1"
  type: "Convolution"
  bottom: "data"
  top: "conv1_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn1_1"
  type: "BatchNorm"
  bottom: "conv1_1"
  top: "conv1_1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale1_1"
  type: "Scale"
  bottom: "conv1_1"
  top: "conv1_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1_1"
  type: "ReLU"
  bottom: "conv1_1"
  top: "conv1_1"
}
layer {
  name: "conv1_2"
  type: "Convolution"
  bottom: "conv1_1"
  top: "conv1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn1_2"
  type: "BatchNorm"
  bottom: "conv1_2"
  top: "conv1_2"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale1_2"
  type: "Scale"
  bottom: "conv1_2"
  top: "conv1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1_2"
  type: "ReLU"
  bottom: "conv1_2"
  top: "conv1_2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1_2"
  top: "pool1"
  top: "pool1_mask"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2"
  top: "pool2_mask"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn3_1"
  type: "BatchNorm"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale3_1"
  type: "Scale"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv3_2"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv3_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn3_2"
  type: "BatchNorm"
  bottom: "conv3_2"
  top: "conv3_2"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale3_2"
  type: "Scale"
  bottom: "conv3_2"
  top: "conv3_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_2"
  type: "ReLU"
  bottom: "conv3_2"
  top: "conv3_2"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3_2"
  top: "pool3"
  top: "pool3_mask"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4"
  top: "pool4_mask"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv5_1"
  type: "Convolution"
  bottom: "pool4"
  top: "conv5_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn5_1"
  type: "BatchNorm"
  bottom: "conv5_1"
  top: "conv5_1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale5_1"
  type: "Scale"
  bottom: "conv5_1"
  top: "conv5_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5_1"
  type: "ReLU"
  bottom: "conv5_1"
  top: "conv5_1"
}
layer {
  name: "conv5_2"
  type: "Convolution"
  bottom: "conv5_1"
  top: "conv5_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn5_2"
  type: "BatchNorm"
  bottom: "conv5_2"
  top: "conv5_2"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale5_2"
  type: "Scale"
  bottom: "conv5_2"
  top: "conv5_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5_2"
  type: "ReLU"
  bottom: "conv5_2"
  top: "conv5_2"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5_2"
  top: "pool5"
  top: "pool5_mask"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "upsample5"
  type: "Upsample"
  bottom: "pool5"
  bottom: "pool5_mask"
  top: "pool5_D"
  upsample_param {
    scale: 2
    upsample_h: 14
    upsample_w: 14
  }
}
layer {
  name: "conv5_2_D"
  type: "Convolution"
  bottom: "pool5_D"
  top: "conv5_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn5_2_D"
  type: "BatchNorm"
  bottom: "conv5_2_D"
  top: "conv5_2_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale5_2_D"
  type: "Scale"
  bottom: "conv5_2_D"
  top: "conv5_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5_2_D"
  type: "ReLU"
  bottom: "conv5_2_D"
  top: "conv5_2_D"
}
layer {
  name: "conv5_1_D"
  type: "Convolution"
  bottom: "conv5_2_D"
  top: "conv5_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn5_1_D"
  type: "BatchNorm"
  bottom: "conv5_1_D"
  top: "conv5_1_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale5_1_D"
  type: "Scale"
  bottom: "conv5_1_D"
  top: "conv5_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5_1_D"
  type: "ReLU"
  bottom: "conv5_1_D"
  top: "conv5_1_D"
}
layer {
  name: "upsample4"
  type: "Upsample"
  bottom: "conv5_1_D"
  bottom: "pool4_mask"
  top: "pool4_D"
  upsample_param {
    scale: 2
    upsample_h: 28
    upsample_w: 28
  }
}
layer {
  name: "conv4_2_D"
  type: "Convolution"
  bottom: "pool4_D"
  top: "conv4_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn4_2_D"
  type: "BatchNorm"
  bottom: "conv4_2_D"
  top: "conv4_2_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale4_2_D"
  type: "Scale"
  bottom: "conv4_2_D"
  top: "conv4_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4_2_D"
  type: "ReLU"
  bottom: "conv4_2_D"
  top: "conv4_2_D"
}
layer {
  name: "conv4_1_D"
  type: "Convolution"
  bottom: "conv4_2_D"
  top: "conv4_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn4_1_D"
  type: "BatchNorm"
  bottom: "conv4_1_D"
  top: "conv4_1_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale4_1_D"
  type: "Scale"
  bottom: "conv4_1_D"
  top: "conv4_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4_1_D"
  type: "ReLU"
  bottom: "conv4_1_D"
  top: "conv4_1_D"
}
layer {
  name: "upsample3"
  type: "Upsample"
  bottom: "conv4_1_D"
  bottom: "pool3_mask"
  top: "pool3_D"
  upsample_param {
    scale: 2
    upsample_h: 56
    upsample_w: 56
  }
}
layer {
  name: "conv3_2_D"
  type: "Convolution"
  bottom: "pool3_D"
  top: "conv3_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3_2_D"
  type: "BatchNorm"
  bottom: "conv3_2_D"
  top: "conv3_2_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale3_2_D"
  type: "Scale"
  bottom: "conv3_2_D"
  top: "conv3_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_2_D"
  type: "ReLU"
  bottom: "conv3_2_D"
  top: "conv3_2_D"
}
layer {
  name: "conv3_1_D"
  type: "Convolution"
  bottom: "conv3_2_D"
  top: "conv3_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn3_1_D"
  type: "BatchNorm"
  bottom: "conv3_1_D"
  top: "conv3_1_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale3_1_D"
  type: "Scale"
  bottom: "conv3_1_D"
  top: "conv3_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_1_D"
  type: "ReLU"
  bottom: "conv3_1_D"
  top: "conv3_1_D"
}
layer {
  name: "upsample2"
  type: "Upsample"
  bottom: "conv3_1_D"
  bottom: "pool2_mask"
  top: "pool2_D"
  upsample_param {
    scale: 2
    upsample_h: 112
    upsample_w: 112
  }
}
layer {
  name: "conv2_2_D"
  type: "Convolution"
  bottom: "pool2_D"
  top: "conv2_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn2_2_D"
  type: "BatchNorm"
  bottom: "conv2_2_D"
  top: "conv2_2_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale2_2_D"
  type: "Scale"
  bottom: "conv2_2_D"
  top: "conv2_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2_2_D"
  type: "ReLU"
  bottom: "conv2_2_D"
  top: "conv2_2_D"
}
layer {
  name: "conv2_1_D"
  type: "Convolution"
  bottom: "conv2_2_D"
  top: "conv2_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn2_1_D"
  type: "BatchNorm"
  bottom: "conv2_1_D"
  top: "conv2_1_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale2_1_D"
  type: "Scale"
  bottom: "conv2_1_D"
  top: "conv2_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2_1_D"
  type: "ReLU"
  bottom: "conv2_1_D"
  top: "conv2_1_D"
}
layer {
  name: "upsample1"
  type: "Upsample"
  bottom: "conv2_1_D"
  bottom: "pool1_mask"
  top: "pool1_D"
  upsample_param {
    scale: 2
    upsample_h: 224
    upsample_w: 224
  }
}
layer {
  name: "conv1_2_D"
  type: "Convolution"
  bottom: "pool1_D"
  top: "conv1_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn1_2_D"
  type: "BatchNorm"
  bottom: "conv1_2_D"
  top: "conv1_2_D"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale1_2_D"
  type: "Scale"
  bottom: "conv1_2_D"
  top: "conv1_2_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1_2_D"
  type: "ReLU"
  bottom: "conv1_2_D"
  top: "conv1_2_D"
}
layer {
  name: "conv1_1_D"
  type: "Convolution"
  bottom: "conv1_2_D"
  top: "conv1_1_D"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 2
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "conv1_1_D"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "conv1_1_D"
  bottom: "label"
  top: "accuracy"
}
I0523 15:14:00.341171 21880 layer_factory.hpp:77] Creating layer data
I0523 15:14:00.341181 21880 net.cpp:91] Creating Layer data
I0523 15:14:00.341183 21880 net.cpp:399] data -> data
I0523 15:14:00.341189 21880 net.cpp:399] data -> label
I0523 15:14:00.341197 21880 dense_image_data_layer.cpp:38] Opening file data/val.txt
I0523 15:14:00.341503 21880 dense_image_data_layer.cpp:48] Shuffling data
I0523 15:14:00.341572 21880 dense_image_data_layer.cpp:53] A total of 705 examples.
I0523 15:14:00.346426 21880 dense_image_data_layer.cpp:92] output data size: 1,1,224,224
I0523 15:14:00.347766 21880 net.cpp:141] Setting up data
I0523 15:14:00.347782 21880 net.cpp:148] Top shape: 1 1 224 224 (50176)
I0523 15:14:00.347787 21880 net.cpp:148] Top shape: 1 1 224 224 (50176)
I0523 15:14:00.347790 21880 net.cpp:156] Memory required for data: 401408
I0523 15:14:00.347795 21880 layer_factory.hpp:77] Creating layer label_data_1_split
I0523 15:14:00.347806 21880 net.cpp:91] Creating Layer label_data_1_split
I0523 15:14:00.347810 21880 net.cpp:425] label_data_1_split <- label
I0523 15:14:00.347815 21880 net.cpp:399] label_data_1_split -> label_data_1_split_0
I0523 15:14:00.347823 21880 net.cpp:399] label_data_1_split -> label_data_1_split_1
I0523 15:14:00.347895 21880 net.cpp:141] Setting up label_data_1_split
I0523 15:14:00.347901 21880 net.cpp:148] Top shape: 1 1 224 224 (50176)
I0523 15:14:00.347904 21880 net.cpp:148] Top shape: 1 1 224 224 (50176)
I0523 15:14:00.347906 21880 net.cpp:156] Memory required for data: 802816
I0523 15:14:00.347908 21880 layer_factory.hpp:77] Creating layer conv1_1
I0523 15:14:00.347918 21880 net.cpp:91] Creating Layer conv1_1
I0523 15:14:00.347921 21880 net.cpp:425] conv1_1 <- data
I0523 15:14:00.347925 21880 net.cpp:399] conv1_1 -> conv1_1
I0523 15:14:00.349086 21880 net.cpp:141] Setting up conv1_1
I0523 15:14:00.349097 21880 net.cpp:148] Top shape: 1 32 224 224 (1605632)
I0523 15:14:00.349099 21880 net.cpp:156] Memory required for data: 7225344
I0523 15:14:00.349105 21880 layer_factory.hpp:77] Creating layer bn1_1
I0523 15:14:00.349112 21880 net.cpp:91] Creating Layer bn1_1
I0523 15:14:00.349114 21880 net.cpp:425] bn1_1 <- conv1_1
I0523 15:14:00.349118 21880 net.cpp:386] bn1_1 -> conv1_1 (in-place)
I0523 15:14:00.349334 21880 net.cpp:141] Setting up bn1_1
I0523 15:14:00.349339 21880 net.cpp:148] Top shape: 1 32 224 224 (1605632)
I0523 15:14:00.349341 21880 net.cpp:156] Memory required for data: 13647872
I0523 15:14:00.349349 21880 layer_factory.hpp:77] Creating layer scale1_1
I0523 15:14:00.349356 21880 net.cpp:91] Creating Layer scale1_1
I0523 15:14:00.349359 21880 net.cpp:425] scale1_1 <- conv1_1
I0523 15:14:00.349362 21880 net.cpp:386] scale1_1 -> conv1_1 (in-place)
I0523 15:14:00.349395 21880 layer_factory.hpp:77] Creating layer scale1_1
I0523 15:14:00.349861 21880 net.cpp:141] Setting up scale1_1
I0523 15:14:00.349870 21880 net.cpp:148] Top shape: 1 32 224 224 (1605632)
I0523 15:14:00.349872 21880 net.cpp:156] Memory required for data: 20070400
I0523 15:14:00.349879 21880 layer_factory.hpp:77] Creating layer relu1_1
I0523 15:14:00.349885 21880 net.cpp:91] Creating Layer relu1_1
I0523 15:14:00.349900 21880 net.cpp:425] relu1_1 <- conv1_1
I0523 15:14:00.349903 21880 net.cpp:386] relu1_1 -> conv1_1 (in-place)
I0523 15:14:00.350050 21880 net.cpp:141] Setting up relu1_1
I0523 15:14:00.350057 21880 net.cpp:148] Top shape: 1 32 224 224 (1605632)
I0523 15:14:00.350059 21880 net.cpp:156] Memory required for data: 26492928
I0523 15:14:00.350062 21880 layer_factory.hpp:77] Creating layer conv1_2
I0523 15:14:00.350069 21880 net.cpp:91] Creating Layer conv1_2
I0523 15:14:00.350071 21880 net.cpp:425] conv1_2 <- conv1_1
I0523 15:14:00.350075 21880 net.cpp:399] conv1_2 -> conv1_2
I0523 15:14:00.351217 21880 net.cpp:141] Setting up conv1_2
I0523 15:14:00.351229 21880 net.cpp:148] Top shape: 1 32 224 224 (1605632)
I0523 15:14:00.351232 21880 net.cpp:156] Memory required for data: 32915456
I0523 15:14:00.351236 21880 layer_factory.hpp:77] Creating layer bn1_2
I0523 15:14:00.351241 21880 net.cpp:91] Creating Layer bn1_2
I0523 15:14:00.351244 21880 net.cpp:425] bn1_2 <- conv1_2
I0523 15:14:00.351248 21880 net.cpp:386] bn1_2 -> conv1_2 (in-place)
I0523 15:14:00.351528 21880 net.cpp:141] Setting up bn1_2
I0523 15:14:00.351536 21880 net.cpp:148] Top shape: 1 32 224 224 (1605632)
I0523 15:14:00.351538 21880 net.cpp:156] Memory required for data: 39337984
I0523 15:14:00.351547 21880 layer_factory.hpp:77] Creating layer scale1_2
I0523 15:14:00.351563 21880 net.cpp:91] Creating Layer scale1_2
I0523 15:14:00.351565 21880 net.cpp:425] scale1_2 <- conv1_2
I0523 15:14:00.351569 21880 net.cpp:386] scale1_2 -> conv1_2 (in-place)
I0523 15:14:00.351601 21880 layer_factory.hpp:77] Creating layer scale1_2
I0523 15:14:00.351758 21880 net.cpp:141] Setting up scale1_2
I0523 15:14:00.351764 21880 net.cpp:148] Top shape: 1 32 224 224 (1605632)
I0523 15:14:00.351766 21880 net.cpp:156] Memory required for data: 45760512
I0523 15:14:00.351770 21880 layer_factory.hpp:77] Creating layer relu1_2
I0523 15:14:00.351775 21880 net.cpp:91] Creating Layer relu1_2
I0523 15:14:00.351778 21880 net.cpp:425] relu1_2 <- conv1_2
I0523 15:14:00.351780 21880 net.cpp:386] relu1_2 -> conv1_2 (in-place)
I0523 15:14:00.352026 21880 net.cpp:141] Setting up relu1_2
I0523 15:14:00.352035 21880 net.cpp:148] Top shape: 1 32 224 224 (1605632)
I0523 15:14:00.352037 21880 net.cpp:156] Memory required for data: 52183040
I0523 15:14:00.352041 21880 layer_factory.hpp:77] Creating layer pool1
I0523 15:14:00.352043 21880 layer_factory.cpp:90] cuDNN does not support multiple tops. Using Caffe's own pooling layer.
I0523 15:14:00.352048 21880 net.cpp:91] Creating Layer pool1
I0523 15:14:00.352051 21880 net.cpp:425] pool1 <- conv1_2
I0523 15:14:00.352054 21880 net.cpp:399] pool1 -> pool1
I0523 15:14:00.352058 21880 net.cpp:399] pool1 -> pool1_mask
I0523 15:14:00.352093 21880 net.cpp:141] Setting up pool1
I0523 15:14:00.352098 21880 net.cpp:148] Top shape: 1 32 112 112 (401408)
I0523 15:14:00.352102 21880 net.cpp:148] Top shape: 1 32 112 112 (401408)
I0523 15:14:00.352103 21880 net.cpp:156] Memory required for data: 55394304
I0523 15:14:00.352105 21880 layer_factory.hpp:77] Creating layer conv2_1
I0523 15:14:00.352111 21880 net.cpp:91] Creating Layer conv2_1
I0523 15:14:00.352113 21880 net.cpp:425] conv2_1 <- pool1
I0523 15:14:00.352118 21880 net.cpp:399] conv2_1 -> conv2_1
I0523 15:14:00.353343 21880 net.cpp:141] Setting up conv2_1
I0523 15:14:00.353351 21880 net.cpp:148] Top shape: 1 64 112 112 (802816)
I0523 15:14:00.353354 21880 net.cpp:156] Memory required for data: 58605568
I0523 15:14:00.353358 21880 layer_factory.hpp:77] Creating layer bn2_1
I0523 15:14:00.353363 21880 net.cpp:91] Creating Layer bn2_1
I0523 15:14:00.353365 21880 net.cpp:425] bn2_1 <- conv2_1
I0523 15:14:00.353369 21880 net.cpp:386] bn2_1 -> conv2_1 (in-place)
I0523 15:14:00.353586 21880 net.cpp:141] Setting up bn2_1
I0523 15:14:00.353592 21880 net.cpp:148] Top shape: 1 64 112 112 (802816)
I0523 15:14:00.353595 21880 net.cpp:156] Memory required for data: 61816832
I0523 15:14:00.353600 21880 layer_factory.hpp:77] Creating layer scale2_1
I0523 15:14:00.353606 21880 net.cpp:91] Creating Layer scale2_1
I0523 15:14:00.353621 21880 net.cpp:425] scale2_1 <- conv2_1
I0523 15:14:00.353626 21880 net.cpp:386] scale2_1 -> conv2_1 (in-place)
I0523 15:14:00.353960 21880 layer_factory.hpp:77] Creating layer scale2_1
I0523 15:14:00.354380 21880 net.cpp:141] Setting up scale2_1
I0523 15:14:00.354387 21880 net.cpp:148] Top shape: 1 64 112 112 (802816)
I0523 15:14:00.354389 21880 net.cpp:156] Memory required for data: 65028096
I0523 15:14:00.354396 21880 layer_factory.hpp:77] Creating layer relu2_1
I0523 15:14:00.354401 21880 net.cpp:91] Creating Layer relu2_1
I0523 15:14:00.354403 21880 net.cpp:425] relu2_1 <- conv2_1
I0523 15:14:00.354408 21880 net.cpp:386] relu2_1 -> conv2_1 (in-place)
I0523 15:14:00.354581 21880 net.cpp:141] Setting up relu2_1
I0523 15:14:00.354589 21880 net.cpp:148] Top shape: 1 64 112 112 (802816)
I0523 15:14:00.354591 21880 net.cpp:156] Memory required for data: 68239360
I0523 15:14:00.354594 21880 layer_factory.hpp:77] Creating layer conv2_2
I0523 15:14:00.354601 21880 net.cpp:91] Creating Layer conv2_2
I0523 15:14:00.354604 21880 net.cpp:425] conv2_2 <- conv2_1
I0523 15:14:00.354607 21880 net.cpp:399] conv2_2 -> conv2_2
I0523 15:14:00.356415 21880 net.cpp:141] Setting up conv2_2
I0523 15:14:00.356425 21880 net.cpp:148] Top shape: 1 64 112 112 (802816)
I0523 15:14:00.356426 21880 net.cpp:156] Memory required for data: 71450624
I0523 15:14:00.356431 21880 layer_factory.hpp:77] Creating layer bn2_2
I0523 15:14:00.356438 21880 net.cpp:91] Creating Layer bn2_2
I0523 15:14:00.356441 21880 net.cpp:425] bn2_2 <- conv2_2
I0523 15:14:00.356444 21880 net.cpp:386] bn2_2 -> conv2_2 (in-place)
I0523 15:14:00.356628 21880 net.cpp:141] Setting up bn2_2
I0523 15:14:00.356633 21880 net.cpp:148] Top shape: 1 64 112 112 (802816)
I0523 15:14:00.356636 21880 net.cpp:156] Memory required for data: 74661888
I0523 15:14:00.356640 21880 layer_factory.hpp:77] Creating layer scale2_2
I0523 15:14:00.356647 21880 net.cpp:91] Creating Layer scale2_2
I0523 15:14:00.356648 21880 net.cpp:425] scale2_2 <- conv2_2
I0523 15:14:00.356652 21880 net.cpp:386] scale2_2 -> conv2_2 (in-place)
I0523 15:14:00.356683 21880 layer_factory.hpp:77] Creating layer scale2_2
I0523 15:14:00.356789 21880 net.cpp:141] Setting up scale2_2
I0523 15:14:00.356794 21880 net.cpp:148] Top shape: 1 64 112 112 (802816)
I0523 15:14:00.356796 21880 net.cpp:156] Memory required for data: 77873152
I0523 15:14:00.356801 21880 layer_factory.hpp:77] Creating layer relu2_2
I0523 15:14:00.356806 21880 net.cpp:91] Creating Layer relu2_2
I0523 15:14:00.356807 21880 net.cpp:425] relu2_2 <- conv2_2
I0523 15:14:00.356811 21880 net.cpp:386] relu2_2 -> conv2_2 (in-place)
I0523 15:14:00.356951 21880 net.cpp:141] Setting up relu2_2
I0523 15:14:00.356957 21880 net.cpp:148] Top shape: 1 64 112 112 (802816)
I0523 15:14:00.356959 21880 net.cpp:156] Memory required for data: 81084416
I0523 15:14:00.356961 21880 layer_factory.hpp:77] Creating layer pool2
I0523 15:14:00.356964 21880 layer_factory.cpp:90] cuDNN does not support multiple tops. Using Caffe's own pooling layer.
I0523 15:14:00.356968 21880 net.cpp:91] Creating Layer pool2
I0523 15:14:00.356971 21880 net.cpp:425] pool2 <- conv2_2
I0523 15:14:00.356974 21880 net.cpp:399] pool2 -> pool2
I0523 15:14:00.356978 21880 net.cpp:399] pool2 -> pool2_mask
I0523 15:14:00.357014 21880 net.cpp:141] Setting up pool2
I0523 15:14:00.357018 21880 net.cpp:148] Top shape: 1 64 56 56 (200704)
I0523 15:14:00.357022 21880 net.cpp:148] Top shape: 1 64 56 56 (200704)
I0523 15:14:00.357023 21880 net.cpp:156] Memory required for data: 82690048
I0523 15:14:00.357025 21880 layer_factory.hpp:77] Creating layer conv3_1
I0523 15:14:00.357031 21880 net.cpp:91] Creating Layer conv3_1
I0523 15:14:00.357033 21880 net.cpp:425] conv3_1 <- pool2
I0523 15:14:00.357038 21880 net.cpp:399] conv3_1 -> conv3_1
I0523 15:14:00.359581 21880 net.cpp:141] Setting up conv3_1
I0523 15:14:00.359592 21880 net.cpp:148] Top shape: 1 128 56 56 (401408)
I0523 15:14:00.359596 21880 net.cpp:156] Memory required for data: 84295680
I0523 15:14:00.359598 21880 layer_factory.hpp:77] Creating layer bn3_1
I0523 15:14:00.359616 21880 net.cpp:91] Creating Layer bn3_1
I0523 15:14:00.359618 21880 net.cpp:425] bn3_1 <- conv3_1
I0523 15:14:00.359622 21880 net.cpp:386] bn3_1 -> conv3_1 (in-place)
I0523 15:14:00.359792 21880 net.cpp:141] Setting up bn3_1
I0523 15:14:00.359797 21880 net.cpp:148] Top shape: 1 128 56 56 (401408)
I0523 15:14:00.359799 21880 net.cpp:156] Memory required for data: 85901312
I0523 15:14:00.359804 21880 layer_factory.hpp:77] Creating layer scale3_1
I0523 15:14:00.359809 21880 net.cpp:91] Creating Layer scale3_1
I0523 15:14:00.359812 21880 net.cpp:425] scale3_1 <- conv3_1
I0523 15:14:00.359814 21880 net.cpp:386] scale3_1 -> conv3_1 (in-place)
I0523 15:14:00.359845 21880 layer_factory.hpp:77] Creating layer scale3_1
I0523 15:14:00.359941 21880 net.cpp:141] Setting up scale3_1
I0523 15:14:00.359944 21880 net.cpp:148] Top shape: 1 128 56 56 (401408)
I0523 15:14:00.359946 21880 net.cpp:156] Memory required for data: 87506944
I0523 15:14:00.359951 21880 layer_factory.hpp:77] Creating layer relu3_1
I0523 15:14:00.359954 21880 net.cpp:91] Creating Layer relu3_1
I0523 15:14:00.359956 21880 net.cpp:425] relu3_1 <- conv3_1
I0523 15:14:00.359959 21880 net.cpp:386] relu3_1 -> conv3_1 (in-place)
I0523 15:14:00.360177 21880 net.cpp:141] Setting up relu3_1
I0523 15:14:00.360186 21880 net.cpp:148] Top shape: 1 128 56 56 (401408)
I0523 15:14:00.360188 21880 net.cpp:156] Memory required for data: 89112576
I0523 15:14:00.360191 21880 layer_factory.hpp:77] Creating layer conv3_2
I0523 15:14:00.360198 21880 net.cpp:91] Creating Layer conv3_2
I0523 15:14:00.360200 21880 net.cpp:425] conv3_2 <- conv3_1
I0523 15:14:00.360203 21880 net.cpp:399] conv3_2 -> conv3_2
I0523 15:14:00.364727 21880 net.cpp:141] Setting up conv3_2
I0523 15:14:00.364739 21880 net.cpp:148] Top shape: 1 128 56 56 (401408)
I0523 15:14:00.364742 21880 net.cpp:156] Memory required for data: 90718208
I0523 15:14:00.364748 21880 layer_factory.hpp:77] Creating layer bn3_2
I0523 15:14:00.364753 21880 net.cpp:91] Creating Layer bn3_2
I0523 15:14:00.364755 21880 net.cpp:425] bn3_2 <- conv3_2
I0523 15:14:00.364759 21880 net.cpp:386] bn3_2 -> conv3_2 (in-place)
I0523 15:14:00.364934 21880 net.cpp:141] Setting up bn3_2
I0523 15:14:00.364939 21880 net.cpp:148] Top shape: 1 128 56 56 (401408)
I0523 15:14:00.364943 21880 net.cpp:156] Memory required for data: 92323840
I0523 15:14:00.364951 21880 layer_factory.hpp:77] Creating layer scale3_2
I0523 15:14:00.364958 21880 net.cpp:91] Creating Layer scale3_2
I0523 15:14:00.364960 21880 net.cpp:425] scale3_2 <- conv3_2
I0523 15:14:00.364964 21880 net.cpp:386] scale3_2 -> conv3_2 (in-place)
I0523 15:14:00.364995 21880 layer_factory.hpp:77] Creating layer scale3_2
I0523 15:14:00.365094 21880 net.cpp:141] Setting up scale3_2
I0523 15:14:00.365099 21880 net.cpp:148] Top shape: 1 128 56 56 (401408)
I0523 15:14:00.365102 21880 net.cpp:156] Memory required for data: 93929472
I0523 15:14:00.365105 21880 layer_factory.hpp:77] Creating layer relu3_2
I0523 15:14:00.365109 21880 net.cpp:91] Creating Layer relu3_2
I0523 15:14:00.365111 21880 net.cpp:425] relu3_2 <- conv3_2
I0523 15:14:00.365115 21880 net.cpp:386] relu3_2 -> conv3_2 (in-place)
I0523 15:14:00.365252 21880 net.cpp:141] Setting up relu3_2
I0523 15:14:00.365257 21880 net.cpp:148] Top shape: 1 128 56 56 (401408)
I0523 15:14:00.365259 21880 net.cpp:156] Memory required for data: 95535104
I0523 15:14:00.365262 21880 layer_factory.hpp:77] Creating layer pool3
I0523 15:14:00.365265 21880 layer_factory.cpp:90] cuDNN does not support multiple tops. Using Caffe's own pooling layer.
I0523 15:14:00.365269 21880 net.cpp:91] Creating Layer pool3
I0523 15:14:00.365272 21880 net.cpp:425] pool3 <- conv3_2
I0523 15:14:00.365275 21880 net.cpp:399] pool3 -> pool3
I0523 15:14:00.365279 21880 net.cpp:399] pool3 -> pool3_mask
I0523 15:14:00.365314 21880 net.cpp:141] Setting up pool3
I0523 15:14:00.365319 21880 net.cpp:148] Top shape: 1 128 28 28 (100352)
I0523 15:14:00.365321 21880 net.cpp:148] Top shape: 1 128 28 28 (100352)
I0523 15:14:00.365324 21880 net.cpp:156] Memory required for data: 96337920
I0523 15:14:00.365335 21880 layer_factory.hpp:77] Creating layer conv4_1
I0523 15:14:00.365342 21880 net.cpp:91] Creating Layer conv4_1
I0523 15:14:00.365345 21880 net.cpp:425] conv4_1 <- pool3
I0523 15:14:00.365350 21880 net.cpp:399] conv4_1 -> conv4_1
I0523 15:14:00.373551 21880 net.cpp:141] Setting up conv4_1
I0523 15:14:00.373570 21880 net.cpp:148] Top shape: 1 256 28 28 (200704)
I0523 15:14:00.373574 21880 net.cpp:156] Memory required for data: 97140736
I0523 15:14:00.373579 21880 layer_factory.hpp:77] Creating layer bn4_1
I0523 15:14:00.373586 21880 net.cpp:91] Creating Layer bn4_1
I0523 15:14:00.373590 21880 net.cpp:425] bn4_1 <- conv4_1
I0523 15:14:00.373596 21880 net.cpp:386] bn4_1 -> conv4_1 (in-place)
I0523 15:14:00.373800 21880 net.cpp:141] Setting up bn4_1
I0523 15:14:00.373806 21880 net.cpp:148] Top shape: 1 256 28 28 (200704)
I0523 15:14:00.373810 21880 net.cpp:156] Memory required for data: 97943552
I0523 15:14:00.373814 21880 layer_factory.hpp:77] Creating layer scale4_1
I0523 15:14:00.373821 21880 net.cpp:91] Creating Layer scale4_1
I0523 15:14:00.373823 21880 net.cpp:425] scale4_1 <- conv4_1
I0523 15:14:00.373827 21880 net.cpp:386] scale4_1 -> conv4_1 (in-place)
I0523 15:14:00.373864 21880 layer_factory.hpp:77] Creating layer scale4_1
I0523 15:14:00.373971 21880 net.cpp:141] Setting up scale4_1
I0523 15:14:00.373976 21880 net.cpp:148] Top shape: 1 256 28 28 (200704)
I0523 15:14:00.373980 21880 net.cpp:156] Memory required for data: 98746368
I0523 15:14:00.373982 21880 layer_factory.hpp:77] Creating layer relu4_1
I0523 15:14:00.373991 21880 net.cpp:91] Creating Layer relu4_1
I0523 15:14:00.373993 21880 net.cpp:425] relu4_1 <- conv4_1
I0523 15:14:00.373997 21880 net.cpp:386] relu4_1 -> conv4_1 (in-place)
I0523 15:14:00.374178 21880 net.cpp:141] Setting up relu4_1
I0523 15:14:00.374188 21880 net.cpp:148] Top shape: 1 256 28 28 (200704)
I0523 15:14:00.374191 21880 net.cpp:156] Memory required for data: 99549184
I0523 15:14:00.374193 21880 layer_factory.hpp:77] Creating layer conv4_2
I0523 15:14:00.374202 21880 net.cpp:91] Creating Layer conv4_2
I0523 15:14:00.374204 21880 net.cpp:425] conv4_2 <- conv4_1
I0523 15:14:00.374208 21880 net.cpp:399] conv4_2 -> conv4_2
I0523 15:14:00.389971 21880 net.cpp:141] Setting up conv4_2
I0523 15:14:00.389991 21880 net.cpp:148] Top shape: 1 256 28 28 (200704)
I0523 15:14:00.389993 21880 net.cpp:156] Memory required for data: 100352000
I0523 15:14:00.390000 21880 layer_factory.hpp:77] Creating layer bn4_2
I0523 15:14:00.390009 21880 net.cpp:91] Creating Layer bn4_2
I0523 15:14:00.390013 21880 net.cpp:425] bn4_2 <- conv4_2
I0523 15:14:00.390019 21880 net.cpp:386] bn4_2 -> conv4_2 (in-place)
I0523 15:14:00.390214 21880 net.cpp:141] Setting up bn4_2
I0523 15:14:00.390220 21880 net.cpp:148] Top shape: 1 256 28 28 (200704)
I0523 15:14:00.390223 21880 net.cpp:156] Memory required for data: 101154816
I0523 15:14:00.390228 21880 layer_factory.hpp:77] Creating layer scale4_2
I0523 15:14:00.390235 21880 net.cpp:91] Creating Layer scale4_2
I0523 15:14:00.390238 21880 net.cpp:425] scale4_2 <- conv4_2
I0523 15:14:00.390241 21880 net.cpp:386] scale4_2 -> conv4_2 (in-place)
I0523 15:14:00.390275 21880 layer_factory.hpp:77] Creating layer scale4_2
I0523 15:14:00.390379 21880 net.cpp:141] Setting up scale4_2
I0523 15:14:00.390384 21880 net.cpp:148] Top shape: 1 256 28 28 (200704)
I0523 15:14:00.390386 21880 net.cpp:156] Memory required for data: 101957632
I0523 15:14:00.390390 21880 layer_factory.hpp:77] Creating layer relu4_2
I0523 15:14:00.390396 21880 net.cpp:91] Creating Layer relu4_2
I0523 15:14:00.390398 21880 net.cpp:425] relu4_2 <- conv4_2
I0523 15:14:00.390401 21880 net.cpp:386] relu4_2 -> conv4_2 (in-place)
I0523 15:14:00.390643 21880 net.cpp:141] Setting up relu4_2
I0523 15:14:00.390652 21880 net.cpp:148] Top shape: 1 256 28 28 (200704)
I0523 15:14:00.390655 21880 net.cpp:156] Memory required for data: 102760448
I0523 15:14:00.390657 21880 layer_factory.hpp:77] Creating layer pool4
I0523 15:14:00.390661 21880 layer_factory.cpp:90] cuDNN does not support multiple tops. Using Caffe's own pooling layer.
I0523 15:14:00.390676 21880 net.cpp:91] Creating Layer pool4
I0523 15:14:00.390679 21880 net.cpp:425] pool4 <- conv4_2
I0523 15:14:00.390684 21880 net.cpp:399] pool4 -> pool4
I0523 15:14:00.390689 21880 net.cpp:399] pool4 -> pool4_mask
I0523 15:14:00.390732 21880 net.cpp:141] Setting up pool4
I0523 15:14:00.390736 21880 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0523 15:14:00.390739 21880 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0523 15:14:00.390741 21880 net.cpp:156] Memory required for data: 103161856
I0523 15:14:00.390743 21880 layer_factory.hpp:77] Creating layer conv5_1
I0523 15:14:00.390751 21880 net.cpp:91] Creating Layer conv5_1
I0523 15:14:00.390753 21880 net.cpp:425] conv5_1 <- pool4
I0523 15:14:00.390756 21880 net.cpp:399] conv5_1 -> conv5_1
I0523 15:14:00.406270 21880 net.cpp:141] Setting up conv5_1
I0523 15:14:00.406292 21880 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0523 15:14:00.406297 21880 net.cpp:156] Memory required for data: 103362560
I0523 15:14:00.406307 21880 layer_factory.hpp:77] Creating layer bn5_1
I0523 15:14:00.406322 21880 net.cpp:91] Creating Layer bn5_1
I0523 15:14:00.406332 21880 net.cpp:425] bn5_1 <- conv5_1
I0523 15:14:00.406339 21880 net.cpp:386] bn5_1 -> conv5_1 (in-place)
I0523 15:14:00.406590 21880 net.cpp:141] Setting up bn5_1
I0523 15:14:00.406601 21880 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0523 15:14:00.406605 21880 net.cpp:156] Memory required for data: 103563264
I0523 15:14:00.406615 21880 layer_factory.hpp:77] Creating layer scale5_1
I0523 15:14:00.406626 21880 net.cpp:91] Creating Layer scale5_1
I0523 15:14:00.406633 21880 net.cpp:425] scale5_1 <- conv5_1
I0523 15:14:00.406641 21880 net.cpp:386] scale5_1 -> conv5_1 (in-place)
I0523 15:14:00.406692 21880 layer_factory.hpp:77] Creating layer scale5_1
I0523 15:14:00.406807 21880 net.cpp:141] Setting up scale5_1
I0523 15:14:00.406816 21880 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0523 15:14:00.406819 21880 net.cpp:156] Memory required for data: 103763968
I0523 15:14:00.406826 21880 layer_factory.hpp:77] Creating layer relu5_1
I0523 15:14:00.406836 21880 net.cpp:91] Creating Layer relu5_1
I0523 15:14:00.406839 21880 net.cpp:425] relu5_1 <- conv5_1
I0523 15:14:00.406848 21880 net.cpp:386] relu5_1 -> conv5_1 (in-place)
I0523 15:14:00.407042 21880 net.cpp:141] Setting up relu5_1
I0523 15:14:00.407049 21880 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0523 15:14:00.407053 21880 net.cpp:156] Memory required for data: 103964672
I0523 15:14:00.407058 21880 layer_factory.hpp:77] Creating layer conv5_2
I0523 15:14:00.407071 21880 net.cpp:91] Creating Layer conv5_2
I0523 15:14:00.407075 21880 net.cpp:425] conv5_2 <- conv5_1
I0523 15:14:00.407083 21880 net.cpp:399] conv5_2 -> conv5_2
I0523 15:14:00.422659 21880 net.cpp:141] Setting up conv5_2
I0523 15:14:00.422688 21880 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0523 15:14:00.422691 21880 net.cpp:156] Memory required for data: 104165376
I0523 15:14:00.422698 21880 layer_factory.hpp:77] Creating layer bn5_2
I0523 15:14:00.422708 21880 net.cpp:91] Creating Layer bn5_2
I0523 15:14:00.422713 21880 net.cpp:425] bn5_2 <- conv5_2
I0523 15:14:00.422719 21880 net.cpp:386] bn5_2 -> conv5_2 (in-place)
I0523 15:14:00.422930 21880 net.cpp:141] Setting up bn5_2
I0523 15:14:00.422935 21880 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0523 15:14:00.422948 21880 net.cpp:156] Memory required for data: 104366080
I0523 15:14:00.422953 21880 layer_factory.hpp:77] Creating layer scale5_2
I0523 15:14:00.422960 21880 net.cpp:91] Creating Layer scale5_2
I0523 15:14:00.422963 21880 net.cpp:425] scale5_2 <- conv5_2
I0523 15:14:00.422967 21880 net.cpp:386] scale5_2 -> conv5_2 (in-place)
I0523 15:14:00.423017 21880 layer_factory.hpp:77] Creating layer scale5_2
I0523 15:14:00.423117 21880 net.cpp:141] Setting up scale5_2
I0523 15:14:00.423122 21880 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0523 15:14:00.423125 21880 net.cpp:156] Memory required for data: 104566784
I0523 15:14:00.423128 21880 layer_factory.hpp:77] Creating layer relu5_2
I0523 15:14:00.423133 21880 net.cpp:91] Creating Layer relu5_2
I0523 15:14:00.423148 21880 net.cpp:425] relu5_2 <- conv5_2
I0523 15:14:00.423151 21880 net.cpp:386] relu5_2 -> conv5_2 (in-place)
I0523 15:14:00.423297 21880 net.cpp:141] Setting up relu5_2
I0523 15:14:00.423305 21880 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0523 15:14:00.423306 21880 net.cpp:156] Memory required for data: 104767488
I0523 15:14:00.423310 21880 layer_factory.hpp:77] Creating layer pool5
I0523 15:14:00.423312 21880 layer_factory.cpp:90] cuDNN does not support multiple tops. Using Caffe's own pooling layer.
I0523 15:14:00.423316 21880 net.cpp:91] Creating Layer pool5
I0523 15:14:00.423318 21880 net.cpp:425] pool5 <- conv5_2
I0523 15:14:00.423323 21880 net.cpp:399] pool5 -> pool5
I0523 15:14:00.423328 21880 net.cpp:399] pool5 -> pool5_mask
I0523 15:14:00.423369 21880 net.cpp:141] Setting up pool5
I0523 15:14:00.423374 21880 net.cpp:148] Top shape: 1 256 7 7 (12544)
I0523 15:14:00.423377 21880 net.cpp:148] Top shape: 1 256 7 7 (12544)
I0523 15:14:00.423378 21880 net.cpp:156] Memory required for data: 104867840
I0523 15:14:00.423380 21880 layer_factory.hpp:77] Creating layer upsample5
I0523 15:14:00.423385 21880 net.cpp:91] Creating Layer upsample5
I0523 15:14:00.423388 21880 net.cpp:425] upsample5 <- pool5
I0523 15:14:00.423390 21880 net.cpp:425] upsample5 <- pool5_mask
I0523 15:14:00.423393 21880 net.cpp:399] upsample5 -> pool5_D
I0523 15:14:00.423414 21880 net.cpp:141] Setting up upsample5
I0523 15:14:00.423418 21880 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0523 15:14:00.423420 21880 net.cpp:156] Memory required for data: 105068544
I0523 15:14:00.423423 21880 layer_factory.hpp:77] Creating layer conv5_2_D
I0523 15:14:00.423431 21880 net.cpp:91] Creating Layer conv5_2_D
I0523 15:14:00.423434 21880 net.cpp:425] conv5_2_D <- pool5_D
I0523 15:14:00.423439 21880 net.cpp:399] conv5_2_D -> conv5_2_D
I0523 15:14:00.439096 21880 net.cpp:141] Setting up conv5_2_D
I0523 15:14:00.439127 21880 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0523 15:14:00.439131 21880 net.cpp:156] Memory required for data: 105269248
I0523 15:14:00.439137 21880 layer_factory.hpp:77] Creating layer bn5_2_D
I0523 15:14:00.439148 21880 net.cpp:91] Creating Layer bn5_2_D
I0523 15:14:00.439152 21880 net.cpp:425] bn5_2_D <- conv5_2_D
I0523 15:14:00.439158 21880 net.cpp:386] bn5_2_D -> conv5_2_D (in-place)
I0523 15:14:00.439355 21880 net.cpp:141] Setting up bn5_2_D
I0523 15:14:00.439360 21880 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0523 15:14:00.439363 21880 net.cpp:156] Memory required for data: 105469952
I0523 15:14:00.439378 21880 layer_factory.hpp:77] Creating layer scale5_2_D
I0523 15:14:00.439385 21880 net.cpp:91] Creating Layer scale5_2_D
I0523 15:14:00.439388 21880 net.cpp:425] scale5_2_D <- conv5_2_D
I0523 15:14:00.439391 21880 net.cpp:386] scale5_2_D -> conv5_2_D (in-place)
I0523 15:14:00.439437 21880 layer_factory.hpp:77] Creating layer scale5_2_D
I0523 15:14:00.439558 21880 net.cpp:141] Setting up scale5_2_D
I0523 15:14:00.439563 21880 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0523 15:14:00.439564 21880 net.cpp:156] Memory required for data: 105670656
I0523 15:14:00.439587 21880 layer_factory.hpp:77] Creating layer relu5_2_D
I0523 15:14:00.439595 21880 net.cpp:91] Creating Layer relu5_2_D
I0523 15:14:00.439597 21880 net.cpp:425] relu5_2_D <- conv5_2_D
I0523 15:14:00.439600 21880 net.cpp:386] relu5_2_D -> conv5_2_D (in-place)
I0523 15:14:00.439846 21880 net.cpp:141] Setting up relu5_2_D
I0523 15:14:00.439853 21880 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0523 15:14:00.439867 21880 net.cpp:156] Memory required for data: 105871360
I0523 15:14:00.439869 21880 layer_factory.hpp:77] Creating layer conv5_1_D
I0523 15:14:00.439877 21880 net.cpp:91] Creating Layer conv5_1_D
I0523 15:14:00.439880 21880 net.cpp:425] conv5_1_D <- conv5_2_D
I0523 15:14:00.439885 21880 net.cpp:399] conv5_1_D -> conv5_1_D
I0523 15:14:00.455083 21880 net.cpp:141] Setting up conv5_1_D
I0523 15:14:00.455112 21880 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0523 15:14:00.455116 21880 net.cpp:156] Memory required for data: 106072064
I0523 15:14:00.455135 21880 layer_factory.hpp:77] Creating layer bn5_1_D
I0523 15:14:00.455145 21880 net.cpp:91] Creating Layer bn5_1_D
I0523 15:14:00.455149 21880 net.cpp:425] bn5_1_D <- conv5_1_D
I0523 15:14:00.455154 21880 net.cpp:386] bn5_1_D -> conv5_1_D (in-place)
I0523 15:14:00.455351 21880 net.cpp:141] Setting up bn5_1_D
I0523 15:14:00.455356 21880 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0523 15:14:00.455369 21880 net.cpp:156] Memory required for data: 106272768
I0523 15:14:00.455374 21880 layer_factory.hpp:77] Creating layer scale5_1_D
I0523 15:14:00.455381 21880 net.cpp:91] Creating Layer scale5_1_D
I0523 15:14:00.455384 21880 net.cpp:425] scale5_1_D <- conv5_1_D
I0523 15:14:00.455386 21880 net.cpp:386] scale5_1_D -> conv5_1_D (in-place)
I0523 15:14:00.455433 21880 layer_factory.hpp:77] Creating layer scale5_1_D
I0523 15:14:00.455552 21880 net.cpp:141] Setting up scale5_1_D
I0523 15:14:00.455557 21880 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0523 15:14:00.455559 21880 net.cpp:156] Memory required for data: 106473472
I0523 15:14:00.455574 21880 layer_factory.hpp:77] Creating layer relu5_1_D
I0523 15:14:00.455579 21880 net.cpp:91] Creating Layer relu5_1_D
I0523 15:14:00.455580 21880 net.cpp:425] relu5_1_D <- conv5_1_D
I0523 15:14:00.455585 21880 net.cpp:386] relu5_1_D -> conv5_1_D (in-place)
I0523 15:14:00.455806 21880 net.cpp:141] Setting up relu5_1_D
I0523 15:14:00.455813 21880 net.cpp:148] Top shape: 1 256 14 14 (50176)
I0523 15:14:00.455826 21880 net.cpp:156] Memory required for data: 106674176
I0523 15:14:00.455828 21880 layer_factory.hpp:77] Creating layer upsample4
I0523 15:14:00.455833 21880 net.cpp:91] Creating Layer upsample4
I0523 15:14:00.455837 21880 net.cpp:425] upsample4 <- conv5_1_D
I0523 15:14:00.455839 21880 net.cpp:425] upsample4 <- pool4_mask
I0523 15:14:00.455843 21880 net.cpp:399] upsample4 -> pool4_D
I0523 15:14:00.455871 21880 net.cpp:141] Setting up upsample4
I0523 15:14:00.455885 21880 net.cpp:148] Top shape: 1 256 28 28 (200704)
I0523 15:14:00.455888 21880 net.cpp:156] Memory required for data: 107476992
I0523 15:14:00.455889 21880 layer_factory.hpp:77] Creating layer conv4_2_D
I0523 15:14:00.455907 21880 net.cpp:91] Creating Layer conv4_2_D
I0523 15:14:00.455909 21880 net.cpp:425] conv4_2_D <- pool4_D
I0523 15:14:00.455914 21880 net.cpp:399] conv4_2_D -> conv4_2_D
I0523 15:14:00.471230 21880 net.cpp:141] Setting up conv4_2_D
I0523 15:14:00.471261 21880 net.cpp:148] Top shape: 1 256 28 28 (200704)
I0523 15:14:00.471263 21880 net.cpp:156] Memory required for data: 108279808
I0523 15:14:00.471269 21880 layer_factory.hpp:77] Creating layer bn4_2_D
I0523 15:14:00.471279 21880 net.cpp:91] Creating Layer bn4_2_D
I0523 15:14:00.471282 21880 net.cpp:425] bn4_2_D <- conv4_2_D
I0523 15:14:00.471288 21880 net.cpp:386] bn4_2_D -> conv4_2_D (in-place)
I0523 15:14:00.471515 21880 net.cpp:141] Setting up bn4_2_D
I0523 15:14:00.471520 21880 net.cpp:148] Top shape: 1 256 28 28 (200704)
I0523 15:14:00.471534 21880 net.cpp:156] Memory required for data: 109082624
I0523 15:14:00.471539 21880 layer_factory.hpp:77] Creating layer scale4_2_D
I0523 15:14:00.471544 21880 net.cpp:91] Creating Layer scale4_2_D
I0523 15:14:00.471546 21880 net.cpp:425] scale4_2_D <- conv4_2_D
I0523 15:14:00.471550 21880 net.cpp:386] scale4_2_D -> conv4_2_D (in-place)
I0523 15:14:00.471611 21880 layer_factory.hpp:77] Creating layer scale4_2_D
I0523 15:14:00.471748 21880 net.cpp:141] Setting up scale4_2_D
I0523 15:14:00.471755 21880 net.cpp:148] Top shape: 1 256 28 28 (200704)
I0523 15:14:00.471766 21880 net.cpp:156] Memory required for data: 109885440
I0523 15:14:00.471771 21880 layer_factory.hpp:77] Creating layer relu4_2_D
I0523 15:14:00.471774 21880 net.cpp:91] Creating Layer relu4_2_D
I0523 15:14:00.471777 21880 net.cpp:425] relu4_2_D <- conv4_2_D
I0523 15:14:00.471781 21880 net.cpp:386] relu4_2_D -> conv4_2_D (in-place)
I0523 15:14:00.471930 21880 net.cpp:141] Setting up relu4_2_D
I0523 15:14:00.471937 21880 net.cpp:148] Top shape: 1 256 28 28 (200704)
I0523 15:14:00.471949 21880 net.cpp:156] Memory required for data: 110688256
I0523 15:14:00.471962 21880 layer_factory.hpp:77] Creating layer conv4_1_D
I0523 15:14:00.471971 21880 net.cpp:91] Creating Layer conv4_1_D
I0523 15:14:00.471973 21880 net.cpp:425] conv4_1_D <- conv4_2_D
I0523 15:14:00.471978 21880 net.cpp:399] conv4_1_D -> conv4_1_D
I0523 15:14:00.480310 21880 net.cpp:141] Setting up conv4_1_D
I0523 15:14:00.480337 21880 net.cpp:148] Top shape: 1 128 28 28 (100352)
I0523 15:14:00.480340 21880 net.cpp:156] Memory required for data: 111089664
I0523 15:14:00.480346 21880 layer_factory.hpp:77] Creating layer bn4_1_D
I0523 15:14:00.480355 21880 net.cpp:91] Creating Layer bn4_1_D
I0523 15:14:00.480360 21880 net.cpp:425] bn4_1_D <- conv4_1_D
I0523 15:14:00.480363 21880 net.cpp:386] bn4_1_D -> conv4_1_D (in-place)
I0523 15:14:00.480563 21880 net.cpp:141] Setting up bn4_1_D
I0523 15:14:00.480568 21880 net.cpp:148] Top shape: 1 128 28 28 (100352)
I0523 15:14:00.480581 21880 net.cpp:156] Memory required for data: 111491072
I0523 15:14:00.480587 21880 layer_factory.hpp:77] Creating layer scale4_1_D
I0523 15:14:00.480592 21880 net.cpp:91] Creating Layer scale4_1_D
I0523 15:14:00.480595 21880 net.cpp:425] scale4_1_D <- conv4_1_D
I0523 15:14:00.480598 21880 net.cpp:386] scale4_1_D -> conv4_1_D (in-place)
I0523 15:14:00.480644 21880 layer_factory.hpp:77] Creating layer scale4_1_D
I0523 15:14:00.480768 21880 net.cpp:141] Setting up scale4_1_D
I0523 15:14:00.480773 21880 net.cpp:148] Top shape: 1 128 28 28 (100352)
I0523 15:14:00.480785 21880 net.cpp:156] Memory required for data: 111892480
I0523 15:14:00.480789 21880 layer_factory.hpp:77] Creating layer relu4_1_D
I0523 15:14:00.480801 21880 net.cpp:91] Creating Layer relu4_1_D
I0523 15:14:00.480803 21880 net.cpp:425] relu4_1_D <- conv4_1_D
I0523 15:14:00.480806 21880 net.cpp:386] relu4_1_D -> conv4_1_D (in-place)
I0523 15:14:00.481040 21880 net.cpp:141] Setting up relu4_1_D
I0523 15:14:00.481048 21880 net.cpp:148] Top shape: 1 128 28 28 (100352)
I0523 15:14:00.481061 21880 net.cpp:156] Memory required for data: 112293888
I0523 15:14:00.481065 21880 layer_factory.hpp:77] Creating layer upsample3
I0523 15:14:00.481070 21880 net.cpp:91] Creating Layer upsample3
I0523 15:14:00.481072 21880 net.cpp:425] upsample3 <- conv4_1_D
I0523 15:14:00.481076 21880 net.cpp:425] upsample3 <- pool3_mask
I0523 15:14:00.481079 21880 net.cpp:399] upsample3 -> pool3_D
I0523 15:14:00.481106 21880 net.cpp:141] Setting up upsample3
I0523 15:14:00.481120 21880 net.cpp:148] Top shape: 1 128 56 56 (401408)
I0523 15:14:00.481122 21880 net.cpp:156] Memory required for data: 113899520
I0523 15:14:00.481124 21880 layer_factory.hpp:77] Creating layer conv3_2_D
I0523 15:14:00.481142 21880 net.cpp:91] Creating Layer conv3_2_D
I0523 15:14:00.481144 21880 net.cpp:425] conv3_2_D <- pool3_D
I0523 15:14:00.481148 21880 net.cpp:399] conv3_2_D -> conv3_2_D
I0523 15:14:00.485746 21880 net.cpp:141] Setting up conv3_2_D
I0523 15:14:00.485766 21880 net.cpp:148] Top shape: 1 128 56 56 (401408)
I0523 15:14:00.485769 21880 net.cpp:156] Memory required for data: 115505152
I0523 15:14:00.485774 21880 layer_factory.hpp:77] Creating layer bn3_2_D
I0523 15:14:00.485780 21880 net.cpp:91] Creating Layer bn3_2_D
I0523 15:14:00.485781 21880 net.cpp:425] bn3_2_D <- conv3_2_D
I0523 15:14:00.485786 21880 net.cpp:386] bn3_2_D -> conv3_2_D (in-place)
I0523 15:14:00.485981 21880 net.cpp:141] Setting up bn3_2_D
I0523 15:14:00.485987 21880 net.cpp:148] Top shape: 1 128 56 56 (401408)
I0523 15:14:00.485999 21880 net.cpp:156] Memory required for data: 117110784
I0523 15:14:00.486004 21880 layer_factory.hpp:77] Creating layer scale3_2_D
I0523 15:14:00.486008 21880 net.cpp:91] Creating Layer scale3_2_D
I0523 15:14:00.486011 21880 net.cpp:425] scale3_2_D <- conv3_2_D
I0523 15:14:00.486014 21880 net.cpp:386] scale3_2_D -> conv3_2_D (in-place)
I0523 15:14:00.486058 21880 layer_factory.hpp:77] Creating layer scale3_2_D
I0523 15:14:00.486176 21880 net.cpp:141] Setting up scale3_2_D
I0523 15:14:00.486181 21880 net.cpp:148] Top shape: 1 128 56 56 (401408)
I0523 15:14:00.486183 21880 net.cpp:156] Memory required for data: 118716416
I0523 15:14:00.486199 21880 layer_factory.hpp:77] Creating layer relu3_2_D
I0523 15:14:00.486203 21880 net.cpp:91] Creating Layer relu3_2_D
I0523 15:14:00.486208 21880 net.cpp:425] relu3_2_D <- conv3_2_D
I0523 15:14:00.486212 21880 net.cpp:386] relu3_2_D -> conv3_2_D (in-place)
I0523 15:14:00.486353 21880 net.cpp:141] Setting up relu3_2_D
I0523 15:14:00.486361 21880 net.cpp:148] Top shape: 1 128 56 56 (401408)
I0523 15:14:00.486362 21880 net.cpp:156] Memory required for data: 120322048
I0523 15:14:00.486366 21880 layer_factory.hpp:77] Creating layer conv3_1_D
I0523 15:14:00.486373 21880 net.cpp:91] Creating Layer conv3_1_D
I0523 15:14:00.486377 21880 net.cpp:425] conv3_1_D <- conv3_2_D
I0523 15:14:00.486380 21880 net.cpp:399] conv3_1_D -> conv3_1_D
I0523 15:14:00.488934 21880 net.cpp:141] Setting up conv3_1_D
I0523 15:14:00.488953 21880 net.cpp:148] Top shape: 1 64 56 56 (200704)
I0523 15:14:00.488955 21880 net.cpp:156] Memory required for data: 121124864
I0523 15:14:00.488960 21880 layer_factory.hpp:77] Creating layer bn3_1_D
I0523 15:14:00.488965 21880 net.cpp:91] Creating Layer bn3_1_D
I0523 15:14:00.488967 21880 net.cpp:425] bn3_1_D <- conv3_1_D
I0523 15:14:00.488970 21880 net.cpp:386] bn3_1_D -> conv3_1_D (in-place)
I0523 15:14:00.489173 21880 net.cpp:141] Setting up bn3_1_D
I0523 15:14:00.489178 21880 net.cpp:148] Top shape: 1 64 56 56 (200704)
I0523 15:14:00.489192 21880 net.cpp:156] Memory required for data: 121927680
I0523 15:14:00.489197 21880 layer_factory.hpp:77] Creating layer scale3_1_D
I0523 15:14:00.489202 21880 net.cpp:91] Creating Layer scale3_1_D
I0523 15:14:00.489204 21880 net.cpp:425] scale3_1_D <- conv3_1_D
I0523 15:14:00.489207 21880 net.cpp:386] scale3_1_D -> conv3_1_D (in-place)
I0523 15:14:00.489253 21880 layer_factory.hpp:77] Creating layer scale3_1_D
I0523 15:14:00.489378 21880 net.cpp:141] Setting up scale3_1_D
I0523 15:14:00.489383 21880 net.cpp:148] Top shape: 1 64 56 56 (200704)
I0523 15:14:00.489385 21880 net.cpp:156] Memory required for data: 122730496
I0523 15:14:00.489389 21880 layer_factory.hpp:77] Creating layer relu3_1_D
I0523 15:14:00.489393 21880 net.cpp:91] Creating Layer relu3_1_D
I0523 15:14:00.489395 21880 net.cpp:425] relu3_1_D <- conv3_1_D
I0523 15:14:00.489398 21880 net.cpp:386] relu3_1_D -> conv3_1_D (in-place)
I0523 15:14:00.489549 21880 net.cpp:141] Setting up relu3_1_D
I0523 15:14:00.489555 21880 net.cpp:148] Top shape: 1 64 56 56 (200704)
I0523 15:14:00.489567 21880 net.cpp:156] Memory required for data: 123533312
I0523 15:14:00.489569 21880 layer_factory.hpp:77] Creating layer upsample2
I0523 15:14:00.489574 21880 net.cpp:91] Creating Layer upsample2
I0523 15:14:00.489576 21880 net.cpp:425] upsample2 <- conv3_1_D
I0523 15:14:00.489579 21880 net.cpp:425] upsample2 <- pool2_mask
I0523 15:14:00.489584 21880 net.cpp:399] upsample2 -> pool2_D
I0523 15:14:00.489610 21880 net.cpp:141] Setting up upsample2
I0523 15:14:00.489624 21880 net.cpp:148] Top shape: 1 64 112 112 (802816)
I0523 15:14:00.489626 21880 net.cpp:156] Memory required for data: 126744576
I0523 15:14:00.489629 21880 layer_factory.hpp:77] Creating layer conv2_2_D
I0523 15:14:00.489645 21880 net.cpp:91] Creating Layer conv2_2_D
I0523 15:14:00.489646 21880 net.cpp:425] conv2_2_D <- pool2_D
I0523 15:14:00.489650 21880 net.cpp:399] conv2_2_D -> conv2_2_D
I0523 15:14:00.491360 21880 net.cpp:141] Setting up conv2_2_D
I0523 15:14:00.491369 21880 net.cpp:148] Top shape: 1 64 112 112 (802816)
I0523 15:14:00.491382 21880 net.cpp:156] Memory required for data: 129955840
I0523 15:14:00.491387 21880 layer_factory.hpp:77] Creating layer bn2_2_D
I0523 15:14:00.491394 21880 net.cpp:91] Creating Layer bn2_2_D
I0523 15:14:00.491395 21880 net.cpp:425] bn2_2_D <- conv2_2_D
I0523 15:14:00.491400 21880 net.cpp:386] bn2_2_D -> conv2_2_D (in-place)
I0523 15:14:00.491880 21880 net.cpp:141] Setting up bn2_2_D
I0523 15:14:00.491888 21880 net.cpp:148] Top shape: 1 64 112 112 (802816)
I0523 15:14:00.491901 21880 net.cpp:156] Memory required for data: 133167104
I0523 15:14:00.491906 21880 layer_factory.hpp:77] Creating layer scale2_2_D
I0523 15:14:00.491919 21880 net.cpp:91] Creating Layer scale2_2_D
I0523 15:14:00.491921 21880 net.cpp:425] scale2_2_D <- conv2_2_D
I0523 15:14:00.491925 21880 net.cpp:386] scale2_2_D -> conv2_2_D (in-place)
I0523 15:14:00.491976 21880 layer_factory.hpp:77] Creating layer scale2_2_D
I0523 15:14:00.492105 21880 net.cpp:141] Setting up scale2_2_D
I0523 15:14:00.492110 21880 net.cpp:148] Top shape: 1 64 112 112 (802816)
I0523 15:14:00.492122 21880 net.cpp:156] Memory required for data: 136378368
I0523 15:14:00.492126 21880 layer_factory.hpp:77] Creating layer relu2_2_D
I0523 15:14:00.492130 21880 net.cpp:91] Creating Layer relu2_2_D
I0523 15:14:00.492133 21880 net.cpp:425] relu2_2_D <- conv2_2_D
I0523 15:14:00.492136 21880 net.cpp:386] relu2_2_D -> conv2_2_D (in-place)
I0523 15:14:00.492378 21880 net.cpp:141] Setting up relu2_2_D
I0523 15:14:00.492398 21880 net.cpp:148] Top shape: 1 64 112 112 (802816)
I0523 15:14:00.492400 21880 net.cpp:156] Memory required for data: 139589632
I0523 15:14:00.492403 21880 layer_factory.hpp:77] Creating layer conv2_1_D
I0523 15:14:00.492409 21880 net.cpp:91] Creating Layer conv2_1_D
I0523 15:14:00.492411 21880 net.cpp:425] conv2_1_D <- conv2_2_D
I0523 15:14:00.492415 21880 net.cpp:399] conv2_1_D -> conv2_1_D
I0523 15:14:00.493604 21880 net.cpp:141] Setting up conv2_1_D
I0523 15:14:00.493613 21880 net.cpp:148] Top shape: 1 32 112 112 (401408)
I0523 15:14:00.493626 21880 net.cpp:156] Memory required for data: 141195264
I0523 15:14:00.493630 21880 layer_factory.hpp:77] Creating layer bn2_1_D
I0523 15:14:00.493634 21880 net.cpp:91] Creating Layer bn2_1_D
I0523 15:14:00.493638 21880 net.cpp:425] bn2_1_D <- conv2_1_D
I0523 15:14:00.493641 21880 net.cpp:386] bn2_1_D -> conv2_1_D (in-place)
I0523 15:14:00.493842 21880 net.cpp:141] Setting up bn2_1_D
I0523 15:14:00.493847 21880 net.cpp:148] Top shape: 1 32 112 112 (401408)
I0523 15:14:00.493860 21880 net.cpp:156] Memory required for data: 142800896
I0523 15:14:00.493865 21880 layer_factory.hpp:77] Creating layer scale2_1_D
I0523 15:14:00.493870 21880 net.cpp:91] Creating Layer scale2_1_D
I0523 15:14:00.493871 21880 net.cpp:425] scale2_1_D <- conv2_1_D
I0523 15:14:00.493875 21880 net.cpp:386] scale2_1_D -> conv2_1_D (in-place)
I0523 15:14:00.493918 21880 layer_factory.hpp:77] Creating layer scale2_1_D
I0523 15:14:00.494048 21880 net.cpp:141] Setting up scale2_1_D
I0523 15:14:00.494053 21880 net.cpp:148] Top shape: 1 32 112 112 (401408)
I0523 15:14:00.494065 21880 net.cpp:156] Memory required for data: 144406528
I0523 15:14:00.494069 21880 layer_factory.hpp:77] Creating layer relu2_1_D
I0523 15:14:00.494072 21880 net.cpp:91] Creating Layer relu2_1_D
I0523 15:14:00.494076 21880 net.cpp:425] relu2_1_D <- conv2_1_D
I0523 15:14:00.494077 21880 net.cpp:386] relu2_1_D -> conv2_1_D (in-place)
I0523 15:14:00.494223 21880 net.cpp:141] Setting up relu2_1_D
I0523 15:14:00.494230 21880 net.cpp:148] Top shape: 1 32 112 112 (401408)
I0523 15:14:00.494241 21880 net.cpp:156] Memory required for data: 146012160
I0523 15:14:00.494243 21880 layer_factory.hpp:77] Creating layer upsample1
I0523 15:14:00.494248 21880 net.cpp:91] Creating Layer upsample1
I0523 15:14:00.494251 21880 net.cpp:425] upsample1 <- conv2_1_D
I0523 15:14:00.494253 21880 net.cpp:425] upsample1 <- pool1_mask
I0523 15:14:00.494257 21880 net.cpp:399] upsample1 -> pool1_D
I0523 15:14:00.494292 21880 net.cpp:141] Setting up upsample1
I0523 15:14:00.494297 21880 net.cpp:148] Top shape: 1 32 224 224 (1605632)
I0523 15:14:00.494308 21880 net.cpp:156] Memory required for data: 152434688
I0523 15:14:00.494310 21880 layer_factory.hpp:77] Creating layer conv1_2_D
I0523 15:14:00.494318 21880 net.cpp:91] Creating Layer conv1_2_D
I0523 15:14:00.494319 21880 net.cpp:425] conv1_2_D <- pool1_D
I0523 15:14:00.494323 21880 net.cpp:399] conv1_2_D -> conv1_2_D
I0523 15:14:00.495437 21880 net.cpp:141] Setting up conv1_2_D
I0523 15:14:00.495447 21880 net.cpp:148] Top shape: 1 32 224 224 (1605632)
I0523 15:14:00.495450 21880 net.cpp:156] Memory required for data: 158857216
I0523 15:14:00.495453 21880 layer_factory.hpp:77] Creating layer bn1_2_D
I0523 15:14:00.495467 21880 net.cpp:91] Creating Layer bn1_2_D
I0523 15:14:00.495471 21880 net.cpp:425] bn1_2_D <- conv1_2_D
I0523 15:14:00.495473 21880 net.cpp:386] bn1_2_D -> conv1_2_D (in-place)
I0523 15:14:00.495702 21880 net.cpp:141] Setting up bn1_2_D
I0523 15:14:00.495707 21880 net.cpp:148] Top shape: 1 32 224 224 (1605632)
I0523 15:14:00.495708 21880 net.cpp:156] Memory required for data: 165279744
I0523 15:14:00.495713 21880 layer_factory.hpp:77] Creating layer scale1_2_D
I0523 15:14:00.495718 21880 net.cpp:91] Creating Layer scale1_2_D
I0523 15:14:00.495720 21880 net.cpp:425] scale1_2_D <- conv1_2_D
I0523 15:14:00.495724 21880 net.cpp:386] scale1_2_D -> conv1_2_D (in-place)
I0523 15:14:00.495760 21880 layer_factory.hpp:77] Creating layer scale1_2_D
I0523 15:14:00.495942 21880 net.cpp:141] Setting up scale1_2_D
I0523 15:14:00.495949 21880 net.cpp:148] Top shape: 1 32 224 224 (1605632)
I0523 15:14:00.495950 21880 net.cpp:156] Memory required for data: 171702272
I0523 15:14:00.495954 21880 layer_factory.hpp:77] Creating layer relu1_2_D
I0523 15:14:00.495959 21880 net.cpp:91] Creating Layer relu1_2_D
I0523 15:14:00.495960 21880 net.cpp:425] relu1_2_D <- conv1_2_D
I0523 15:14:00.495964 21880 net.cpp:386] relu1_2_D -> conv1_2_D (in-place)
I0523 15:14:00.496109 21880 net.cpp:141] Setting up relu1_2_D
I0523 15:14:00.496115 21880 net.cpp:148] Top shape: 1 32 224 224 (1605632)
I0523 15:14:00.496117 21880 net.cpp:156] Memory required for data: 178124800
I0523 15:14:00.496119 21880 layer_factory.hpp:77] Creating layer conv1_1_D
I0523 15:14:00.496126 21880 net.cpp:91] Creating Layer conv1_1_D
I0523 15:14:00.496129 21880 net.cpp:425] conv1_1_D <- conv1_2_D
I0523 15:14:00.496134 21880 net.cpp:399] conv1_1_D -> conv1_1_D
I0523 15:14:00.497230 21880 net.cpp:141] Setting up conv1_1_D
I0523 15:14:00.497239 21880 net.cpp:148] Top shape: 1 2 224 224 (100352)
I0523 15:14:00.497252 21880 net.cpp:156] Memory required for data: 178526208
I0523 15:14:00.497257 21880 layer_factory.hpp:77] Creating layer conv1_1_D_conv1_1_D_0_split
I0523 15:14:00.497263 21880 net.cpp:91] Creating Layer conv1_1_D_conv1_1_D_0_split
I0523 15:14:00.497265 21880 net.cpp:425] conv1_1_D_conv1_1_D_0_split <- conv1_1_D
I0523 15:14:00.497269 21880 net.cpp:399] conv1_1_D_conv1_1_D_0_split -> conv1_1_D_conv1_1_D_0_split_0
I0523 15:14:00.497275 21880 net.cpp:399] conv1_1_D_conv1_1_D_0_split -> conv1_1_D_conv1_1_D_0_split_1
I0523 15:14:00.497328 21880 net.cpp:141] Setting up conv1_1_D_conv1_1_D_0_split
I0523 15:14:00.497333 21880 net.cpp:148] Top shape: 1 2 224 224 (100352)
I0523 15:14:00.497345 21880 net.cpp:148] Top shape: 1 2 224 224 (100352)
I0523 15:14:00.497347 21880 net.cpp:156] Memory required for data: 179329024
I0523 15:14:00.497349 21880 layer_factory.hpp:77] Creating layer loss
I0523 15:14:00.497354 21880 net.cpp:91] Creating Layer loss
I0523 15:14:00.497356 21880 net.cpp:425] loss <- conv1_1_D_conv1_1_D_0_split_0
I0523 15:14:00.497359 21880 net.cpp:425] loss <- label_data_1_split_0
I0523 15:14:00.497362 21880 net.cpp:399] loss -> loss
I0523 15:14:00.497367 21880 layer_factory.hpp:77] Creating layer loss
I0523 15:14:00.497776 21880 net.cpp:141] Setting up loss
I0523 15:14:00.497786 21880 net.cpp:148] Top shape: (1)
I0523 15:14:00.497797 21880 net.cpp:151]     with loss weight 1
I0523 15:14:00.497805 21880 net.cpp:156] Memory required for data: 179329028
I0523 15:14:00.497808 21880 layer_factory.hpp:77] Creating layer accuracy
I0523 15:14:00.497813 21880 net.cpp:91] Creating Layer accuracy
I0523 15:14:00.497815 21880 net.cpp:425] accuracy <- conv1_1_D_conv1_1_D_0_split_1
I0523 15:14:00.497818 21880 net.cpp:425] accuracy <- label_data_1_split_1
I0523 15:14:00.497825 21880 net.cpp:399] accuracy -> accuracy
I0523 15:14:00.497831 21880 net.cpp:141] Setting up accuracy
I0523 15:14:00.497834 21880 net.cpp:148] Top shape: (1)
I0523 15:14:00.497836 21880 net.cpp:156] Memory required for data: 179329032
I0523 15:14:00.497838 21880 net.cpp:219] accuracy does not need backward computation.
I0523 15:14:00.497841 21880 net.cpp:217] loss needs backward computation.
I0523 15:14:00.497853 21880 net.cpp:217] conv1_1_D_conv1_1_D_0_split needs backward computation.
I0523 15:14:00.497854 21880 net.cpp:217] conv1_1_D needs backward computation.
I0523 15:14:00.497858 21880 net.cpp:217] relu1_2_D needs backward computation.
I0523 15:14:00.497859 21880 net.cpp:217] scale1_2_D needs backward computation.
I0523 15:14:00.497861 21880 net.cpp:217] bn1_2_D needs backward computation.
I0523 15:14:00.497862 21880 net.cpp:217] conv1_2_D needs backward computation.
I0523 15:14:00.497864 21880 net.cpp:217] upsample1 needs backward computation.
I0523 15:14:00.497867 21880 net.cpp:217] relu2_1_D needs backward computation.
I0523 15:14:00.497870 21880 net.cpp:217] scale2_1_D needs backward computation.
I0523 15:14:00.497872 21880 net.cpp:217] bn2_1_D needs backward computation.
I0523 15:14:00.497874 21880 net.cpp:217] conv2_1_D needs backward computation.
I0523 15:14:00.497876 21880 net.cpp:217] relu2_2_D needs backward computation.
I0523 15:14:00.497879 21880 net.cpp:217] scale2_2_D needs backward computation.
I0523 15:14:00.497880 21880 net.cpp:217] bn2_2_D needs backward computation.
I0523 15:14:00.497882 21880 net.cpp:217] conv2_2_D needs backward computation.
I0523 15:14:00.497884 21880 net.cpp:217] upsample2 needs backward computation.
I0523 15:14:00.497887 21880 net.cpp:217] relu3_1_D needs backward computation.
I0523 15:14:00.497890 21880 net.cpp:217] scale3_1_D needs backward computation.
I0523 15:14:00.497892 21880 net.cpp:217] bn3_1_D needs backward computation.
I0523 15:14:00.497895 21880 net.cpp:217] conv3_1_D needs backward computation.
I0523 15:14:00.497897 21880 net.cpp:217] relu3_2_D needs backward computation.
I0523 15:14:00.497900 21880 net.cpp:217] scale3_2_D needs backward computation.
I0523 15:14:00.497901 21880 net.cpp:217] bn3_2_D needs backward computation.
I0523 15:14:00.497903 21880 net.cpp:217] conv3_2_D needs backward computation.
I0523 15:14:00.497906 21880 net.cpp:217] upsample3 needs backward computation.
I0523 15:14:00.497908 21880 net.cpp:217] relu4_1_D needs backward computation.
I0523 15:14:00.497911 21880 net.cpp:217] scale4_1_D needs backward computation.
I0523 15:14:00.497913 21880 net.cpp:217] bn4_1_D needs backward computation.
I0523 15:14:00.497916 21880 net.cpp:217] conv4_1_D needs backward computation.
I0523 15:14:00.497920 21880 net.cpp:217] relu4_2_D needs backward computation.
I0523 15:14:00.497921 21880 net.cpp:217] scale4_2_D needs backward computation.
I0523 15:14:00.497923 21880 net.cpp:217] bn4_2_D needs backward computation.
I0523 15:14:00.497925 21880 net.cpp:217] conv4_2_D needs backward computation.
I0523 15:14:00.497928 21880 net.cpp:217] upsample4 needs backward computation.
I0523 15:14:00.497931 21880 net.cpp:217] relu5_1_D needs backward computation.
I0523 15:14:00.497934 21880 net.cpp:217] scale5_1_D needs backward computation.
I0523 15:14:00.497936 21880 net.cpp:217] bn5_1_D needs backward computation.
I0523 15:14:00.497938 21880 net.cpp:217] conv5_1_D needs backward computation.
I0523 15:14:00.497941 21880 net.cpp:217] relu5_2_D needs backward computation.
I0523 15:14:00.497943 21880 net.cpp:217] scale5_2_D needs backward computation.
I0523 15:14:00.497946 21880 net.cpp:217] bn5_2_D needs backward computation.
I0523 15:14:00.497947 21880 net.cpp:217] conv5_2_D needs backward computation.
I0523 15:14:00.497951 21880 net.cpp:217] upsample5 needs backward computation.
I0523 15:14:00.497954 21880 net.cpp:217] pool5 needs backward computation.
I0523 15:14:00.497957 21880 net.cpp:217] relu5_2 needs backward computation.
I0523 15:14:00.497959 21880 net.cpp:217] scale5_2 needs backward computation.
I0523 15:14:00.497961 21880 net.cpp:217] bn5_2 needs backward computation.
I0523 15:14:00.497963 21880 net.cpp:217] conv5_2 needs backward computation.
I0523 15:14:00.497967 21880 net.cpp:217] relu5_1 needs backward computation.
I0523 15:14:00.497968 21880 net.cpp:217] scale5_1 needs backward computation.
I0523 15:14:00.497970 21880 net.cpp:217] bn5_1 needs backward computation.
I0523 15:14:00.497972 21880 net.cpp:217] conv5_1 needs backward computation.
I0523 15:14:00.497979 21880 net.cpp:217] pool4 needs backward computation.
I0523 15:14:00.497982 21880 net.cpp:217] relu4_2 needs backward computation.
I0523 15:14:00.497984 21880 net.cpp:217] scale4_2 needs backward computation.
I0523 15:14:00.497987 21880 net.cpp:217] bn4_2 needs backward computation.
I0523 15:14:00.497989 21880 net.cpp:217] conv4_2 needs backward computation.
I0523 15:14:00.497992 21880 net.cpp:217] relu4_1 needs backward computation.
I0523 15:14:00.497993 21880 net.cpp:217] scale4_1 needs backward computation.
I0523 15:14:00.497995 21880 net.cpp:217] bn4_1 needs backward computation.
I0523 15:14:00.497998 21880 net.cpp:217] conv4_1 needs backward computation.
I0523 15:14:00.498000 21880 net.cpp:217] pool3 needs backward computation.
I0523 15:14:00.498003 21880 net.cpp:217] relu3_2 needs backward computation.
I0523 15:14:00.498006 21880 net.cpp:217] scale3_2 needs backward computation.
I0523 15:14:00.498008 21880 net.cpp:217] bn3_2 needs backward computation.
I0523 15:14:00.498010 21880 net.cpp:217] conv3_2 needs backward computation.
I0523 15:14:00.498013 21880 net.cpp:217] relu3_1 needs backward computation.
I0523 15:14:00.498015 21880 net.cpp:217] scale3_1 needs backward computation.
I0523 15:14:00.498018 21880 net.cpp:217] bn3_1 needs backward computation.
I0523 15:14:00.498019 21880 net.cpp:217] conv3_1 needs backward computation.
I0523 15:14:00.498023 21880 net.cpp:217] pool2 needs backward computation.
I0523 15:14:00.498025 21880 net.cpp:217] relu2_2 needs backward computation.
I0523 15:14:00.498028 21880 net.cpp:217] scale2_2 needs backward computation.
I0523 15:14:00.498030 21880 net.cpp:217] bn2_2 needs backward computation.
I0523 15:14:00.498033 21880 net.cpp:217] conv2_2 needs backward computation.
I0523 15:14:00.498034 21880 net.cpp:217] relu2_1 needs backward computation.
I0523 15:14:00.498037 21880 net.cpp:217] scale2_1 needs backward computation.
I0523 15:14:00.498039 21880 net.cpp:217] bn2_1 needs backward computation.
I0523 15:14:00.498041 21880 net.cpp:217] conv2_1 needs backward computation.
I0523 15:14:00.498045 21880 net.cpp:217] pool1 needs backward computation.
I0523 15:14:00.498047 21880 net.cpp:217] relu1_2 needs backward computation.
I0523 15:14:00.498049 21880 net.cpp:217] scale1_2 needs backward computation.
I0523 15:14:00.498051 21880 net.cpp:217] bn1_2 needs backward computation.
I0523 15:14:00.498054 21880 net.cpp:217] conv1_2 needs backward computation.
I0523 15:14:00.498056 21880 net.cpp:217] relu1_1 needs backward computation.
I0523 15:14:00.498059 21880 net.cpp:217] scale1_1 needs backward computation.
I0523 15:14:00.498060 21880 net.cpp:217] bn1_1 needs backward computation.
I0523 15:14:00.498062 21880 net.cpp:217] conv1_1 needs backward computation.
I0523 15:14:00.498067 21880 net.cpp:219] label_data_1_split does not need backward computation.
I0523 15:14:00.498070 21880 net.cpp:219] data does not need backward computation.
I0523 15:14:00.498072 21880 net.cpp:261] This network produces output accuracy
I0523 15:14:00.498075 21880 net.cpp:261] This network produces output loss
I0523 15:14:00.498106 21880 net.cpp:274] Network initialization done.
I0523 15:14:00.498330 21880 solver.cpp:60] Solver scaffolding done.
I0523 15:14:00.502480 21880 caffe.cpp:209] Resuming from data/models/segnet_iter_5000.solverstate
I0523 15:14:00.584270 21880 sgd_solver.cpp:318] SGDSolver: restoring history
I0523 15:14:00.594867 21880 caffe.cpp:219] Starting Optimization
I0523 15:14:00.594895 21880 solver.cpp:279] Solving segnet
I0523 15:14:00.594899 21880 solver.cpp:280] Learning Rate Policy: step
I0523 15:14:00.597647 21880 solver.cpp:337] Iteration 5000, Testing net (#0)
I0523 15:14:01.470727 21880 solver.cpp:404]     Test net output #0: accuracy = 0.98815
I0523 15:14:01.470755 21880 solver.cpp:404]     Test net output #1: loss = 0.0326981 (* 1 = 0.0326981 loss)
I0523 15:14:02.202735 21880 solver.cpp:228] Iteration 5000, loss = 0.0252585
I0523 15:14:02.202759 21880 solver.cpp:244]     Train net output #0: accuracy = 0.990705
I0523 15:14:02.202767 21880 solver.cpp:244]     Train net output #1: loss = 0.0252585 (* 1 = 0.0252585 loss)
I0523 15:14:02.202810 21880 sgd_solver.cpp:106] Iteration 5000, lr = 0.001
I0523 15:14:16.385725 21880 solver.cpp:228] Iteration 5020, loss = 0.0201454
I0523 15:14:16.385754 21880 solver.cpp:244]     Train net output #0: accuracy = 0.992276
I0523 15:14:16.385761 21880 solver.cpp:244]     Train net output #1: loss = 0.0201454 (* 1 = 0.0201454 loss)
I0523 15:14:16.385767 21880 sgd_solver.cpp:106] Iteration 5020, lr = 0.001
I0523 15:14:30.873921 21880 solver.cpp:228] Iteration 5040, loss = 0.0231382
I0523 15:14:30.874078 21880 solver.cpp:244]     Train net output #0: accuracy = 0.989082
I0523 15:14:30.874088 21880 solver.cpp:244]     Train net output #1: loss = 0.0231382 (* 1 = 0.0231382 loss)
I0523 15:14:30.874102 21880 sgd_solver.cpp:106] Iteration 5040, lr = 0.001
I0523 15:14:45.415004 21880 solver.cpp:228] Iteration 5060, loss = 0.0174377
I0523 15:14:45.415030 21880 solver.cpp:244]     Train net output #0: accuracy = 0.993896
I0523 15:14:45.415036 21880 solver.cpp:244]     Train net output #1: loss = 0.0174377 (* 1 = 0.0174377 loss)
I0523 15:14:45.415040 21880 sgd_solver.cpp:106] Iteration 5060, lr = 0.001
I0523 15:14:59.978575 21880 solver.cpp:228] Iteration 5080, loss = 0.020902
I0523 15:14:59.978603 21880 solver.cpp:244]     Train net output #0: accuracy = 0.992722
I0523 15:14:59.978611 21880 solver.cpp:244]     Train net output #1: loss = 0.020902 (* 1 = 0.020902 loss)
I0523 15:14:59.978616 21880 sgd_solver.cpp:106] Iteration 5080, lr = 0.001
I0523 15:15:14.190201 21880 solver.cpp:337] Iteration 5100, Testing net (#0)
I0523 15:15:15.049655 21880 solver.cpp:404]     Test net output #0: accuracy = 0.988033
I0523 15:15:15.049679 21880 solver.cpp:404]     Test net output #1: loss = 0.0302583 (* 1 = 0.0302583 loss)
I0523 15:15:15.455814 21880 solver.cpp:228] Iteration 5100, loss = 0.0230043
I0523 15:15:15.455842 21880 solver.cpp:244]     Train net output #0: accuracy = 0.990038
I0523 15:15:15.455849 21880 solver.cpp:244]     Train net output #1: loss = 0.0230043 (* 1 = 0.0230043 loss)
I0523 15:15:15.455855 21880 sgd_solver.cpp:106] Iteration 5100, lr = 0.001
I0523 15:15:30.105270 21880 solver.cpp:228] Iteration 5120, loss = 0.022822
I0523 15:15:30.105361 21880 solver.cpp:244]     Train net output #0: accuracy = 0.991537
I0523 15:15:30.105381 21880 solver.cpp:244]     Train net output #1: loss = 0.022822 (* 1 = 0.022822 loss)
I0523 15:15:30.105394 21880 sgd_solver.cpp:106] Iteration 5120, lr = 0.001
I0523 15:15:44.728390 21880 solver.cpp:228] Iteration 5140, loss = 0.0191662
I0523 15:15:44.728447 21880 solver.cpp:244]     Train net output #0: accuracy = 0.992472
I0523 15:15:44.728457 21880 solver.cpp:244]     Train net output #1: loss = 0.0191662 (* 1 = 0.0191662 loss)
I0523 15:15:44.728462 21880 sgd_solver.cpp:106] Iteration 5140, lr = 0.001
I0523 15:15:59.347085 21880 solver.cpp:228] Iteration 5160, loss = 0.0201113
I0523 15:15:59.347112 21880 solver.cpp:244]     Train net output #0: accuracy = 0.99189
I0523 15:15:59.347120 21880 solver.cpp:244]     Train net output #1: loss = 0.0201113 (* 1 = 0.0201113 loss)
I0523 15:15:59.347124 21880 sgd_solver.cpp:106] Iteration 5160, lr = 0.001
I0523 15:16:13.968029 21880 solver.cpp:228] Iteration 5180, loss = 0.0308152
I0523 15:16:13.968057 21880 solver.cpp:244]     Train net output #0: accuracy = 0.988404
I0523 15:16:13.968065 21880 solver.cpp:244]     Train net output #1: loss = 0.0308152 (* 1 = 0.0308152 loss)
I0523 15:16:13.968070 21880 sgd_solver.cpp:106] Iteration 5180, lr = 0.001
I0523 15:16:28.315613 21880 solver.cpp:337] Iteration 5200, Testing net (#0)
I0523 15:16:29.179019 21880 solver.cpp:404]     Test net output #0: accuracy = 0.982034
I0523 15:16:29.179044 21880 solver.cpp:404]     Test net output #1: loss = 0.042641 (* 1 = 0.042641 loss)
I0523 15:16:29.585136 21880 solver.cpp:228] Iteration 5200, loss = 0.026055
I0523 15:16:29.585162 21880 solver.cpp:244]     Train net output #0: accuracy = 0.990007
I0523 15:16:29.585170 21880 solver.cpp:244]     Train net output #1: loss = 0.026055 (* 1 = 0.026055 loss)
I0523 15:16:29.585173 21880 sgd_solver.cpp:106] Iteration 5200, lr = 0.001
I0523 15:16:44.213043 21880 solver.cpp:228] Iteration 5220, loss = 0.0271152
I0523 15:16:44.213080 21880 solver.cpp:244]     Train net output #0: accuracy = 0.989237
I0523 15:16:44.213088 21880 solver.cpp:244]     Train net output #1: loss = 0.0271152 (* 1 = 0.0271152 loss)
I0523 15:16:44.213093 21880 sgd_solver.cpp:106] Iteration 5220, lr = 0.001
I0523 15:16:58.843554 21880 solver.cpp:228] Iteration 5240, loss = 0.0187918
I0523 15:16:58.843664 21880 solver.cpp:244]     Train net output #0: accuracy = 0.993439
I0523 15:16:58.843683 21880 solver.cpp:244]     Train net output #1: loss = 0.0187918 (* 1 = 0.0187918 loss)
I0523 15:16:58.843688 21880 sgd_solver.cpp:106] Iteration 5240, lr = 0.001
I0523 15:17:13.474951 21880 solver.cpp:228] Iteration 5260, loss = 0.0215293
I0523 15:17:13.474978 21880 solver.cpp:244]     Train net output #0: accuracy = 0.992528
I0523 15:17:13.474985 21880 solver.cpp:244]     Train net output #1: loss = 0.0215293 (* 1 = 0.0215293 loss)
I0523 15:17:13.474992 21880 sgd_solver.cpp:106] Iteration 5260, lr = 0.001
I0523 15:17:28.101824 21880 solver.cpp:228] Iteration 5280, loss = 0.0283777
I0523 15:17:28.101850 21880 solver.cpp:244]     Train net output #0: accuracy = 0.99029
I0523 15:17:28.101857 21880 solver.cpp:244]     Train net output #1: loss = 0.0283777 (* 1 = 0.0283777 loss)
I0523 15:17:28.101862 21880 sgd_solver.cpp:106] Iteration 5280, lr = 0.001
I0523 15:17:42.324978 21880 solver.cpp:337] Iteration 5300, Testing net (#0)
I0523 15:17:43.217874 21880 solver.cpp:404]     Test net output #0: accuracy = 0.988081
I0523 15:17:43.217896 21880 solver.cpp:404]     Test net output #1: loss = 0.0350334 (* 1 = 0.0350334 loss)
I0523 15:17:43.627141 21880 solver.cpp:228] Iteration 5300, loss = 0.0245111
I0523 15:17:43.627166 21880 solver.cpp:244]     Train net output #0: accuracy = 0.990315
I0523 15:17:43.627174 21880 solver.cpp:244]     Train net output #1: loss = 0.0245111 (* 1 = 0.0245111 loss)
I0523 15:17:43.627180 21880 sgd_solver.cpp:106] Iteration 5300, lr = 0.001
I0523 15:17:58.247838 21880 solver.cpp:228] Iteration 5320, loss = 0.0229044
I0523 15:17:58.247865 21880 solver.cpp:244]     Train net output #0: accuracy = 0.990669
I0523 15:17:58.247872 21880 solver.cpp:244]     Train net output #1: loss = 0.0229044 (* 1 = 0.0229044 loss)
I0523 15:17:58.247877 21880 sgd_solver.cpp:106] Iteration 5320, lr = 0.001
I0523 15:18:12.888944 21880 solver.cpp:228] Iteration 5340, loss = 0.0310826
I0523 15:18:12.889024 21880 solver.cpp:244]     Train net output #0: accuracy = 0.987422
I0523 15:18:12.889042 21880 solver.cpp:244]     Train net output #1: loss = 0.0310826 (* 1 = 0.0310826 loss)
I0523 15:18:12.889047 21880 sgd_solver.cpp:106] Iteration 5340, lr = 0.001
I0523 15:18:27.526346 21880 solver.cpp:228] Iteration 5360, loss = 0.0227367
I0523 15:18:27.526373 21880 solver.cpp:244]     Train net output #0: accuracy = 0.99123
I0523 15:18:27.526381 21880 solver.cpp:244]     Train net output #1: loss = 0.0227367 (* 1 = 0.0227367 loss)
I0523 15:18:27.526386 21880 sgd_solver.cpp:106] Iteration 5360, lr = 0.001
I0523 15:18:42.157867 21880 solver.cpp:228] Iteration 5380, loss = 0.0198968
I0523 15:18:42.157892 21880 solver.cpp:244]     Train net output #0: accuracy = 0.992303
I0523 15:18:42.157899 21880 solver.cpp:244]     Train net output #1: loss = 0.0198968 (* 1 = 0.0198968 loss)
I0523 15:18:42.157904 21880 sgd_solver.cpp:106] Iteration 5380, lr = 0.001
I0523 15:18:56.380506 21880 solver.cpp:337] Iteration 5400, Testing net (#0)
I0523 15:18:57.244065 21880 solver.cpp:404]     Test net output #0: accuracy = 0.9858
I0523 15:18:57.244089 21880 solver.cpp:404]     Test net output #1: loss = 0.0362716 (* 1 = 0.0362716 loss)
I0523 15:18:57.649552 21880 solver.cpp:228] Iteration 5400, loss = 0.0174874
I0523 15:18:57.649577 21880 solver.cpp:244]     Train net output #0: accuracy = 0.993608
I0523 15:18:57.649585 21880 solver.cpp:244]     Train net output #1: loss = 0.0174874 (* 1 = 0.0174874 loss)
I0523 15:18:57.649590 21880 sgd_solver.cpp:106] Iteration 5400, lr = 0.001
I0523 15:19:12.548444 21880 solver.cpp:228] Iteration 5420, loss = 0.0242648
I0523 15:19:12.548473 21880 solver.cpp:244]     Train net output #0: accuracy = 0.989372
I0523 15:19:12.548480 21880 solver.cpp:244]     Train net output #1: loss = 0.0242648 (* 1 = 0.0242648 loss)
I0523 15:19:12.548485 21880 sgd_solver.cpp:106] Iteration 5420, lr = 0.001
I0523 15:19:27.177927 21880 solver.cpp:228] Iteration 5440, loss = 0.0195637
I0523 15:19:27.178023 21880 solver.cpp:244]     Train net output #0: accuracy = 0.99196
I0523 15:19:27.178033 21880 solver.cpp:244]     Train net output #1: loss = 0.0195637 (* 1 = 0.0195637 loss)
I0523 15:19:27.178048 21880 sgd_solver.cpp:106] Iteration 5440, lr = 0.001
I0523 15:19:41.834775 21880 solver.cpp:228] Iteration 5460, loss = 0.0269286
I0523 15:19:41.834803 21880 solver.cpp:244]     Train net output #0: accuracy = 0.990134
I0523 15:19:41.834810 21880 solver.cpp:244]     Train net output #1: loss = 0.0269286 (* 1 = 0.0269286 loss)
I0523 15:19:41.834815 21880 sgd_solver.cpp:106] Iteration 5460, lr = 0.001
I0523 15:19:56.477130 21880 solver.cpp:228] Iteration 5480, loss = 0.0220906
I0523 15:19:56.477156 21880 solver.cpp:244]     Train net output #0: accuracy = 0.992761
I0523 15:19:56.477164 21880 solver.cpp:244]     Train net output #1: loss = 0.0220906 (* 1 = 0.0220906 loss)
I0523 15:19:56.477170 21880 sgd_solver.cpp:106] Iteration 5480, lr = 0.001
I0523 15:20:10.709851 21880 solver.cpp:454] Snapshotting to binary proto file data/models/segnet_iter_5500.caffemodel
I0523 15:20:10.771206 21880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file data/models/segnet_iter_5500.solverstate
I0523 15:20:10.793184 21880 solver.cpp:337] Iteration 5500, Testing net (#0)
I0523 15:20:11.654616 21880 solver.cpp:404]     Test net output #0: accuracy = 0.986094
I0523 15:20:11.654652 21880 solver.cpp:404]     Test net output #1: loss = 0.0446189 (* 1 = 0.0446189 loss)
I0523 15:20:12.059864 21880 solver.cpp:228] Iteration 5500, loss = 0.026077
I0523 15:20:12.059890 21880 solver.cpp:244]     Train net output #0: accuracy = 0.988997
I0523 15:20:12.059898 21880 solver.cpp:244]     Train net output #1: loss = 0.026077 (* 1 = 0.026077 loss)
I0523 15:20:12.059903 21880 sgd_solver.cpp:106] Iteration 5500, lr = 0.001
I0523 15:20:26.696473 21880 solver.cpp:228] Iteration 5520, loss = 0.0207709
I0523 15:20:26.696501 21880 solver.cpp:244]     Train net output #0: accuracy = 0.989592
I0523 15:20:26.696508 21880 solver.cpp:244]     Train net output #1: loss = 0.0207709 (* 1 = 0.0207709 loss)
I0523 15:20:26.696513 21880 sgd_solver.cpp:106] Iteration 5520, lr = 0.001
I0523 15:20:41.327440 21880 solver.cpp:228] Iteration 5540, loss = 0.028134
I0523 15:20:41.327538 21880 solver.cpp:244]     Train net output #0: accuracy = 0.989285
I0523 15:20:41.327545 21880 solver.cpp:244]     Train net output #1: loss = 0.028134 (* 1 = 0.028134 loss)
I0523 15:20:41.327549 21880 sgd_solver.cpp:106] Iteration 5540, lr = 0.001
I0523 15:20:56.003669 21880 solver.cpp:228] Iteration 5560, loss = 0.0301244
I0523 15:20:56.003708 21880 solver.cpp:244]     Train net output #0: accuracy = 0.989021
I0523 15:20:56.003715 21880 solver.cpp:244]     Train net output #1: loss = 0.0301244 (* 1 = 0.0301244 loss)
I0523 15:20:56.003720 21880 sgd_solver.cpp:106] Iteration 5560, lr = 0.001
I0523 15:21:10.664095 21880 solver.cpp:228] Iteration 5580, loss = 0.0289837
I0523 15:21:10.664122 21880 solver.cpp:244]     Train net output #0: accuracy = 0.988852
I0523 15:21:10.664129 21880 solver.cpp:244]     Train net output #1: loss = 0.0289837 (* 1 = 0.0289837 loss)
I0523 15:21:10.664134 21880 sgd_solver.cpp:106] Iteration 5580, lr = 0.001
I0523 15:21:24.908385 21880 solver.cpp:337] Iteration 5600, Testing net (#0)
I0523 15:21:25.778612 21880 solver.cpp:404]     Test net output #0: accuracy = 0.987709
I0523 15:21:25.778635 21880 solver.cpp:404]     Test net output #1: loss = 0.0365102 (* 1 = 0.0365102 loss)
I0523 15:21:26.183413 21880 solver.cpp:228] Iteration 5600, loss = 0.0201269
I0523 15:21:26.183439 21880 solver.cpp:244]     Train net output #0: accuracy = 0.990952
I0523 15:21:26.183446 21880 solver.cpp:244]     Train net output #1: loss = 0.0201269 (* 1 = 0.0201269 loss)
I0523 15:21:26.183451 21880 sgd_solver.cpp:106] Iteration 5600, lr = 0.001
I0523 15:21:40.845397 21880 solver.cpp:228] Iteration 5620, loss = 0.0269023
I0523 15:21:40.845424 21880 solver.cpp:244]     Train net output #0: accuracy = 0.988412
I0523 15:21:40.845432 21880 solver.cpp:244]     Train net output #1: loss = 0.0269023 (* 1 = 0.0269023 loss)
I0523 15:21:40.845438 21880 sgd_solver.cpp:106] Iteration 5620, lr = 0.001
I0523 15:21:55.492367 21880 solver.cpp:228] Iteration 5640, loss = 0.0401943
I0523 15:21:55.492492 21880 solver.cpp:244]     Train net output #0: accuracy = 0.987193
I0523 15:21:55.492513 21880 solver.cpp:244]     Train net output #1: loss = 0.0401943 (* 1 = 0.0401943 loss)
I0523 15:21:55.492518 21880 sgd_solver.cpp:106] Iteration 5640, lr = 0.001
I0523 15:22:10.161461 21880 solver.cpp:228] Iteration 5660, loss = 0.02326
I0523 15:22:10.161489 21880 solver.cpp:244]     Train net output #0: accuracy = 0.992127
I0523 15:22:10.161495 21880 solver.cpp:244]     Train net output #1: loss = 0.02326 (* 1 = 0.02326 loss)
I0523 15:22:10.161501 21880 sgd_solver.cpp:106] Iteration 5660, lr = 0.001
I0523 15:22:24.867768 21880 solver.cpp:228] Iteration 5680, loss = 0.0177386
I0523 15:22:24.867794 21880 solver.cpp:244]     Train net output #0: accuracy = 0.992666
I0523 15:22:24.867800 21880 solver.cpp:244]     Train net output #1: loss = 0.0177386 (* 1 = 0.0177386 loss)
I0523 15:22:24.867805 21880 sgd_solver.cpp:106] Iteration 5680, lr = 0.001
I0523 15:22:39.106096 21880 solver.cpp:337] Iteration 5700, Testing net (#0)
I0523 15:22:39.973168 21880 solver.cpp:404]     Test net output #0: accuracy = 0.985313
I0523 15:22:39.973194 21880 solver.cpp:404]     Test net output #1: loss = 0.0388825 (* 1 = 0.0388825 loss)
I0523 15:22:40.377815 21880 solver.cpp:228] Iteration 5700, loss = 0.0216065
I0523 15:22:40.377840 21880 solver.cpp:244]     Train net output #0: accuracy = 0.991515
I0523 15:22:40.377846 21880 solver.cpp:244]     Train net output #1: loss = 0.0216065 (* 1 = 0.0216065 loss)
I0523 15:22:40.377851 21880 sgd_solver.cpp:106] Iteration 5700, lr = 0.001
I0523 15:22:55.017206 21880 solver.cpp:228] Iteration 5720, loss = 0.0273264
I0523 15:22:55.017235 21880 solver.cpp:244]     Train net output #0: accuracy = 0.988847
I0523 15:22:55.017242 21880 solver.cpp:244]     Train net output #1: loss = 0.0273264 (* 1 = 0.0273264 loss)
I0523 15:22:55.017247 21880 sgd_solver.cpp:106] Iteration 5720, lr = 0.001
I0523 15:23:09.644278 21880 solver.cpp:228] Iteration 5740, loss = 0.0260654
I0523 15:23:09.644373 21880 solver.cpp:244]     Train net output #0: accuracy = 0.989438
I0523 15:23:09.644382 21880 solver.cpp:244]     Train net output #1: loss = 0.0260654 (* 1 = 0.0260654 loss)
I0523 15:23:09.644387 21880 sgd_solver.cpp:106] Iteration 5740, lr = 0.001
I0523 15:23:24.350455 21880 solver.cpp:228] Iteration 5760, loss = 0.0280374
I0523 15:23:24.350484 21880 solver.cpp:244]     Train net output #0: accuracy = 0.989588
I0523 15:23:24.350492 21880 solver.cpp:244]     Train net output #1: loss = 0.0280374 (* 1 = 0.0280374 loss)
I0523 15:23:24.350507 21880 sgd_solver.cpp:106] Iteration 5760, lr = 0.001
I0523 15:23:39.032198 21880 solver.cpp:228] Iteration 5780, loss = 0.02642
I0523 15:23:39.032224 21880 solver.cpp:244]     Train net output #0: accuracy = 0.990102
I0523 15:23:39.032232 21880 solver.cpp:244]     Train net output #1: loss = 0.02642 (* 1 = 0.02642 loss)
I0523 15:23:39.032237 21880 sgd_solver.cpp:106] Iteration 5780, lr = 0.001
I0523 15:23:53.250213 21880 solver.cpp:337] Iteration 5800, Testing net (#0)
I0523 15:23:54.140537 21880 solver.cpp:404]     Test net output #0: accuracy = 0.985927
I0523 15:23:54.140573 21880 solver.cpp:404]     Test net output #1: loss = 0.0479329 (* 1 = 0.0479329 loss)
I0523 15:23:54.544708 21880 solver.cpp:228] Iteration 5800, loss = 0.0267832
I0523 15:23:54.544734 21880 solver.cpp:244]     Train net output #0: accuracy = 0.990152
I0523 15:23:54.544741 21880 solver.cpp:244]     Train net output #1: loss = 0.0267832 (* 1 = 0.0267832 loss)
I0523 15:23:54.544747 21880 sgd_solver.cpp:106] Iteration 5800, lr = 0.001
I0523 15:24:09.158996 21880 solver.cpp:228] Iteration 5820, loss = 0.0267006
I0523 15:24:09.159023 21880 solver.cpp:244]     Train net output #0: accuracy = 0.988965
I0523 15:24:09.159030 21880 solver.cpp:244]     Train net output #1: loss = 0.0267006 (* 1 = 0.0267006 loss)
I0523 15:24:09.159035 21880 sgd_solver.cpp:106] Iteration 5820, lr = 0.001
I0523 15:24:23.774394 21880 solver.cpp:228] Iteration 5840, loss = 0.0188792
I0523 15:24:23.774513 21880 solver.cpp:244]     Train net output #0: accuracy = 0.994638
I0523 15:24:23.774523 21880 solver.cpp:244]     Train net output #1: loss = 0.0188792 (* 1 = 0.0188792 loss)
I0523 15:24:23.774528 21880 sgd_solver.cpp:106] Iteration 5840, lr = 0.001
I0523 15:24:38.392597 21880 solver.cpp:228] Iteration 5860, loss = 0.0249411
I0523 15:24:38.392624 21880 solver.cpp:244]     Train net output #0: accuracy = 0.991178
I0523 15:24:38.392632 21880 solver.cpp:244]     Train net output #1: loss = 0.0249411 (* 1 = 0.0249411 loss)
I0523 15:24:38.392635 21880 sgd_solver.cpp:106] Iteration 5860, lr = 0.001
I0523 15:24:53.009228 21880 solver.cpp:228] Iteration 5880, loss = 0.0255993
I0523 15:24:53.009255 21880 solver.cpp:244]     Train net output #0: accuracy = 0.989578
I0523 15:24:53.009263 21880 solver.cpp:244]     Train net output #1: loss = 0.0255993 (* 1 = 0.0255993 loss)
I0523 15:24:53.009268 21880 sgd_solver.cpp:106] Iteration 5880, lr = 0.001
I0523 15:25:07.227973 21880 solver.cpp:337] Iteration 5900, Testing net (#0)
I0523 15:25:08.090529 21880 solver.cpp:404]     Test net output #0: accuracy = 0.988218
I0523 15:25:08.090553 21880 solver.cpp:404]     Test net output #1: loss = 0.0334379 (* 1 = 0.0334379 loss)
I0523 15:25:08.496628 21880 solver.cpp:228] Iteration 5900, loss = 0.0267581
I0523 15:25:08.496654 21880 solver.cpp:244]     Train net output #0: accuracy = 0.990163
I0523 15:25:08.496661 21880 solver.cpp:244]     Train net output #1: loss = 0.0267581 (* 1 = 0.0267581 loss)
I0523 15:25:08.496666 21880 sgd_solver.cpp:106] Iteration 5900, lr = 0.001
I0523 15:25:23.119941 21880 solver.cpp:228] Iteration 5920, loss = 0.0269169
I0523 15:25:23.119966 21880 solver.cpp:244]     Train net output #0: accuracy = 0.989293
I0523 15:25:23.119973 21880 solver.cpp:244]     Train net output #1: loss = 0.0269169 (* 1 = 0.0269169 loss)
I0523 15:25:23.119978 21880 sgd_solver.cpp:106] Iteration 5920, lr = 0.001
I0523 15:25:37.750464 21880 solver.cpp:228] Iteration 5940, loss = 0.018612
I0523 15:25:37.750542 21880 solver.cpp:244]     Train net output #0: accuracy = 0.991908
I0523 15:25:37.750552 21880 solver.cpp:244]     Train net output #1: loss = 0.018612 (* 1 = 0.018612 loss)
I0523 15:25:37.750556 21880 sgd_solver.cpp:106] Iteration 5940, lr = 0.001
I0523 15:25:52.373517 21880 solver.cpp:228] Iteration 5960, loss = 0.0204781
I0523 15:25:52.373543 21880 solver.cpp:244]     Train net output #0: accuracy = 0.991387
I0523 15:25:52.373550 21880 solver.cpp:244]     Train net output #1: loss = 0.0204781 (* 1 = 0.0204781 loss)
I0523 15:25:52.373554 21880 sgd_solver.cpp:106] Iteration 5960, lr = 0.001
I0523 15:26:07.023967 21880 solver.cpp:228] Iteration 5980, loss = 0.0326399
I0523 15:26:07.023994 21880 solver.cpp:244]     Train net output #0: accuracy = 0.988576
I0523 15:26:07.024003 21880 solver.cpp:244]     Train net output #1: loss = 0.0326399 (* 1 = 0.0326399 loss)
I0523 15:26:07.024008 21880 sgd_solver.cpp:106] Iteration 5980, lr = 0.001
I0523 15:26:21.255890 21880 solver.cpp:454] Snapshotting to binary proto file data/models/segnet_iter_6000.caffemodel
I0523 15:26:23.143646 21880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file data/models/segnet_iter_6000.solverstate
I0523 15:26:23.164825 21880 solver.cpp:337] Iteration 6000, Testing net (#0)
I0523 15:26:24.032234 21880 solver.cpp:404]     Test net output #0: accuracy = 0.989874
I0523 15:26:24.032258 21880 solver.cpp:404]     Test net output #1: loss = 0.0264076 (* 1 = 0.0264076 loss)
I0523 15:26:24.441187 21880 solver.cpp:228] Iteration 6000, loss = 0.0242252
I0523 15:26:24.441213 21880 solver.cpp:244]     Train net output #0: accuracy = 0.99107
I0523 15:26:24.441220 21880 solver.cpp:244]     Train net output #1: loss = 0.0242252 (* 1 = 0.0242252 loss)
I0523 15:26:24.441226 21880 sgd_solver.cpp:106] Iteration 6000, lr = 0.0001
I0523 15:26:39.066323 21880 solver.cpp:228] Iteration 6020, loss = 0.028418
I0523 15:26:39.066350 21880 solver.cpp:244]     Train net output #0: accuracy = 0.987533
I0523 15:26:39.066357 21880 solver.cpp:244]     Train net output #1: loss = 0.028418 (* 1 = 0.028418 loss)
I0523 15:26:39.066362 21880 sgd_solver.cpp:106] Iteration 6020, lr = 0.0001
I0523 15:26:53.692348 21880 solver.cpp:228] Iteration 6040, loss = 0.0207576
I0523 15:26:53.692452 21880 solver.cpp:244]     Train net output #0: accuracy = 0.989764
I0523 15:26:53.692471 21880 solver.cpp:244]     Train net output #1: loss = 0.0207576 (* 1 = 0.0207576 loss)
I0523 15:26:53.692476 21880 sgd_solver.cpp:106] Iteration 6040, lr = 0.0001
I0523 15:27:08.323032 21880 solver.cpp:228] Iteration 6060, loss = 0.0263263
I0523 15:27:08.323055 21880 solver.cpp:244]     Train net output #0: accuracy = 0.98993
I0523 15:27:08.323062 21880 solver.cpp:244]     Train net output #1: loss = 0.0263263 (* 1 = 0.0263263 loss)
I0523 15:27:08.323067 21880 sgd_solver.cpp:106] Iteration 6060, lr = 0.0001
I0523 15:27:23.090493 21880 solver.cpp:228] Iteration 6080, loss = 0.0229925
I0523 15:27:23.090522 21880 solver.cpp:244]     Train net output #0: accuracy = 0.990701
I0523 15:27:23.090529 21880 solver.cpp:244]     Train net output #1: loss = 0.0229925 (* 1 = 0.0229925 loss)
I0523 15:27:23.090535 21880 sgd_solver.cpp:106] Iteration 6080, lr = 0.0001
I0523 15:27:37.706920 21880 solver.cpp:337] Iteration 6100, Testing net (#0)
I0523 15:27:38.599639 21880 solver.cpp:404]     Test net output #0: accuracy = 0.987965
I0523 15:27:38.599663 21880 solver.cpp:404]     Test net output #1: loss = 0.0368537 (* 1 = 0.0368537 loss)
I0523 15:27:39.009320 21880 solver.cpp:228] Iteration 6100, loss = 0.0273366
I0523 15:27:39.009346 21880 solver.cpp:244]     Train net output #0: accuracy = 0.988728
I0523 15:27:39.009354 21880 solver.cpp:244]     Train net output #1: loss = 0.0273366 (* 1 = 0.0273366 loss)
I0523 15:27:39.009361 21880 sgd_solver.cpp:106] Iteration 6100, lr = 0.0001
I0523 15:27:53.698276 21880 solver.cpp:228] Iteration 6120, loss = 0.0295728
I0523 15:27:53.698312 21880 solver.cpp:244]     Train net output #0: accuracy = 0.990681
I0523 15:27:53.698318 21880 solver.cpp:244]     Train net output #1: loss = 0.0295728 (* 1 = 0.0295728 loss)
I0523 15:27:53.698323 21880 sgd_solver.cpp:106] Iteration 6120, lr = 0.0001
I0523 15:28:08.382300 21880 solver.cpp:228] Iteration 6140, loss = 0.0215893
I0523 15:28:08.382400 21880 solver.cpp:244]     Train net output #0: accuracy = 0.991018
I0523 15:28:08.382418 21880 solver.cpp:244]     Train net output #1: loss = 0.0215893 (* 1 = 0.0215893 loss)
I0523 15:28:08.382433 21880 sgd_solver.cpp:106] Iteration 6140, lr = 0.0001
I0523 15:28:23.077738 21880 solver.cpp:228] Iteration 6160, loss = 0.022261
I0523 15:28:23.077764 21880 solver.cpp:244]     Train net output #0: accuracy = 0.991155
I0523 15:28:23.077772 21880 solver.cpp:244]     Train net output #1: loss = 0.022261 (* 1 = 0.022261 loss)
I0523 15:28:23.077777 21880 sgd_solver.cpp:106] Iteration 6160, lr = 0.0001
I0523 15:28:37.782516 21880 solver.cpp:228] Iteration 6180, loss = 0.0251366
I0523 15:28:37.782541 21880 solver.cpp:244]     Train net output #0: accuracy = 0.989182
I0523 15:28:37.782548 21880 solver.cpp:244]     Train net output #1: loss = 0.0251366 (* 1 = 0.0251366 loss)
I0523 15:28:37.782553 21880 sgd_solver.cpp:106] Iteration 6180, lr = 0.0001
I0523 15:28:52.074604 21880 solver.cpp:337] Iteration 6200, Testing net (#0)
I0523 15:28:52.941956 21880 solver.cpp:404]     Test net output #0: accuracy = 0.98866
I0523 15:28:52.941979 21880 solver.cpp:404]     Test net output #1: loss = 0.0322688 (* 1 = 0.0322688 loss)
I0523 15:28:53.348503 21880 solver.cpp:228] Iteration 6200, loss = 0.0195497
I0523 15:28:53.348531 21880 solver.cpp:244]     Train net output #0: accuracy = 0.992845
I0523 15:28:53.348537 21880 solver.cpp:244]     Train net output #1: loss = 0.0195497 (* 1 = 0.0195497 loss)
I0523 15:28:53.348541 21880 sgd_solver.cpp:106] Iteration 6200, lr = 0.0001
I0523 15:29:08.043401 21880 solver.cpp:228] Iteration 6220, loss = 0.021354
I0523 15:29:08.043428 21880 solver.cpp:244]     Train net output #0: accuracy = 0.991197
I0523 15:29:08.043436 21880 solver.cpp:244]     Train net output #1: loss = 0.021354 (* 1 = 0.021354 loss)
I0523 15:29:08.043439 21880 sgd_solver.cpp:106] Iteration 6220, lr = 0.0001
I0523 15:29:22.765461 21880 solver.cpp:228] Iteration 6240, loss = 0.0180387
I0523 15:29:22.765593 21880 solver.cpp:244]     Train net output #0: accuracy = 0.993893
I0523 15:29:22.765614 21880 solver.cpp:244]     Train net output #1: loss = 0.0180387 (* 1 = 0.0180387 loss)
I0523 15:29:22.765619 21880 sgd_solver.cpp:106] Iteration 6240, lr = 0.0001
I0523 15:29:37.490020 21880 solver.cpp:228] Iteration 6260, loss = 0.0230763
I0523 15:29:37.490057 21880 solver.cpp:244]     Train net output #0: accuracy = 0.990916
I0523 15:29:37.490064 21880 solver.cpp:244]     Train net output #1: loss = 0.0230763 (* 1 = 0.0230763 loss)
I0523 15:29:37.490069 21880 sgd_solver.cpp:106] Iteration 6260, lr = 0.0001
I0523 15:29:52.207564 21880 solver.cpp:228] Iteration 6280, loss = 0.0234638
I0523 15:29:52.207590 21880 solver.cpp:244]     Train net output #0: accuracy = 0.990401
I0523 15:29:52.207598 21880 solver.cpp:244]     Train net output #1: loss = 0.0234638 (* 1 = 0.0234638 loss)
I0523 15:29:52.207603 21880 sgd_solver.cpp:106] Iteration 6280, lr = 0.0001
I0523 15:30:06.494186 21880 solver.cpp:337] Iteration 6300, Testing net (#0)
I0523 15:30:07.360746 21880 solver.cpp:404]     Test net output #0: accuracy = 0.989513
I0523 15:30:07.360770 21880 solver.cpp:404]     Test net output #1: loss = 0.0312096 (* 1 = 0.0312096 loss)
I0523 15:30:07.768414 21880 solver.cpp:228] Iteration 6300, loss = 0.0215207
I0523 15:30:07.768437 21880 solver.cpp:244]     Train net output #0: accuracy = 0.993743
I0523 15:30:07.768445 21880 solver.cpp:244]     Train net output #1: loss = 0.0215207 (* 1 = 0.0215207 loss)
I0523 15:30:07.768450 21880 sgd_solver.cpp:106] Iteration 6300, lr = 0.0001
I0523 15:30:22.488350 21880 solver.cpp:228] Iteration 6320, loss = 0.0268315
I0523 15:30:22.488376 21880 solver.cpp:244]     Train net output #0: accuracy = 0.991464
I0523 15:30:22.488384 21880 solver.cpp:244]     Train net output #1: loss = 0.0268315 (* 1 = 0.0268315 loss)
I0523 15:30:22.488389 21880 sgd_solver.cpp:106] Iteration 6320, lr = 0.0001
I0523 15:30:37.185735 21880 solver.cpp:228] Iteration 6340, loss = 0.0190173
I0523 15:30:37.185818 21880 solver.cpp:244]     Train net output #0: accuracy = 0.992992
I0523 15:30:37.185837 21880 solver.cpp:244]     Train net output #1: loss = 0.0190173 (* 1 = 0.0190173 loss)
I0523 15:30:37.185842 21880 sgd_solver.cpp:106] Iteration 6340, lr = 0.0001
I0523 15:30:51.907565 21880 solver.cpp:228] Iteration 6360, loss = 0.0272347
I0523 15:30:51.907593 21880 solver.cpp:244]     Train net output #0: accuracy = 0.988132
I0523 15:30:51.907600 21880 solver.cpp:244]     Train net output #1: loss = 0.0272347 (* 1 = 0.0272347 loss)
I0523 15:30:51.907605 21880 sgd_solver.cpp:106] Iteration 6360, lr = 0.0001
I0523 15:31:06.614699 21880 solver.cpp:228] Iteration 6380, loss = 0.0151327
I0523 15:31:06.614727 21880 solver.cpp:244]     Train net output #0: accuracy = 0.995449
I0523 15:31:06.614734 21880 solver.cpp:244]     Train net output #1: loss = 0.0151327 (* 1 = 0.0151327 loss)
I0523 15:31:06.614739 21880 sgd_solver.cpp:106] Iteration 6380, lr = 0.0001
I0523 15:31:20.929090 21880 solver.cpp:337] Iteration 6400, Testing net (#0)
I0523 15:31:21.812561 21880 solver.cpp:404]     Test net output #0: accuracy = 0.98739
I0523 15:31:21.812588 21880 solver.cpp:404]     Test net output #1: loss = 0.0328722 (* 1 = 0.0328722 loss)
I0523 15:31:22.238611 21880 solver.cpp:228] Iteration 6400, loss = 0.0234613
I0523 15:31:22.238639 21880 solver.cpp:244]     Train net output #0: accuracy = 0.991422
I0523 15:31:22.238647 21880 solver.cpp:244]     Train net output #1: loss = 0.0234613 (* 1 = 0.0234613 loss)
I0523 15:31:22.238652 21880 sgd_solver.cpp:106] Iteration 6400, lr = 0.0001
I0523 15:31:36.931336 21880 solver.cpp:228] Iteration 6420, loss = 0.0226627
I0523 15:31:36.931362 21880 solver.cpp:244]     Train net output #0: accuracy = 0.991498
I0523 15:31:36.931370 21880 solver.cpp:244]     Train net output #1: loss = 0.0226627 (* 1 = 0.0226627 loss)
I0523 15:31:36.931375 21880 sgd_solver.cpp:106] Iteration 6420, lr = 0.0001
I0523 15:31:51.638423 21880 solver.cpp:228] Iteration 6440, loss = 0.0259535
I0523 15:31:51.638530 21880 solver.cpp:244]     Train net output #0: accuracy = 0.990588
I0523 15:31:51.638550 21880 solver.cpp:244]     Train net output #1: loss = 0.0259535 (* 1 = 0.0259535 loss)
I0523 15:31:51.638553 21880 sgd_solver.cpp:106] Iteration 6440, lr = 0.0001
I0523 15:32:06.340039 21880 solver.cpp:228] Iteration 6460, loss = 0.0265017
I0523 15:32:06.340067 21880 solver.cpp:244]     Train net output #0: accuracy = 0.992376
I0523 15:32:06.340075 21880 solver.cpp:244]     Train net output #1: loss = 0.0265017 (* 1 = 0.0265017 loss)
I0523 15:32:06.340080 21880 sgd_solver.cpp:106] Iteration 6460, lr = 0.0001
I0523 15:32:21.054905 21880 solver.cpp:228] Iteration 6480, loss = 0.0239574
I0523 15:32:21.054932 21880 solver.cpp:244]     Train net output #0: accuracy = 0.991009
I0523 15:32:21.054940 21880 solver.cpp:244]     Train net output #1: loss = 0.0239574 (* 1 = 0.0239574 loss)
I0523 15:32:21.054946 21880 sgd_solver.cpp:106] Iteration 6480, lr = 0.0001
I0523 15:32:35.346135 21880 solver.cpp:454] Snapshotting to binary proto file data/models/segnet_iter_6500.caffemodel
I0523 15:32:35.796737 21880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file data/models/segnet_iter_6500.solverstate
I0523 15:32:35.818023 21880 solver.cpp:337] Iteration 6500, Testing net (#0)
I0523 15:32:36.687952 21880 solver.cpp:404]     Test net output #0: accuracy = 0.987159
I0523 15:32:36.687974 21880 solver.cpp:404]     Test net output #1: loss = 0.0403109 (* 1 = 0.0403109 loss)
I0523 15:32:37.097223 21880 solver.cpp:228] Iteration 6500, loss = 0.0213193
I0523 15:32:37.097249 21880 solver.cpp:244]     Train net output #0: accuracy = 0.99158
I0523 15:32:37.097257 21880 solver.cpp:244]     Train net output #1: loss = 0.0213193 (* 1 = 0.0213193 loss)
I0523 15:32:37.097262 21880 sgd_solver.cpp:106] Iteration 6500, lr = 0.0001
I0523 15:32:51.802449 21880 solver.cpp:228] Iteration 6520, loss = 0.0192867
I0523 15:32:51.802477 21880 solver.cpp:244]     Train net output #0: accuracy = 0.992355
I0523 15:32:51.802485 21880 solver.cpp:244]     Train net output #1: loss = 0.0192867 (* 1 = 0.0192867 loss)
I0523 15:32:51.802489 21880 sgd_solver.cpp:106] Iteration 6520, lr = 0.0001
I0523 15:33:06.564574 21880 solver.cpp:228] Iteration 6540, loss = 0.0282547
I0523 15:33:06.564679 21880 solver.cpp:244]     Train net output #0: accuracy = 0.989691
I0523 15:33:06.564698 21880 solver.cpp:244]     Train net output #1: loss = 0.0282547 (* 1 = 0.0282547 loss)
I0523 15:33:06.564703 21880 sgd_solver.cpp:106] Iteration 6540, lr = 0.0001
I0523 15:33:21.300081 21880 solver.cpp:228] Iteration 6560, loss = 0.0276687
I0523 15:33:21.300107 21880 solver.cpp:244]     Train net output #0: accuracy = 0.988488
I0523 15:33:21.300114 21880 solver.cpp:244]     Train net output #1: loss = 0.0276687 (* 1 = 0.0276687 loss)
I0523 15:33:21.300119 21880 sgd_solver.cpp:106] Iteration 6560, lr = 0.0001
I0523 15:33:35.998455 21880 solver.cpp:228] Iteration 6580, loss = 0.0262123
I0523 15:33:35.998484 21880 solver.cpp:244]     Train net output #0: accuracy = 0.989168
I0523 15:33:35.998492 21880 solver.cpp:244]     Train net output #1: loss = 0.0262123 (* 1 = 0.0262123 loss)
I0523 15:33:35.998497 21880 sgd_solver.cpp:106] Iteration 6580, lr = 0.0001
I0523 15:33:50.294199 21880 solver.cpp:337] Iteration 6600, Testing net (#0)
I0523 15:33:51.160416 21880 solver.cpp:404]     Test net output #0: accuracy = 0.985742
I0523 15:33:51.160451 21880 solver.cpp:404]     Test net output #1: loss = 0.0418779 (* 1 = 0.0418779 loss)
I0523 15:33:51.568199 21880 solver.cpp:228] Iteration 6600, loss = 0.0260035
I0523 15:33:51.568227 21880 solver.cpp:244]     Train net output #0: accuracy = 0.990968
I0523 15:33:51.568233 21880 solver.cpp:244]     Train net output #1: loss = 0.0260035 (* 1 = 0.0260035 loss)
I0523 15:33:51.568238 21880 sgd_solver.cpp:106] Iteration 6600, lr = 0.0001
I0523 15:34:06.268471 21880 solver.cpp:228] Iteration 6620, loss = 0.0209526
I0523 15:34:06.268496 21880 solver.cpp:244]     Train net output #0: accuracy = 0.99211
I0523 15:34:06.268504 21880 solver.cpp:244]     Train net output #1: loss = 0.0209526 (* 1 = 0.0209526 loss)
I0523 15:34:06.268512 21880 sgd_solver.cpp:106] Iteration 6620, lr = 0.0001
I0523 15:34:20.968008 21880 solver.cpp:228] Iteration 6640, loss = 0.0204656
I0523 15:34:20.968088 21880 solver.cpp:244]     Train net output #0: accuracy = 0.991864
I0523 15:34:20.968107 21880 solver.cpp:244]     Train net output #1: loss = 0.0204656 (* 1 = 0.0204656 loss)
I0523 15:34:20.968112 21880 sgd_solver.cpp:106] Iteration 6640, lr = 0.0001
I0523 15:34:35.656646 21880 solver.cpp:228] Iteration 6660, loss = 0.0207577
I0523 15:34:35.656672 21880 solver.cpp:244]     Train net output #0: accuracy = 0.992187
I0523 15:34:35.656678 21880 solver.cpp:244]     Train net output #1: loss = 0.0207577 (* 1 = 0.0207577 loss)
I0523 15:34:35.656683 21880 sgd_solver.cpp:106] Iteration 6660, lr = 0.0001
I0523 15:34:50.355821 21880 solver.cpp:228] Iteration 6680, loss = 0.0203711
I0523 15:34:50.355847 21880 solver.cpp:244]     Train net output #0: accuracy = 0.993292
I0523 15:34:50.355854 21880 solver.cpp:244]     Train net output #1: loss = 0.0203711 (* 1 = 0.0203711 loss)
I0523 15:34:50.355859 21880 sgd_solver.cpp:106] Iteration 6680, lr = 0.0001
I0523 15:35:04.652638 21880 solver.cpp:337] Iteration 6700, Testing net (#0)
I0523 15:35:05.523648 21880 solver.cpp:404]     Test net output #0: accuracy = 0.983336
I0523 15:35:05.523676 21880 solver.cpp:404]     Test net output #1: loss = 0.0450777 (* 1 = 0.0450777 loss)
I0523 15:35:05.955134 21880 solver.cpp:228] Iteration 6700, loss = 0.0186441
I0523 15:35:05.955162 21880 solver.cpp:244]     Train net output #0: accuracy = 0.993482
I0523 15:35:05.955169 21880 solver.cpp:244]     Train net output #1: loss = 0.0186441 (* 1 = 0.0186441 loss)
I0523 15:35:05.955175 21880 sgd_solver.cpp:106] Iteration 6700, lr = 0.0001
I0523 15:35:20.661711 21880 solver.cpp:228] Iteration 6720, loss = 0.0212846
I0523 15:35:20.661738 21880 solver.cpp:244]     Train net output #0: accuracy = 0.992191
I0523 15:35:20.661746 21880 solver.cpp:244]     Train net output #1: loss = 0.0212846 (* 1 = 0.0212846 loss)
I0523 15:35:20.661752 21880 sgd_solver.cpp:106] Iteration 6720, lr = 0.0001
I0523 15:35:35.367772 21880 solver.cpp:228] Iteration 6740, loss = 0.0375356
I0523 15:35:35.367822 21880 solver.cpp:244]     Train net output #0: accuracy = 0.986375
I0523 15:35:35.367832 21880 solver.cpp:244]     Train net output #1: loss = 0.0375356 (* 1 = 0.0375356 loss)
I0523 15:35:35.367837 21880 sgd_solver.cpp:106] Iteration 6740, lr = 0.0001
I0523 15:35:50.069762 21880 solver.cpp:228] Iteration 6760, loss = 0.0223263
I0523 15:35:50.069789 21880 solver.cpp:244]     Train net output #0: accuracy = 0.990749
I0523 15:35:50.069797 21880 solver.cpp:244]     Train net output #1: loss = 0.0223263 (* 1 = 0.0223263 loss)
I0523 15:35:50.069802 21880 sgd_solver.cpp:106] Iteration 6760, lr = 0.0001
I0523 15:36:04.768996 21880 solver.cpp:228] Iteration 6780, loss = 0.0216789
I0523 15:36:04.769021 21880 solver.cpp:244]     Train net output #0: accuracy = 0.992912
I0523 15:36:04.769027 21880 solver.cpp:244]     Train net output #1: loss = 0.0216789 (* 1 = 0.0216789 loss)
I0523 15:36:04.769033 21880 sgd_solver.cpp:106] Iteration 6780, lr = 0.0001
I0523 15:36:19.067915 21880 solver.cpp:337] Iteration 6800, Testing net (#0)
I0523 15:36:19.937633 21880 solver.cpp:404]     Test net output #0: accuracy = 0.98957
I0523 15:36:19.937659 21880 solver.cpp:404]     Test net output #1: loss = 0.0292426 (* 1 = 0.0292426 loss)
I0523 15:36:20.345114 21880 solver.cpp:228] Iteration 6800, loss = 0.0243828
I0523 15:36:20.345142 21880 solver.cpp:244]     Train net output #0: accuracy = 0.990901
I0523 15:36:20.345149 21880 solver.cpp:244]     Train net output #1: loss = 0.0243828 (* 1 = 0.0243828 loss)
I0523 15:36:20.345155 21880 sgd_solver.cpp:106] Iteration 6800, lr = 0.0001
I0523 15:36:35.048135 21880 solver.cpp:228] Iteration 6820, loss = 0.0222016
I0523 15:36:35.048173 21880 solver.cpp:244]     Train net output #0: accuracy = 0.990178
I0523 15:36:35.048182 21880 solver.cpp:244]     Train net output #1: loss = 0.0222016 (* 1 = 0.0222016 loss)
I0523 15:36:35.048187 21880 sgd_solver.cpp:106] Iteration 6820, lr = 0.0001
I0523 15:36:49.753065 21880 solver.cpp:228] Iteration 6840, loss = 0.0281836
I0523 15:36:49.753165 21880 solver.cpp:244]     Train net output #0: accuracy = 0.990378
I0523 15:36:49.753190 21880 solver.cpp:244]     Train net output #1: loss = 0.0281836 (* 1 = 0.0281836 loss)
I0523 15:36:49.753195 21880 sgd_solver.cpp:106] Iteration 6840, lr = 0.0001
I0523 15:37:04.441917 21880 solver.cpp:228] Iteration 6860, loss = 0.0333692
I0523 15:37:04.441944 21880 solver.cpp:244]     Train net output #0: accuracy = 0.987413
I0523 15:37:04.441951 21880 solver.cpp:244]     Train net output #1: loss = 0.0333692 (* 1 = 0.0333692 loss)
I0523 15:37:04.441957 21880 sgd_solver.cpp:106] Iteration 6860, lr = 0.0001
I0523 15:37:19.181375 21880 solver.cpp:228] Iteration 6880, loss = 0.0150547
I0523 15:37:19.181411 21880 solver.cpp:244]     Train net output #0: accuracy = 0.993744
I0523 15:37:19.181419 21880 solver.cpp:244]     Train net output #1: loss = 0.0150547 (* 1 = 0.0150547 loss)
I0523 15:37:19.181426 21880 sgd_solver.cpp:106] Iteration 6880, lr = 0.0001
I0523 15:37:33.480648 21880 solver.cpp:337] Iteration 6900, Testing net (#0)
I0523 15:37:34.349671 21880 solver.cpp:404]     Test net output #0: accuracy = 0.985178
I0523 15:37:34.349695 21880 solver.cpp:404]     Test net output #1: loss = 0.0432301 (* 1 = 0.0432301 loss)
I0523 15:37:34.757203 21880 solver.cpp:228] Iteration 6900, loss = 0.0205019
I0523 15:37:34.757227 21880 solver.cpp:244]     Train net output #0: accuracy = 0.992782
I0523 15:37:34.757236 21880 solver.cpp:244]     Train net output #1: loss = 0.0205019 (* 1 = 0.0205019 loss)
I0523 15:37:34.757241 21880 sgd_solver.cpp:106] Iteration 6900, lr = 0.0001
I0523 15:37:49.458560 21880 solver.cpp:228] Iteration 6920, loss = 0.0287256
I0523 15:37:49.458586 21880 solver.cpp:244]     Train net output #0: accuracy = 0.989399
I0523 15:37:49.458595 21880 solver.cpp:244]     Train net output #1: loss = 0.0287256 (* 1 = 0.0287256 loss)
I0523 15:37:49.458600 21880 sgd_solver.cpp:106] Iteration 6920, lr = 0.0001
I0523 15:38:04.161016 21880 solver.cpp:228] Iteration 6940, loss = 0.0206692
I0523 15:38:04.161073 21880 solver.cpp:244]     Train net output #0: accuracy = 0.992532
I0523 15:38:04.161084 21880 solver.cpp:244]     Train net output #1: loss = 0.0206692 (* 1 = 0.0206692 loss)
I0523 15:38:04.161089 21880 sgd_solver.cpp:106] Iteration 6940, lr = 0.0001
I0523 15:38:18.851151 21880 solver.cpp:228] Iteration 6960, loss = 0.0208815
I0523 15:38:18.851200 21880 solver.cpp:244]     Train net output #0: accuracy = 0.992324
I0523 15:38:18.851217 21880 solver.cpp:244]     Train net output #1: loss = 0.0208815 (* 1 = 0.0208815 loss)
I0523 15:38:18.851222 21880 sgd_solver.cpp:106] Iteration 6960, lr = 0.0001
I0523 15:38:33.547178 21880 solver.cpp:228] Iteration 6980, loss = 0.0270474
I0523 15:38:33.547205 21880 solver.cpp:244]     Train net output #0: accuracy = 0.990035
I0523 15:38:33.547212 21880 solver.cpp:244]     Train net output #1: loss = 0.0270474 (* 1 = 0.0270474 loss)
I0523 15:38:33.547217 21880 sgd_solver.cpp:106] Iteration 6980, lr = 0.0001
I0523 15:38:47.869098 21880 solver.cpp:454] Snapshotting to binary proto file data/models/segnet_iter_7000.caffemodel
I0523 15:38:48.464684 21880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file data/models/segnet_iter_7000.solverstate
I0523 15:38:48.485920 21880 solver.cpp:337] Iteration 7000, Testing net (#0)
I0523 15:38:49.352818 21880 solver.cpp:404]     Test net output #0: accuracy = 0.986926
I0523 15:38:49.352843 21880 solver.cpp:404]     Test net output #1: loss = 0.037035 (* 1 = 0.037035 loss)
I0523 15:38:49.776582 21880 solver.cpp:228] Iteration 7000, loss = 0.0237829
I0523 15:38:49.776609 21880 solver.cpp:244]     Train net output #0: accuracy = 0.99052
I0523 15:38:49.776618 21880 solver.cpp:244]     Train net output #1: loss = 0.0237829 (* 1 = 0.0237829 loss)
I0523 15:38:49.776623 21880 sgd_solver.cpp:106] Iteration 7000, lr = 0.0001
I0523 15:39:04.549859 21880 solver.cpp:228] Iteration 7020, loss = 0.0233913
I0523 15:39:04.549883 21880 solver.cpp:244]     Train net output #0: accuracy = 0.991141
I0523 15:39:04.549891 21880 solver.cpp:244]     Train net output #1: loss = 0.0233913 (* 1 = 0.0233913 loss)
I0523 15:39:04.549897 21880 sgd_solver.cpp:106] Iteration 7020, lr = 0.0001
I0523 15:39:19.229861 21880 solver.cpp:228] Iteration 7040, loss = 0.0161905
I0523 15:39:19.229926 21880 solver.cpp:244]     Train net output #0: accuracy = 0.993124
I0523 15:39:19.229935 21880 solver.cpp:244]     Train net output #1: loss = 0.0161905 (* 1 = 0.0161905 loss)
I0523 15:39:19.229941 21880 sgd_solver.cpp:106] Iteration 7040, lr = 0.0001
I0523 15:39:33.934249 21880 solver.cpp:228] Iteration 7060, loss = 0.0208077
I0523 15:39:33.934278 21880 solver.cpp:244]     Train net output #0: accuracy = 0.99195
I0523 15:39:33.934285 21880 solver.cpp:244]     Train net output #1: loss = 0.0208077 (* 1 = 0.0208077 loss)
I0523 15:39:33.934290 21880 sgd_solver.cpp:106] Iteration 7060, lr = 0.0001
I0523 15:39:48.639482 21880 solver.cpp:228] Iteration 7080, loss = 0.0156597
I0523 15:39:48.639509 21880 solver.cpp:244]     Train net output #0: accuracy = 0.994402
I0523 15:39:48.639516 21880 solver.cpp:244]     Train net output #1: loss = 0.0156597 (* 1 = 0.0156597 loss)
I0523 15:39:48.639521 21880 sgd_solver.cpp:106] Iteration 7080, lr = 0.0001
I0523 15:40:02.946521 21880 solver.cpp:337] Iteration 7100, Testing net (#0)
I0523 15:40:03.813329 21880 solver.cpp:404]     Test net output #0: accuracy = 0.985894
I0523 15:40:03.813356 21880 solver.cpp:404]     Test net output #1: loss = 0.0442971 (* 1 = 0.0442971 loss)
I0523 15:40:04.221669 21880 solver.cpp:228] Iteration 7100, loss = 0.0133132
I0523 15:40:04.221694 21880 solver.cpp:244]     Train net output #0: accuracy = 0.995662
I0523 15:40:04.221703 21880 solver.cpp:244]     Train net output #1: loss = 0.0133132 (* 1 = 0.0133132 loss)
I0523 15:40:04.221707 21880 sgd_solver.cpp:106] Iteration 7100, lr = 0.0001
I0523 15:40:18.918303 21880 solver.cpp:228] Iteration 7120, loss = 0.0246623
I0523 15:40:18.918330 21880 solver.cpp:244]     Train net output #0: accuracy = 0.990654
I0523 15:40:18.918339 21880 solver.cpp:244]     Train net output #1: loss = 0.0246623 (* 1 = 0.0246623 loss)
I0523 15:40:18.918344 21880 sgd_solver.cpp:106] Iteration 7120, lr = 0.0001
I0523 15:40:33.614617 21880 solver.cpp:228] Iteration 7140, loss = 0.0241861
I0523 15:40:33.614666 21880 solver.cpp:244]     Train net output #0: accuracy = 0.991137
I0523 15:40:33.614675 21880 solver.cpp:244]     Train net output #1: loss = 0.0241861 (* 1 = 0.0241861 loss)
I0523 15:40:33.614680 21880 sgd_solver.cpp:106] Iteration 7140, lr = 0.0001
I0523 15:40:48.316797 21880 solver.cpp:228] Iteration 7160, loss = 0.0323806
I0523 15:40:48.316824 21880 solver.cpp:244]     Train net output #0: accuracy = 0.98824
I0523 15:40:48.316831 21880 solver.cpp:244]     Train net output #1: loss = 0.0323806 (* 1 = 0.0323806 loss)
I0523 15:40:48.316836 21880 sgd_solver.cpp:106] Iteration 7160, lr = 0.0001
I0523 15:41:03.058917 21880 solver.cpp:228] Iteration 7180, loss = 0.0243583
I0523 15:41:03.058945 21880 solver.cpp:244]     Train net output #0: accuracy = 0.991479
I0523 15:41:03.058954 21880 solver.cpp:244]     Train net output #1: loss = 0.0243583 (* 1 = 0.0243583 loss)
I0523 15:41:03.058959 21880 sgd_solver.cpp:106] Iteration 7180, lr = 0.0001
I0523 15:41:17.357475 21880 solver.cpp:337] Iteration 7200, Testing net (#0)
I0523 15:41:18.226531 21880 solver.cpp:404]     Test net output #0: accuracy = 0.985675
I0523 15:41:18.226553 21880 solver.cpp:404]     Test net output #1: loss = 0.0458437 (* 1 = 0.0458437 loss)
I0523 15:41:18.633795 21880 solver.cpp:228] Iteration 7200, loss = 0.022185
I0523 15:41:18.633821 21880 solver.cpp:244]     Train net output #0: accuracy = 0.990906
I0523 15:41:18.633828 21880 solver.cpp:244]     Train net output #1: loss = 0.022185 (* 1 = 0.022185 loss)
I0523 15:41:18.633832 21880 sgd_solver.cpp:106] Iteration 7200, lr = 0.0001
I0523 15:41:33.325522 21880 solver.cpp:228] Iteration 7220, loss = 0.0175032
I0523 15:41:33.325548 21880 solver.cpp:244]     Train net output #0: accuracy = 0.993566
I0523 15:41:33.325556 21880 solver.cpp:244]     Train net output #1: loss = 0.0175032 (* 1 = 0.0175032 loss)
I0523 15:41:33.325561 21880 sgd_solver.cpp:106] Iteration 7220, lr = 0.0001
I0523 15:41:48.014488 21880 solver.cpp:228] Iteration 7240, loss = 0.0296208
I0523 15:41:48.014611 21880 solver.cpp:244]     Train net output #0: accuracy = 0.989227
I0523 15:41:48.014629 21880 solver.cpp:244]     Train net output #1: loss = 0.0296208 (* 1 = 0.0296208 loss)
I0523 15:41:48.014634 21880 sgd_solver.cpp:106] Iteration 7240, lr = 0.0001
I0523 15:42:02.720743 21880 solver.cpp:228] Iteration 7260, loss = 0.0169877
I0523 15:42:02.720772 21880 solver.cpp:244]     Train net output #0: accuracy = 0.992508
I0523 15:42:02.720779 21880 solver.cpp:244]     Train net output #1: loss = 0.0169877 (* 1 = 0.0169877 loss)
I0523 15:42:02.720784 21880 sgd_solver.cpp:106] Iteration 7260, lr = 0.0001
I0523 15:42:17.429265 21880 solver.cpp:228] Iteration 7280, loss = 0.0375004
I0523 15:42:17.429292 21880 solver.cpp:244]     Train net output #0: accuracy = 0.983374
I0523 15:42:17.429301 21880 solver.cpp:244]     Train net output #1: loss = 0.0375004 (* 1 = 0.0375004 loss)
I0523 15:42:17.429306 21880 sgd_solver.cpp:106] Iteration 7280, lr = 0.0001
I0523 15:42:31.724223 21880 solver.cpp:337] Iteration 7300, Testing net (#0)
I0523 15:42:32.586961 21880 solver.cpp:404]     Test net output #0: accuracy = 0.984331
I0523 15:42:32.586988 21880 solver.cpp:404]     Test net output #1: loss = 0.0418933 (* 1 = 0.0418933 loss)
I0523 15:42:32.997182 21880 solver.cpp:228] Iteration 7300, loss = 0.0263881
I0523 15:42:32.997210 21880 solver.cpp:244]     Train net output #0: accuracy = 0.990642
I0523 15:42:32.997221 21880 solver.cpp:244]     Train net output #1: loss = 0.0263881 (* 1 = 0.0263881 loss)
I0523 15:42:32.997228 21880 sgd_solver.cpp:106] Iteration 7300, lr = 0.0001
I0523 15:42:47.731503 21880 solver.cpp:228] Iteration 7320, loss = 0.0365829
I0523 15:42:47.731530 21880 solver.cpp:244]     Train net output #0: accuracy = 0.988065
I0523 15:42:47.731541 21880 solver.cpp:244]     Train net output #1: loss = 0.0365829 (* 1 = 0.0365829 loss)
I0523 15:42:47.731547 21880 sgd_solver.cpp:106] Iteration 7320, lr = 0.0001
I0523 15:43:02.463426 21880 solver.cpp:228] Iteration 7340, loss = 0.0302341
I0523 15:43:02.463522 21880 solver.cpp:244]     Train net output #0: accuracy = 0.988238
I0523 15:43:02.463534 21880 solver.cpp:244]     Train net output #1: loss = 0.0302341 (* 1 = 0.0302341 loss)
I0523 15:43:02.463541 21880 sgd_solver.cpp:106] Iteration 7340, lr = 0.0001
I0523 15:43:17.162554 21880 solver.cpp:228] Iteration 7360, loss = 0.0228032
I0523 15:43:17.162581 21880 solver.cpp:244]     Train net output #0: accuracy = 0.992275
I0523 15:43:17.162592 21880 solver.cpp:244]     Train net output #1: loss = 0.0228032 (* 1 = 0.0228032 loss)
I0523 15:43:17.162598 21880 sgd_solver.cpp:106] Iteration 7360, lr = 0.0001
I0523 15:43:31.862205 21880 solver.cpp:228] Iteration 7380, loss = 0.0198634
I0523 15:43:31.862233 21880 solver.cpp:244]     Train net output #0: accuracy = 0.993129
I0523 15:43:31.862241 21880 solver.cpp:244]     Train net output #1: loss = 0.0198634 (* 1 = 0.0198634 loss)
I0523 15:43:31.862246 21880 sgd_solver.cpp:106] Iteration 7380, lr = 0.0001
I0523 15:43:46.165765 21880 solver.cpp:337] Iteration 7400, Testing net (#0)
I0523 15:43:47.031066 21880 solver.cpp:404]     Test net output #0: accuracy = 0.985272
I0523 15:43:47.031090 21880 solver.cpp:404]     Test net output #1: loss = 0.0407054 (* 1 = 0.0407054 loss)
I0523 15:43:47.438558 21880 solver.cpp:228] Iteration 7400, loss = 0.0275756
I0523 15:43:47.438582 21880 solver.cpp:244]     Train net output #0: accuracy = 0.989264
I0523 15:43:47.438591 21880 solver.cpp:244]     Train net output #1: loss = 0.0275756 (* 1 = 0.0275756 loss)
I0523 15:43:47.438596 21880 sgd_solver.cpp:106] Iteration 7400, lr = 0.0001
I0523 15:44:02.151356 21880 solver.cpp:228] Iteration 7420, loss = 0.0248458
I0523 15:44:02.151384 21880 solver.cpp:244]     Train net output #0: accuracy = 0.991154
I0523 15:44:02.151392 21880 solver.cpp:244]     Train net output #1: loss = 0.0248458 (* 1 = 0.0248458 loss)
I0523 15:44:02.151398 21880 sgd_solver.cpp:106] Iteration 7420, lr = 0.0001
I0523 15:44:16.855993 21880 solver.cpp:228] Iteration 7440, loss = 0.0191268
I0523 15:44:16.856089 21880 solver.cpp:244]     Train net output #0: accuracy = 0.993336
I0523 15:44:16.856098 21880 solver.cpp:244]     Train net output #1: loss = 0.0191268 (* 1 = 0.0191268 loss)
I0523 15:44:16.856103 21880 sgd_solver.cpp:106] Iteration 7440, lr = 0.0001
I0523 15:44:31.558711 21880 solver.cpp:228] Iteration 7460, loss = 0.0242342
I0523 15:44:31.558737 21880 solver.cpp:244]     Train net output #0: accuracy = 0.989679
I0523 15:44:31.558744 21880 solver.cpp:244]     Train net output #1: loss = 0.0242342 (* 1 = 0.0242342 loss)
I0523 15:44:31.558750 21880 sgd_solver.cpp:106] Iteration 7460, lr = 0.0001
I0523 15:44:46.271118 21880 solver.cpp:228] Iteration 7480, loss = 0.0289278
I0523 15:44:46.271144 21880 solver.cpp:244]     Train net output #0: accuracy = 0.989823
I0523 15:44:46.271152 21880 solver.cpp:244]     Train net output #1: loss = 0.0289278 (* 1 = 0.0289278 loss)
I0523 15:44:46.271157 21880 sgd_solver.cpp:106] Iteration 7480, lr = 0.0001
I0523 15:45:00.601709 21880 solver.cpp:454] Snapshotting to binary proto file data/models/segnet_iter_7500.caffemodel
I0523 15:45:00.960866 21880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file data/models/segnet_iter_7500.solverstate
I0523 15:45:00.982971 21880 solver.cpp:337] Iteration 7500, Testing net (#0)
I0523 15:45:01.861874 21880 solver.cpp:404]     Test net output #0: accuracy = 0.986603
I0523 15:45:01.861908 21880 solver.cpp:404]     Test net output #1: loss = 0.0401875 (* 1 = 0.0401875 loss)
I0523 15:45:02.271059 21880 solver.cpp:228] Iteration 7500, loss = 0.0173517
I0523 15:45:02.271085 21880 solver.cpp:244]     Train net output #0: accuracy = 0.994225
I0523 15:45:02.271092 21880 solver.cpp:244]     Train net output #1: loss = 0.0173517 (* 1 = 0.0173517 loss)
I0523 15:45:02.271098 21880 sgd_solver.cpp:106] Iteration 7500, lr = 0.0001
I0523 15:45:16.971653 21880 solver.cpp:228] Iteration 7520, loss = 0.0247726
I0523 15:45:16.971680 21880 solver.cpp:244]     Train net output #0: accuracy = 0.989618
I0523 15:45:16.971688 21880 solver.cpp:244]     Train net output #1: loss = 0.0247726 (* 1 = 0.0247726 loss)
I0523 15:45:16.971693 21880 sgd_solver.cpp:106] Iteration 7520, lr = 0.0001
I0523 15:45:31.678172 21880 solver.cpp:228] Iteration 7540, loss = 0.0182724
I0523 15:45:31.678272 21880 solver.cpp:244]     Train net output #0: accuracy = 0.992628
I0523 15:45:31.678282 21880 solver.cpp:244]     Train net output #1: loss = 0.0182724 (* 1 = 0.0182724 loss)
I0523 15:45:31.678287 21880 sgd_solver.cpp:106] Iteration 7540, lr = 0.0001
I0523 15:45:46.381956 21880 solver.cpp:228] Iteration 7560, loss = 0.0176821
I0523 15:45:46.381983 21880 solver.cpp:244]     Train net output #0: accuracy = 0.993697
I0523 15:45:46.381990 21880 solver.cpp:244]     Train net output #1: loss = 0.0176821 (* 1 = 0.0176821 loss)
I0523 15:45:46.381995 21880 sgd_solver.cpp:106] Iteration 7560, lr = 0.0001
I0523 15:46:01.085119 21880 solver.cpp:228] Iteration 7580, loss = 0.0199771
I0523 15:46:01.085146 21880 solver.cpp:244]     Train net output #0: accuracy = 0.993148
I0523 15:46:01.085155 21880 solver.cpp:244]     Train net output #1: loss = 0.0199771 (* 1 = 0.0199771 loss)
I0523 15:46:01.085160 21880 sgd_solver.cpp:106] Iteration 7580, lr = 0.0001
I0523 15:46:15.379853 21880 solver.cpp:337] Iteration 7600, Testing net (#0)
I0523 15:46:16.249027 21880 solver.cpp:404]     Test net output #0: accuracy = 0.988486
I0523 15:46:16.249052 21880 solver.cpp:404]     Test net output #1: loss = 0.0312784 (* 1 = 0.0312784 loss)
I0523 15:46:16.657173 21880 solver.cpp:228] Iteration 7600, loss = 0.0198752
I0523 15:46:16.657199 21880 solver.cpp:244]     Train net output #0: accuracy = 0.993266
I0523 15:46:16.657207 21880 solver.cpp:244]     Train net output #1: loss = 0.0198752 (* 1 = 0.0198752 loss)
I0523 15:46:16.657212 21880 sgd_solver.cpp:106] Iteration 7600, lr = 0.0001
I0523 15:46:31.352171 21880 solver.cpp:228] Iteration 7620, loss = 0.0219169
I0523 15:46:31.352200 21880 solver.cpp:244]     Train net output #0: accuracy = 0.991068
I0523 15:46:31.352206 21880 solver.cpp:244]     Train net output #1: loss = 0.0219169 (* 1 = 0.0219169 loss)
I0523 15:46:31.352212 21880 sgd_solver.cpp:106] Iteration 7620, lr = 0.0001
I0523 15:46:46.097002 21880 solver.cpp:228] Iteration 7640, loss = 0.0253302
I0523 15:46:46.097106 21880 solver.cpp:244]     Train net output #0: accuracy = 0.989427
I0523 15:46:46.097126 21880 solver.cpp:244]     Train net output #1: loss = 0.0253302 (* 1 = 0.0253302 loss)
I0523 15:46:46.097131 21880 sgd_solver.cpp:106] Iteration 7640, lr = 0.0001
I0523 15:47:00.797844 21880 solver.cpp:228] Iteration 7660, loss = 0.021512
I0523 15:47:00.797871 21880 solver.cpp:244]     Train net output #0: accuracy = 0.991253
I0523 15:47:00.797878 21880 solver.cpp:244]     Train net output #1: loss = 0.021512 (* 1 = 0.021512 loss)
I0523 15:47:00.797883 21880 sgd_solver.cpp:106] Iteration 7660, lr = 0.0001
I0523 15:47:15.490581 21880 solver.cpp:228] Iteration 7680, loss = 0.0193419
I0523 15:47:15.490620 21880 solver.cpp:244]     Train net output #0: accuracy = 0.992401
I0523 15:47:15.490629 21880 solver.cpp:244]     Train net output #1: loss = 0.0193419 (* 1 = 0.0193419 loss)
I0523 15:47:15.490634 21880 sgd_solver.cpp:106] Iteration 7680, lr = 0.0001
I0523 15:47:29.783457 21880 solver.cpp:337] Iteration 7700, Testing net (#0)
I0523 15:47:30.648752 21880 solver.cpp:404]     Test net output #0: accuracy = 0.986964
I0523 15:47:30.648787 21880 solver.cpp:404]     Test net output #1: loss = 0.0366566 (* 1 = 0.0366566 loss)
I0523 15:47:31.056968 21880 solver.cpp:228] Iteration 7700, loss = 0.0243114
I0523 15:47:31.056994 21880 solver.cpp:244]     Train net output #0: accuracy = 0.991088
I0523 15:47:31.057003 21880 solver.cpp:244]     Train net output #1: loss = 0.0243114 (* 1 = 0.0243114 loss)
I0523 15:47:31.057008 21880 sgd_solver.cpp:106] Iteration 7700, lr = 0.0001
I0523 15:47:45.757900 21880 solver.cpp:228] Iteration 7720, loss = 0.0248465
I0523 15:47:45.757936 21880 solver.cpp:244]     Train net output #0: accuracy = 0.990654
I0523 15:47:45.757943 21880 solver.cpp:244]     Train net output #1: loss = 0.0248465 (* 1 = 0.0248465 loss)
I0523 15:47:45.757948 21880 sgd_solver.cpp:106] Iteration 7720, lr = 0.0001
I0523 15:48:00.457089 21880 solver.cpp:228] Iteration 7740, loss = 0.0203127
I0523 15:48:00.457161 21880 solver.cpp:244]     Train net output #0: accuracy = 0.991294
I0523 15:48:00.457171 21880 solver.cpp:244]     Train net output #1: loss = 0.0203127 (* 1 = 0.0203127 loss)
I0523 15:48:00.457176 21880 sgd_solver.cpp:106] Iteration 7740, lr = 0.0001
I0523 15:48:15.158262 21880 solver.cpp:228] Iteration 7760, loss = 0.0253289
I0523 15:48:15.158289 21880 solver.cpp:244]     Train net output #0: accuracy = 0.990367
I0523 15:48:15.158296 21880 solver.cpp:244]     Train net output #1: loss = 0.0253289 (* 1 = 0.0253289 loss)
I0523 15:48:15.158303 21880 sgd_solver.cpp:106] Iteration 7760, lr = 0.0001
I0523 15:48:29.859400 21880 solver.cpp:228] Iteration 7780, loss = 0.031308
I0523 15:48:29.859427 21880 solver.cpp:244]     Train net output #0: accuracy = 0.988028
I0523 15:48:29.859436 21880 solver.cpp:244]     Train net output #1: loss = 0.031308 (* 1 = 0.031308 loss)
I0523 15:48:29.859441 21880 sgd_solver.cpp:106] Iteration 7780, lr = 0.0001
I0523 15:48:44.167053 21880 solver.cpp:337] Iteration 7800, Testing net (#0)
I0523 15:48:45.075661 21880 solver.cpp:404]     Test net output #0: accuracy = 0.990199
I0523 15:48:45.075700 21880 solver.cpp:404]     Test net output #1: loss = 0.0270596 (* 1 = 0.0270596 loss)
I0523 15:48:45.487855 21880 solver.cpp:228] Iteration 7800, loss = 0.0261837
I0523 15:48:45.487880 21880 solver.cpp:244]     Train net output #0: accuracy = 0.989392
I0523 15:48:45.487887 21880 solver.cpp:244]     Train net output #1: loss = 0.0261837 (* 1 = 0.0261837 loss)
I0523 15:48:45.487892 21880 sgd_solver.cpp:106] Iteration 7800, lr = 0.0001
I0523 15:49:00.181630 21880 solver.cpp:228] Iteration 7820, loss = 0.0228565
I0523 15:49:00.181669 21880 solver.cpp:244]     Train net output #0: accuracy = 0.991481
I0523 15:49:00.181679 21880 solver.cpp:244]     Train net output #1: loss = 0.0228565 (* 1 = 0.0228565 loss)
I0523 15:49:00.181684 21880 sgd_solver.cpp:106] Iteration 7820, lr = 0.0001
I0523 15:49:14.880672 21880 solver.cpp:228] Iteration 7840, loss = 0.0227641
I0523 15:49:14.880758 21880 solver.cpp:244]     Train net output #0: accuracy = 0.99179
I0523 15:49:14.880766 21880 solver.cpp:244]     Train net output #1: loss = 0.0227641 (* 1 = 0.0227641 loss)
I0523 15:49:14.880772 21880 sgd_solver.cpp:106] Iteration 7840, lr = 0.0001
I0523 15:49:29.576472 21880 solver.cpp:228] Iteration 7860, loss = 0.0296211
I0523 15:49:29.576498 21880 solver.cpp:244]     Train net output #0: accuracy = 0.988094
I0523 15:49:29.576504 21880 solver.cpp:244]     Train net output #1: loss = 0.0296211 (* 1 = 0.0296211 loss)
I0523 15:49:29.576508 21880 sgd_solver.cpp:106] Iteration 7860, lr = 0.0001
I0523 15:49:44.277931 21880 solver.cpp:228] Iteration 7880, loss = 0.018013
I0523 15:49:44.277969 21880 solver.cpp:244]     Train net output #0: accuracy = 0.992936
I0523 15:49:44.277978 21880 solver.cpp:244]     Train net output #1: loss = 0.018013 (* 1 = 0.018013 loss)
I0523 15:49:44.277983 21880 sgd_solver.cpp:106] Iteration 7880, lr = 0.0001
I0523 15:49:58.569495 21880 solver.cpp:337] Iteration 7900, Testing net (#0)
I0523 15:49:59.435492 21880 solver.cpp:404]     Test net output #0: accuracy = 0.986411
I0523 15:49:59.435513 21880 solver.cpp:404]     Test net output #1: loss = 0.0411639 (* 1 = 0.0411639 loss)
I0523 15:49:59.844370 21880 solver.cpp:228] Iteration 7900, loss = 0.0247495
I0523 15:49:59.844398 21880 solver.cpp:244]     Train net output #0: accuracy = 0.991905
I0523 15:49:59.844404 21880 solver.cpp:244]     Train net output #1: loss = 0.0247495 (* 1 = 0.0247495 loss)
I0523 15:49:59.844410 21880 sgd_solver.cpp:106] Iteration 7900, lr = 0.0001
I0523 15:50:14.549523 21880 solver.cpp:228] Iteration 7920, loss = 0.0261455
I0523 15:50:14.549582 21880 solver.cpp:244]     Train net output #0: accuracy = 0.990074
I0523 15:50:14.549590 21880 solver.cpp:244]     Train net output #1: loss = 0.0261455 (* 1 = 0.0261455 loss)
I0523 15:50:14.549605 21880 sgd_solver.cpp:106] Iteration 7920, lr = 0.0001
I0523 15:50:29.243815 21880 solver.cpp:228] Iteration 7940, loss = 0.023972
I0523 15:50:29.243885 21880 solver.cpp:244]     Train net output #0: accuracy = 0.991263
I0523 15:50:29.243906 21880 solver.cpp:244]     Train net output #1: loss = 0.023972 (* 1 = 0.023972 loss)
I0523 15:50:29.243911 21880 sgd_solver.cpp:106] Iteration 7940, lr = 0.0001
I0523 15:50:43.978926 21880 solver.cpp:228] Iteration 7960, loss = 0.0197047
I0523 15:50:43.978953 21880 solver.cpp:244]     Train net output #0: accuracy = 0.99274
I0523 15:50:43.978962 21880 solver.cpp:244]     Train net output #1: loss = 0.0197047 (* 1 = 0.0197047 loss)
I0523 15:50:43.978967 21880 sgd_solver.cpp:106] Iteration 7960, lr = 0.0001
I0523 15:50:58.692111 21880 solver.cpp:228] Iteration 7980, loss = 0.0222451
I0523 15:50:58.692137 21880 solver.cpp:244]     Train net output #0: accuracy = 0.991284
I0523 15:50:58.692144 21880 solver.cpp:244]     Train net output #1: loss = 0.0222451 (* 1 = 0.0222451 loss)
I0523 15:50:58.692149 21880 sgd_solver.cpp:106] Iteration 7980, lr = 0.0001
I0523 15:51:12.985643 21880 solver.cpp:454] Snapshotting to binary proto file data/models/segnet_iter_8000.caffemodel
I0523 15:51:13.564684 21880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file data/models/segnet_iter_8000.solverstate
I0523 15:51:13.586470 21880 solver.cpp:337] Iteration 8000, Testing net (#0)
I0523 15:51:14.457084 21880 solver.cpp:404]     Test net output #0: accuracy = 0.990466
I0523 15:51:14.457108 21880 solver.cpp:404]     Test net output #1: loss = 0.026053 (* 1 = 0.026053 loss)
I0523 15:51:14.865137 21880 solver.cpp:228] Iteration 8000, loss = 0.0295753
I0523 15:51:14.865164 21880 solver.cpp:244]     Train net output #0: accuracy = 0.989524
I0523 15:51:14.865171 21880 solver.cpp:244]     Train net output #1: loss = 0.0295753 (* 1 = 0.0295753 loss)
I0523 15:51:14.865177 21880 sgd_solver.cpp:106] Iteration 8000, lr = 0.0001
I0523 15:51:29.554419 21880 solver.cpp:228] Iteration 8020, loss = 0.0245656
I0523 15:51:29.554445 21880 solver.cpp:244]     Train net output #0: accuracy = 0.990331
I0523 15:51:29.554452 21880 solver.cpp:244]     Train net output #1: loss = 0.0245656 (* 1 = 0.0245656 loss)
I0523 15:51:29.554457 21880 sgd_solver.cpp:106] Iteration 8020, lr = 0.0001
I0523 15:51:44.247875 21880 solver.cpp:228] Iteration 8040, loss = 0.0314644
I0523 15:51:44.247925 21880 solver.cpp:244]     Train net output #0: accuracy = 0.987595
I0523 15:51:44.247934 21880 solver.cpp:244]     Train net output #1: loss = 0.0314644 (* 1 = 0.0314644 loss)
I0523 15:51:44.247939 21880 sgd_solver.cpp:106] Iteration 8040, lr = 0.0001
I0523 15:51:58.939345 21880 solver.cpp:228] Iteration 8060, loss = 0.0298275
I0523 15:51:58.939373 21880 solver.cpp:244]     Train net output #0: accuracy = 0.988583
I0523 15:51:58.939380 21880 solver.cpp:244]     Train net output #1: loss = 0.0298275 (* 1 = 0.0298275 loss)
I0523 15:51:58.939385 21880 sgd_solver.cpp:106] Iteration 8060, lr = 0.0001
I0523 15:52:13.629076 21880 solver.cpp:228] Iteration 8080, loss = 0.0292185
I0523 15:52:13.629112 21880 solver.cpp:244]     Train net output #0: accuracy = 0.988673
I0523 15:52:13.629120 21880 solver.cpp:244]     Train net output #1: loss = 0.0292185 (* 1 = 0.0292185 loss)
I0523 15:52:13.629125 21880 sgd_solver.cpp:106] Iteration 8080, lr = 0.0001
I0523 15:52:27.930114 21880 solver.cpp:337] Iteration 8100, Testing net (#0)
I0523 15:52:28.820173 21880 solver.cpp:404]     Test net output #0: accuracy = 0.984783
I0523 15:52:28.820211 21880 solver.cpp:404]     Test net output #1: loss = 0.0443488 (* 1 = 0.0443488 loss)
I0523 15:52:29.248312 21880 solver.cpp:228] Iteration 8100, loss = 0.0354485
I0523 15:52:29.248340 21880 solver.cpp:244]     Train net output #0: accuracy = 0.987406
I0523 15:52:29.248348 21880 solver.cpp:244]     Train net output #1: loss = 0.0354485 (* 1 = 0.0354485 loss)
I0523 15:52:29.248353 21880 sgd_solver.cpp:106] Iteration 8100, lr = 0.0001
I0523 15:52:43.956416 21880 solver.cpp:228] Iteration 8120, loss = 0.0263844
I0523 15:52:43.956444 21880 solver.cpp:244]     Train net output #0: accuracy = 0.98993
I0523 15:52:43.956451 21880 solver.cpp:244]     Train net output #1: loss = 0.0263844 (* 1 = 0.0263844 loss)
I0523 15:52:43.956456 21880 sgd_solver.cpp:106] Iteration 8120, lr = 0.0001
I0523 15:52:59.146558 21880 solver.cpp:228] Iteration 8140, loss = 0.0238514
I0523 15:52:59.146646 21880 solver.cpp:244]     Train net output #0: accuracy = 0.990726
I0523 15:52:59.146666 21880 solver.cpp:244]     Train net output #1: loss = 0.0238514 (* 1 = 0.0238514 loss)
I0523 15:52:59.146670 21880 sgd_solver.cpp:106] Iteration 8140, lr = 0.0001
I0523 15:53:13.847168 21880 solver.cpp:228] Iteration 8160, loss = 0.0240536
I0523 15:53:13.847195 21880 solver.cpp:244]     Train net output #0: accuracy = 0.989983
I0523 15:53:13.847203 21880 solver.cpp:244]     Train net output #1: loss = 0.0240536 (* 1 = 0.0240536 loss)
I0523 15:53:13.847208 21880 sgd_solver.cpp:106] Iteration 8160, lr = 0.0001
I0523 15:53:28.543066 21880 solver.cpp:228] Iteration 8180, loss = 0.0287384
I0523 15:53:28.543092 21880 solver.cpp:244]     Train net output #0: accuracy = 0.990064
I0523 15:53:28.543099 21880 solver.cpp:244]     Train net output #1: loss = 0.0287384 (* 1 = 0.0287384 loss)
I0523 15:53:28.543103 21880 sgd_solver.cpp:106] Iteration 8180, lr = 0.0001
I0523 15:53:42.839187 21880 solver.cpp:337] Iteration 8200, Testing net (#0)
I0523 15:53:43.707216 21880 solver.cpp:404]     Test net output #0: accuracy = 0.986096
I0523 15:53:43.707239 21880 solver.cpp:404]     Test net output #1: loss = 0.0414274 (* 1 = 0.0414274 loss)
I0523 15:53:44.114938 21880 solver.cpp:228] Iteration 8200, loss = 0.0225662
I0523 15:53:44.114964 21880 solver.cpp:244]     Train net output #0: accuracy = 0.991135
I0523 15:53:44.114971 21880 solver.cpp:244]     Train net output #1: loss = 0.0225662 (* 1 = 0.0225662 loss)
I0523 15:53:44.114976 21880 sgd_solver.cpp:106] Iteration 8200, lr = 0.0001
I0523 15:53:58.816994 21880 solver.cpp:228] Iteration 8220, loss = 0.0338339
I0523 15:53:58.817020 21880 solver.cpp:244]     Train net output #0: accuracy = 0.986194
I0523 15:53:58.817028 21880 solver.cpp:244]     Train net output #1: loss = 0.0338339 (* 1 = 0.0338339 loss)
I0523 15:53:58.817033 21880 sgd_solver.cpp:106] Iteration 8220, lr = 0.0001
I0523 15:54:13.512362 21880 solver.cpp:228] Iteration 8240, loss = 0.0241713
I0523 15:54:13.512455 21880 solver.cpp:244]     Train net output #0: accuracy = 0.990558
I0523 15:54:13.512467 21880 solver.cpp:244]     Train net output #1: loss = 0.0241713 (* 1 = 0.0241713 loss)
I0523 15:54:13.512475 21880 sgd_solver.cpp:106] Iteration 8240, lr = 0.0001
I0523 15:54:28.261255 21880 solver.cpp:228] Iteration 8260, loss = 0.0299011
I0523 15:54:28.261293 21880 solver.cpp:244]     Train net output #0: accuracy = 0.988972
I0523 15:54:28.261301 21880 solver.cpp:244]     Train net output #1: loss = 0.0299011 (* 1 = 0.0299011 loss)
I0523 15:54:28.261307 21880 sgd_solver.cpp:106] Iteration 8260, lr = 0.0001
I0523 15:54:42.955747 21880 solver.cpp:228] Iteration 8280, loss = 0.0227615
I0523 15:54:42.955772 21880 solver.cpp:244]     Train net output #0: accuracy = 0.99231
I0523 15:54:42.955780 21880 solver.cpp:244]     Train net output #1: loss = 0.0227615 (* 1 = 0.0227615 loss)
I0523 15:54:42.955785 21880 sgd_solver.cpp:106] Iteration 8280, lr = 0.0001
I0523 15:54:57.253424 21880 solver.cpp:337] Iteration 8300, Testing net (#0)
I0523 15:54:58.120385 21880 solver.cpp:404]     Test net output #0: accuracy = 0.985415
I0523 15:54:58.120412 21880 solver.cpp:404]     Test net output #1: loss = 0.0381299 (* 1 = 0.0381299 loss)
I0523 15:54:58.526489 21880 solver.cpp:228] Iteration 8300, loss = 0.0215029
I0523 15:54:58.526515 21880 solver.cpp:244]     Train net output #0: accuracy = 0.99051
I0523 15:54:58.526521 21880 solver.cpp:244]     Train net output #1: loss = 0.0215029 (* 1 = 0.0215029 loss)
I0523 15:54:58.526527 21880 sgd_solver.cpp:106] Iteration 8300, lr = 0.0001
I0523 15:55:13.221945 21880 solver.cpp:228] Iteration 8320, loss = 0.0251636
I0523 15:55:13.221971 21880 solver.cpp:244]     Train net output #0: accuracy = 0.990981
I0523 15:55:13.221978 21880 solver.cpp:244]     Train net output #1: loss = 0.0251636 (* 1 = 0.0251636 loss)
I0523 15:55:13.221983 21880 sgd_solver.cpp:106] Iteration 8320, lr = 0.0001
I0523 15:55:27.918436 21880 solver.cpp:228] Iteration 8340, loss = 0.0228761
I0523 15:55:27.918517 21880 solver.cpp:244]     Train net output #0: accuracy = 0.991297
I0523 15:55:27.918535 21880 solver.cpp:244]     Train net output #1: loss = 0.0228761 (* 1 = 0.0228761 loss)
I0523 15:55:27.918540 21880 sgd_solver.cpp:106] Iteration 8340, lr = 0.0001
I0523 15:55:42.603467 21880 solver.cpp:228] Iteration 8360, loss = 0.0274478
I0523 15:55:42.603494 21880 solver.cpp:244]     Train net output #0: accuracy = 0.988652
I0523 15:55:42.603502 21880 solver.cpp:244]     Train net output #1: loss = 0.0274478 (* 1 = 0.0274478 loss)
I0523 15:55:42.603508 21880 sgd_solver.cpp:106] Iteration 8360, lr = 0.0001
I0523 15:55:57.295244 21880 solver.cpp:228] Iteration 8380, loss = 0.0238757
I0523 15:55:57.295271 21880 solver.cpp:244]     Train net output #0: accuracy = 0.990018
I0523 15:55:57.295279 21880 solver.cpp:244]     Train net output #1: loss = 0.0238757 (* 1 = 0.0238757 loss)
I0523 15:55:57.295284 21880 sgd_solver.cpp:106] Iteration 8380, lr = 0.0001
I0523 15:56:11.583014 21880 solver.cpp:337] Iteration 8400, Testing net (#0)
I0523 15:56:12.451043 21880 solver.cpp:404]     Test net output #0: accuracy = 0.989076
I0523 15:56:12.451077 21880 solver.cpp:404]     Test net output #1: loss = 0.027572 (* 1 = 0.027572 loss)
I0523 15:56:12.888177 21880 solver.cpp:228] Iteration 8400, loss = 0.0293505
I0523 15:56:12.888216 21880 solver.cpp:244]     Train net output #0: accuracy = 0.989413
I0523 15:56:12.888224 21880 solver.cpp:244]     Train net output #1: loss = 0.0293505 (* 1 = 0.0293505 loss)
I0523 15:56:12.888229 21880 sgd_solver.cpp:106] Iteration 8400, lr = 0.0001
I0523 15:56:27.598628 21880 solver.cpp:228] Iteration 8420, loss = 0.0237581
I0523 15:56:27.598654 21880 solver.cpp:244]     Train net output #0: accuracy = 0.990421
I0523 15:56:27.598661 21880 solver.cpp:244]     Train net output #1: loss = 0.0237581 (* 1 = 0.0237581 loss)
I0523 15:56:27.598666 21880 sgd_solver.cpp:106] Iteration 8420, lr = 0.0001
I0523 15:56:42.309005 21880 solver.cpp:228] Iteration 8440, loss = 0.0229568
I0523 15:56:42.309063 21880 solver.cpp:244]     Train net output #0: accuracy = 0.9902
I0523 15:56:42.309073 21880 solver.cpp:244]     Train net output #1: loss = 0.0229568 (* 1 = 0.0229568 loss)
I0523 15:56:42.309079 21880 sgd_solver.cpp:106] Iteration 8440, lr = 0.0001
I0523 15:56:56.991910 21880 solver.cpp:228] Iteration 8460, loss = 0.0225314
I0523 15:56:56.991936 21880 solver.cpp:244]     Train net output #0: accuracy = 0.992463
I0523 15:56:56.991943 21880 solver.cpp:244]     Train net output #1: loss = 0.0225314 (* 1 = 0.0225314 loss)
I0523 15:56:56.991948 21880 sgd_solver.cpp:106] Iteration 8460, lr = 0.0001
I0523 15:57:11.691189 21880 solver.cpp:228] Iteration 8480, loss = 0.0221833
I0523 15:57:11.691215 21880 solver.cpp:244]     Train net output #0: accuracy = 0.99118
I0523 15:57:11.691222 21880 solver.cpp:244]     Train net output #1: loss = 0.0221833 (* 1 = 0.0221833 loss)
I0523 15:57:11.691228 21880 sgd_solver.cpp:106] Iteration 8480, lr = 0.0001
I0523 15:57:25.995110 21880 solver.cpp:454] Snapshotting to binary proto file data/models/segnet_iter_8500.caffemodel
I0523 15:57:26.077173 21880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file data/models/segnet_iter_8500.solverstate
I0523 15:57:26.098373 21880 solver.cpp:337] Iteration 8500, Testing net (#0)
I0523 15:57:26.967716 21880 solver.cpp:404]     Test net output #0: accuracy = 0.986565
I0523 15:57:26.967741 21880 solver.cpp:404]     Test net output #1: loss = 0.0406121 (* 1 = 0.0406121 loss)
I0523 15:57:27.374620 21880 solver.cpp:228] Iteration 8500, loss = 0.0297583
I0523 15:57:27.374645 21880 solver.cpp:244]     Train net output #0: accuracy = 0.98949
I0523 15:57:27.374653 21880 solver.cpp:244]     Train net output #1: loss = 0.0297583 (* 1 = 0.0297583 loss)
I0523 15:57:27.374658 21880 sgd_solver.cpp:106] Iteration 8500, lr = 0.0001
I0523 15:57:42.074796 21880 solver.cpp:228] Iteration 8520, loss = 0.0278499
I0523 15:57:42.074823 21880 solver.cpp:244]     Train net output #0: accuracy = 0.988942
I0523 15:57:42.074831 21880 solver.cpp:244]     Train net output #1: loss = 0.0278499 (* 1 = 0.0278499 loss)
I0523 15:57:42.074836 21880 sgd_solver.cpp:106] Iteration 8520, lr = 0.0001
I0523 15:57:56.760932 21880 solver.cpp:228] Iteration 8540, loss = 0.0307663
I0523 15:57:56.761029 21880 solver.cpp:244]     Train net output #0: accuracy = 0.988445
I0523 15:57:56.761049 21880 solver.cpp:244]     Train net output #1: loss = 0.0307663 (* 1 = 0.0307663 loss)
I0523 15:57:56.761062 21880 sgd_solver.cpp:106] Iteration 8540, lr = 0.0001
I0523 15:58:11.445747 21880 solver.cpp:228] Iteration 8560, loss = 0.0189583
I0523 15:58:11.445775 21880 solver.cpp:244]     Train net output #0: accuracy = 0.992928
I0523 15:58:11.445782 21880 solver.cpp:244]     Train net output #1: loss = 0.0189583 (* 1 = 0.0189583 loss)
I0523 15:58:11.445787 21880 sgd_solver.cpp:106] Iteration 8560, lr = 0.0001
I0523 15:58:26.177139 21880 solver.cpp:228] Iteration 8580, loss = 0.0224384
I0523 15:58:26.177167 21880 solver.cpp:244]     Train net output #0: accuracy = 0.992749
I0523 15:58:26.177175 21880 solver.cpp:244]     Train net output #1: loss = 0.0224384 (* 1 = 0.0224384 loss)
I0523 15:58:26.177181 21880 sgd_solver.cpp:106] Iteration 8580, lr = 0.0001
I0523 15:58:40.468883 21880 solver.cpp:337] Iteration 8600, Testing net (#0)
I0523 15:58:41.338745 21880 solver.cpp:404]     Test net output #0: accuracy = 0.984984
I0523 15:58:41.338779 21880 solver.cpp:404]     Test net output #1: loss = 0.0463851 (* 1 = 0.0463851 loss)
I0523 15:58:41.745172 21880 solver.cpp:228] Iteration 8600, loss = 0.034252
I0523 15:58:41.745198 21880 solver.cpp:244]     Train net output #0: accuracy = 0.988314
I0523 15:58:41.745204 21880 solver.cpp:244]     Train net output #1: loss = 0.034252 (* 1 = 0.034252 loss)
I0523 15:58:41.745209 21880 sgd_solver.cpp:106] Iteration 8600, lr = 0.0001
I0523 15:58:56.428985 21880 solver.cpp:228] Iteration 8620, loss = 0.0329638
I0523 15:58:56.429011 21880 solver.cpp:244]     Train net output #0: accuracy = 0.990121
I0523 15:58:56.429018 21880 solver.cpp:244]     Train net output #1: loss = 0.0329638 (* 1 = 0.0329638 loss)
I0523 15:58:56.429023 21880 sgd_solver.cpp:106] Iteration 8620, lr = 0.0001
I0523 15:59:11.125754 21880 solver.cpp:228] Iteration 8640, loss = 0.0230431
I0523 15:59:11.125828 21880 solver.cpp:244]     Train net output #0: accuracy = 0.990963
I0523 15:59:11.125838 21880 solver.cpp:244]     Train net output #1: loss = 0.0230431 (* 1 = 0.0230431 loss)
I0523 15:59:11.125844 21880 sgd_solver.cpp:106] Iteration 8640, lr = 0.0001
I0523 15:59:25.806288 21880 solver.cpp:228] Iteration 8660, loss = 0.0194391
I0523 15:59:25.806314 21880 solver.cpp:244]     Train net output #0: accuracy = 0.992483
I0523 15:59:25.806321 21880 solver.cpp:244]     Train net output #1: loss = 0.0194391 (* 1 = 0.0194391 loss)
I0523 15:59:25.806325 21880 sgd_solver.cpp:106] Iteration 8660, lr = 0.0001
I0523 15:59:40.494873 21880 solver.cpp:228] Iteration 8680, loss = 0.0235634
I0523 15:59:40.494910 21880 solver.cpp:244]     Train net output #0: accuracy = 0.991917
I0523 15:59:40.494917 21880 solver.cpp:244]     Train net output #1: loss = 0.0235634 (* 1 = 0.0235634 loss)
I0523 15:59:40.494922 21880 sgd_solver.cpp:106] Iteration 8680, lr = 0.0001
I0523 15:59:54.784494 21880 solver.cpp:337] Iteration 8700, Testing net (#0)
I0523 15:59:55.650496 21880 solver.cpp:404]     Test net output #0: accuracy = 0.988367
I0523 15:59:55.650518 21880 solver.cpp:404]     Test net output #1: loss = 0.0338533 (* 1 = 0.0338533 loss)
I0523 15:59:56.056588 21880 solver.cpp:228] Iteration 8700, loss = 0.0180515
I0523 15:59:56.056614 21880 solver.cpp:244]     Train net output #0: accuracy = 0.992464
I0523 15:59:56.056622 21880 solver.cpp:244]     Train net output #1: loss = 0.0180515 (* 1 = 0.0180515 loss)
I0523 15:59:56.056625 21880 sgd_solver.cpp:106] Iteration 8700, lr = 0.0001
I0523 16:00:10.778118 21880 solver.cpp:228] Iteration 8720, loss = 0.0295698
I0523 16:00:10.778147 21880 solver.cpp:244]     Train net output #0: accuracy = 0.986879
I0523 16:00:10.778156 21880 solver.cpp:244]     Train net output #1: loss = 0.0295698 (* 1 = 0.0295698 loss)
I0523 16:00:10.778162 21880 sgd_solver.cpp:106] Iteration 8720, lr = 0.0001
I0523 16:00:25.505136 21880 solver.cpp:228] Iteration 8740, loss = 0.0229326
I0523 16:00:25.505213 21880 solver.cpp:244]     Train net output #0: accuracy = 0.990895
I0523 16:00:25.505233 21880 solver.cpp:244]     Train net output #1: loss = 0.0229326 (* 1 = 0.0229326 loss)
I0523 16:00:25.505237 21880 sgd_solver.cpp:106] Iteration 8740, lr = 0.0001
I0523 16:00:40.201537 21880 solver.cpp:228] Iteration 8760, loss = 0.0311933
I0523 16:00:40.201563 21880 solver.cpp:244]     Train net output #0: accuracy = 0.987863
I0523 16:00:40.201570 21880 solver.cpp:244]     Train net output #1: loss = 0.0311933 (* 1 = 0.0311933 loss)
I0523 16:00:40.201576 21880 sgd_solver.cpp:106] Iteration 8760, lr = 0.0001
I0523 16:00:54.901083 21880 solver.cpp:228] Iteration 8780, loss = 0.0311617
I0523 16:00:54.901113 21880 solver.cpp:244]     Train net output #0: accuracy = 0.988904
I0523 16:00:54.901119 21880 solver.cpp:244]     Train net output #1: loss = 0.0311617 (* 1 = 0.0311617 loss)
I0523 16:00:54.901124 21880 sgd_solver.cpp:106] Iteration 8780, lr = 0.0001
I0523 16:01:09.202306 21880 solver.cpp:337] Iteration 8800, Testing net (#0)
I0523 16:01:10.069365 21880 solver.cpp:404]     Test net output #0: accuracy = 0.986965
I0523 16:01:10.069389 21880 solver.cpp:404]     Test net output #1: loss = 0.0371114 (* 1 = 0.0371114 loss)
I0523 16:01:10.475384 21880 solver.cpp:228] Iteration 8800, loss = 0.0239465
I0523 16:01:10.475409 21880 solver.cpp:244]     Train net output #0: accuracy = 0.991048
I0523 16:01:10.475416 21880 solver.cpp:244]     Train net output #1: loss = 0.0239465 (* 1 = 0.0239465 loss)
I0523 16:01:10.475422 21880 sgd_solver.cpp:106] Iteration 8800, lr = 0.0001
I0523 16:01:25.168866 21880 solver.cpp:228] Iteration 8820, loss = 0.0213934
I0523 16:01:25.168895 21880 solver.cpp:244]     Train net output #0: accuracy = 0.991357
I0523 16:01:25.168902 21880 solver.cpp:244]     Train net output #1: loss = 0.0213934 (* 1 = 0.0213934 loss)
I0523 16:01:25.168907 21880 sgd_solver.cpp:106] Iteration 8820, lr = 0.0001
I0523 16:01:39.879483 21880 solver.cpp:228] Iteration 8840, loss = 0.0390558
I0523 16:01:39.879565 21880 solver.cpp:244]     Train net output #0: accuracy = 0.986465
I0523 16:01:39.879583 21880 solver.cpp:244]     Train net output #1: loss = 0.0390558 (* 1 = 0.0390558 loss)
I0523 16:01:39.879590 21880 sgd_solver.cpp:106] Iteration 8840, lr = 0.0001
I0523 16:01:54.574395 21880 solver.cpp:228] Iteration 8860, loss = 0.022004
I0523 16:01:54.574421 21880 solver.cpp:244]     Train net output #0: accuracy = 0.991329
I0523 16:01:54.574429 21880 solver.cpp:244]     Train net output #1: loss = 0.022004 (* 1 = 0.022004 loss)
I0523 16:01:54.574435 21880 sgd_solver.cpp:106] Iteration 8860, lr = 0.0001
I0523 16:02:09.272811 21880 solver.cpp:228] Iteration 8880, loss = 0.0242103
I0523 16:02:09.272840 21880 solver.cpp:244]     Train net output #0: accuracy = 0.992453
I0523 16:02:09.272846 21880 solver.cpp:244]     Train net output #1: loss = 0.0242103 (* 1 = 0.0242103 loss)
I0523 16:02:09.272852 21880 sgd_solver.cpp:106] Iteration 8880, lr = 0.0001
I0523 16:02:23.585261 21880 solver.cpp:337] Iteration 8900, Testing net (#0)
I0523 16:02:24.491863 21880 solver.cpp:404]     Test net output #0: accuracy = 0.9866
I0523 16:02:24.491888 21880 solver.cpp:404]     Test net output #1: loss = 0.0382009 (* 1 = 0.0382009 loss)
I0523 16:02:24.897738 21880 solver.cpp:228] Iteration 8900, loss = 0.0322297
I0523 16:02:24.897774 21880 solver.cpp:244]     Train net output #0: accuracy = 0.988456
I0523 16:02:24.897781 21880 solver.cpp:244]     Train net output #1: loss = 0.0322297 (* 1 = 0.0322297 loss)
I0523 16:02:24.897786 21880 sgd_solver.cpp:106] Iteration 8900, lr = 0.0001
I0523 16:02:39.600930 21880 solver.cpp:228] Iteration 8920, loss = 0.0278887
I0523 16:02:39.600956 21880 solver.cpp:244]     Train net output #0: accuracy = 0.99011
I0523 16:02:39.600963 21880 solver.cpp:244]     Train net output #1: loss = 0.0278887 (* 1 = 0.0278887 loss)
I0523 16:02:39.600970 21880 sgd_solver.cpp:106] Iteration 8920, lr = 0.0001
I0523 16:02:54.290189 21880 solver.cpp:228] Iteration 8940, loss = 0.0236627
I0523 16:02:54.290282 21880 solver.cpp:244]     Train net output #0: accuracy = 0.991963
I0523 16:02:54.290292 21880 solver.cpp:244]     Train net output #1: loss = 0.0236627 (* 1 = 0.0236627 loss)
I0523 16:02:54.290297 21880 sgd_solver.cpp:106] Iteration 8940, lr = 0.0001
I0523 16:03:08.972712 21880 solver.cpp:228] Iteration 8960, loss = 0.0188975
I0523 16:03:08.972739 21880 solver.cpp:244]     Train net output #0: accuracy = 0.9934
I0523 16:03:08.972748 21880 solver.cpp:244]     Train net output #1: loss = 0.0188975 (* 1 = 0.0188975 loss)
I0523 16:03:08.972754 21880 sgd_solver.cpp:106] Iteration 8960, lr = 0.0001
I0523 16:03:23.670219 21880 solver.cpp:228] Iteration 8980, loss = 0.028138
I0523 16:03:23.670244 21880 solver.cpp:244]     Train net output #0: accuracy = 0.98979
I0523 16:03:23.670253 21880 solver.cpp:244]     Train net output #1: loss = 0.028138 (* 1 = 0.028138 loss)
I0523 16:03:23.670258 21880 sgd_solver.cpp:106] Iteration 8980, lr = 0.0001
I0523 16:03:37.965273 21880 solver.cpp:454] Snapshotting to binary proto file data/models/segnet_iter_9000.caffemodel
I0523 16:03:38.047641 21880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file data/models/segnet_iter_9000.solverstate
I0523 16:03:38.068794 21880 solver.cpp:337] Iteration 9000, Testing net (#0)
I0523 16:03:38.937315 21880 solver.cpp:404]     Test net output #0: accuracy = 0.985622
I0523 16:03:38.937340 21880 solver.cpp:404]     Test net output #1: loss = 0.0388521 (* 1 = 0.0388521 loss)
I0523 16:03:39.349107 21880 solver.cpp:228] Iteration 9000, loss = 0.025665
I0523 16:03:39.349133 21880 solver.cpp:244]     Train net output #0: accuracy = 0.989574
I0523 16:03:39.349140 21880 solver.cpp:244]     Train net output #1: loss = 0.025665 (* 1 = 0.025665 loss)
I0523 16:03:39.349154 21880 sgd_solver.cpp:106] Iteration 9000, lr = 1e-05
I0523 16:03:54.033315 21880 solver.cpp:228] Iteration 9020, loss = 0.0306435
I0523 16:03:54.033341 21880 solver.cpp:244]     Train net output #0: accuracy = 0.989272
I0523 16:03:54.033349 21880 solver.cpp:244]     Train net output #1: loss = 0.0306435 (* 1 = 0.0306435 loss)
I0523 16:03:54.033354 21880 sgd_solver.cpp:106] Iteration 9020, lr = 1e-05
I0523 16:04:08.740576 21880 solver.cpp:228] Iteration 9040, loss = 0.0328041
I0523 16:04:08.740684 21880 solver.cpp:244]     Train net output #0: accuracy = 0.986446
I0523 16:04:08.740694 21880 solver.cpp:244]     Train net output #1: loss = 0.0328041 (* 1 = 0.0328041 loss)
I0523 16:04:08.740700 21880 sgd_solver.cpp:106] Iteration 9040, lr = 1e-05
I0523 16:04:23.465303 21880 solver.cpp:228] Iteration 9060, loss = 0.0267765
I0523 16:04:23.465330 21880 solver.cpp:244]     Train net output #0: accuracy = 0.990386
I0523 16:04:23.465337 21880 solver.cpp:244]     Train net output #1: loss = 0.0267765 (* 1 = 0.0267765 loss)
I0523 16:04:23.465343 21880 sgd_solver.cpp:106] Iteration 9060, lr = 1e-05
I0523 16:04:38.165321 21880 solver.cpp:228] Iteration 9080, loss = 0.0200231
I0523 16:04:38.165357 21880 solver.cpp:244]     Train net output #0: accuracy = 0.992135
I0523 16:04:38.165365 21880 solver.cpp:244]     Train net output #1: loss = 0.0200231 (* 1 = 0.0200231 loss)
I0523 16:04:38.165370 21880 sgd_solver.cpp:106] Iteration 9080, lr = 1e-05
I0523 16:04:52.440044 21880 solver.cpp:337] Iteration 9100, Testing net (#0)
I0523 16:04:53.308930 21880 solver.cpp:404]     Test net output #0: accuracy = 0.987827
I0523 16:04:53.308954 21880 solver.cpp:404]     Test net output #1: loss = 0.0304337 (* 1 = 0.0304337 loss)
I0523 16:04:53.714936 21880 solver.cpp:228] Iteration 9100, loss = 0.0203835
I0523 16:04:53.714961 21880 solver.cpp:244]     Train net output #0: accuracy = 0.993381
I0523 16:04:53.714967 21880 solver.cpp:244]     Train net output #1: loss = 0.0203835 (* 1 = 0.0203835 loss)
I0523 16:04:53.714973 21880 sgd_solver.cpp:106] Iteration 9100, lr = 1e-05
I0523 16:05:08.398133 21880 solver.cpp:228] Iteration 9120, loss = 0.0200601
I0523 16:05:08.398159 21880 solver.cpp:244]     Train net output #0: accuracy = 0.991854
I0523 16:05:08.398166 21880 solver.cpp:244]     Train net output #1: loss = 0.0200601 (* 1 = 0.0200601 loss)
I0523 16:05:08.398172 21880 sgd_solver.cpp:106] Iteration 9120, lr = 1e-05
I0523 16:05:23.077257 21880 solver.cpp:228] Iteration 9140, loss = 0.0229052
I0523 16:05:23.077359 21880 solver.cpp:244]     Train net output #0: accuracy = 0.991114
I0523 16:05:23.077368 21880 solver.cpp:244]     Train net output #1: loss = 0.0229052 (* 1 = 0.0229052 loss)
I0523 16:05:23.077375 21880 sgd_solver.cpp:106] Iteration 9140, lr = 1e-05
I0523 16:05:37.768690 21880 solver.cpp:228] Iteration 9160, loss = 0.0222703
I0523 16:05:37.768715 21880 solver.cpp:244]     Train net output #0: accuracy = 0.991872
I0523 16:05:37.768723 21880 solver.cpp:244]     Train net output #1: loss = 0.0222703 (* 1 = 0.0222703 loss)
I0523 16:05:37.768728 21880 sgd_solver.cpp:106] Iteration 9160, lr = 1e-05
I0523 16:05:52.466502 21880 solver.cpp:228] Iteration 9180, loss = 0.0298551
I0523 16:05:52.466529 21880 solver.cpp:244]     Train net output #0: accuracy = 0.988746
I0523 16:05:52.466537 21880 solver.cpp:244]     Train net output #1: loss = 0.0298551 (* 1 = 0.0298551 loss)
I0523 16:05:52.466543 21880 sgd_solver.cpp:106] Iteration 9180, lr = 1e-05
I0523 16:06:06.759994 21880 solver.cpp:337] Iteration 9200, Testing net (#0)
I0523 16:06:07.640200 21880 solver.cpp:404]     Test net output #0: accuracy = 0.988467
I0523 16:06:07.640239 21880 solver.cpp:404]     Test net output #1: loss = 0.0368718 (* 1 = 0.0368718 loss)
I0523 16:06:08.071110 21880 solver.cpp:228] Iteration 9200, loss = 0.0274307
I0523 16:06:08.071158 21880 solver.cpp:244]     Train net output #0: accuracy = 0.98979
I0523 16:06:08.071233 21880 solver.cpp:244]     Train net output #1: loss = 0.0274307 (* 1 = 0.0274307 loss)
I0523 16:06:08.071243 21880 sgd_solver.cpp:106] Iteration 9200, lr = 1e-05
I0523 16:06:22.764659 21880 solver.cpp:228] Iteration 9220, loss = 0.0202402
I0523 16:06:22.764685 21880 solver.cpp:244]     Train net output #0: accuracy = 0.991328
I0523 16:06:22.764695 21880 solver.cpp:244]     Train net output #1: loss = 0.0202402 (* 1 = 0.0202402 loss)
I0523 16:06:22.764703 21880 sgd_solver.cpp:106] Iteration 9220, lr = 1e-05
I0523 16:06:37.483494 21880 solver.cpp:228] Iteration 9240, loss = 0.0230525
I0523 16:06:37.483546 21880 solver.cpp:244]     Train net output #0: accuracy = 0.99144
I0523 16:06:37.483558 21880 solver.cpp:244]     Train net output #1: loss = 0.0230525 (* 1 = 0.0230525 loss)
I0523 16:06:37.483566 21880 sgd_solver.cpp:106] Iteration 9240, lr = 1e-05
I0523 16:06:52.217481 21880 solver.cpp:228] Iteration 9260, loss = 0.0176779
I0523 16:06:52.217511 21880 solver.cpp:244]     Train net output #0: accuracy = 0.993445
I0523 16:06:52.217521 21880 solver.cpp:244]     Train net output #1: loss = 0.0176779 (* 1 = 0.0176779 loss)
I0523 16:06:52.217528 21880 sgd_solver.cpp:106] Iteration 9260, lr = 1e-05
I0523 16:07:06.907356 21880 solver.cpp:228] Iteration 9280, loss = 0.0295486
I0523 16:07:06.907384 21880 solver.cpp:244]     Train net output #0: accuracy = 0.987876
I0523 16:07:06.907395 21880 solver.cpp:244]     Train net output #1: loss = 0.0295486 (* 1 = 0.0295486 loss)
I0523 16:07:06.907403 21880 sgd_solver.cpp:106] Iteration 9280, lr = 1e-05
I0523 16:07:21.198568 21880 solver.cpp:337] Iteration 9300, Testing net (#0)
I0523 16:07:22.064952 21880 solver.cpp:404]     Test net output #0: accuracy = 0.988571
I0523 16:07:22.064978 21880 solver.cpp:404]     Test net output #1: loss = 0.0301211 (* 1 = 0.0301211 loss)
I0523 16:07:22.471714 21880 solver.cpp:228] Iteration 9300, loss = 0.017312
I0523 16:07:22.471741 21880 solver.cpp:244]     Train net output #0: accuracy = 0.993424
I0523 16:07:22.471751 21880 solver.cpp:244]     Train net output #1: loss = 0.017312 (* 1 = 0.017312 loss)
I0523 16:07:22.471760 21880 sgd_solver.cpp:106] Iteration 9300, lr = 1e-05
I0523 16:07:37.161804 21880 solver.cpp:228] Iteration 9320, loss = 0.0300456
I0523 16:07:37.161833 21880 solver.cpp:244]     Train net output #0: accuracy = 0.988622
I0523 16:07:37.161844 21880 solver.cpp:244]     Train net output #1: loss = 0.0300456 (* 1 = 0.0300456 loss)
I0523 16:07:37.161851 21880 sgd_solver.cpp:106] Iteration 9320, lr = 1e-05
I0523 16:07:51.864205 21880 solver.cpp:228] Iteration 9340, loss = 0.0165612
I0523 16:07:51.864303 21880 solver.cpp:244]     Train net output #0: accuracy = 0.995079
I0523 16:07:51.864316 21880 solver.cpp:244]     Train net output #1: loss = 0.0165612 (* 1 = 0.0165612 loss)
I0523 16:07:51.864325 21880 sgd_solver.cpp:106] Iteration 9340, lr = 1e-05
I0523 16:08:06.565042 21880 solver.cpp:228] Iteration 9360, loss = 0.0243193
I0523 16:08:06.565071 21880 solver.cpp:244]     Train net output #0: accuracy = 0.991649
I0523 16:08:06.565079 21880 solver.cpp:244]     Train net output #1: loss = 0.0243193 (* 1 = 0.0243193 loss)
I0523 16:08:06.565085 21880 sgd_solver.cpp:106] Iteration 9360, lr = 1e-05
I0523 16:08:21.292788 21880 solver.cpp:228] Iteration 9380, loss = 0.0231019
I0523 16:08:21.292816 21880 solver.cpp:244]     Train net output #0: accuracy = 0.991587
I0523 16:08:21.292824 21880 solver.cpp:244]     Train net output #1: loss = 0.0231019 (* 1 = 0.0231019 loss)
I0523 16:08:21.292830 21880 sgd_solver.cpp:106] Iteration 9380, lr = 1e-05
I0523 16:08:35.576752 21880 solver.cpp:337] Iteration 9400, Testing net (#0)
I0523 16:08:36.443511 21880 solver.cpp:404]     Test net output #0: accuracy = 0.98449
I0523 16:08:36.443534 21880 solver.cpp:404]     Test net output #1: loss = 0.053266 (* 1 = 0.053266 loss)
I0523 16:08:36.850569 21880 solver.cpp:228] Iteration 9400, loss = 0.0326496
I0523 16:08:36.850596 21880 solver.cpp:244]     Train net output #0: accuracy = 0.990117
I0523 16:08:36.850603 21880 solver.cpp:244]     Train net output #1: loss = 0.0326496 (* 1 = 0.0326496 loss)
I0523 16:08:36.850608 21880 sgd_solver.cpp:106] Iteration 9400, lr = 1e-05
I0523 16:08:51.546984 21880 solver.cpp:228] Iteration 9420, loss = 0.0221325
I0523 16:08:51.547011 21880 solver.cpp:244]     Train net output #0: accuracy = 0.992983
I0523 16:08:51.547019 21880 solver.cpp:244]     Train net output #1: loss = 0.0221325 (* 1 = 0.0221325 loss)
I0523 16:08:51.547024 21880 sgd_solver.cpp:106] Iteration 9420, lr = 1e-05
I0523 16:09:06.235894 21880 solver.cpp:228] Iteration 9440, loss = 0.0277251
I0523 16:09:06.235968 21880 solver.cpp:244]     Train net output #0: accuracy = 0.989609
I0523 16:09:06.235980 21880 solver.cpp:244]     Train net output #1: loss = 0.0277251 (* 1 = 0.0277251 loss)
I0523 16:09:06.235996 21880 sgd_solver.cpp:106] Iteration 9440, lr = 1e-05
I0523 16:09:20.915169 21880 solver.cpp:228] Iteration 9460, loss = 0.0378626
I0523 16:09:20.915194 21880 solver.cpp:244]     Train net output #0: accuracy = 0.987321
I0523 16:09:20.915202 21880 solver.cpp:244]     Train net output #1: loss = 0.0378626 (* 1 = 0.0378626 loss)
I0523 16:09:20.915207 21880 sgd_solver.cpp:106] Iteration 9460, lr = 1e-05
I0523 16:09:35.601069 21880 solver.cpp:228] Iteration 9480, loss = 0.0278649
I0523 16:09:35.601106 21880 solver.cpp:244]     Train net output #0: accuracy = 0.989141
I0523 16:09:35.601114 21880 solver.cpp:244]     Train net output #1: loss = 0.0278649 (* 1 = 0.0278649 loss)
I0523 16:09:35.601120 21880 sgd_solver.cpp:106] Iteration 9480, lr = 1e-05
I0523 16:09:49.902757 21880 solver.cpp:454] Snapshotting to binary proto file data/models/segnet_iter_9500.caffemodel
I0523 16:09:49.979594 21880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file data/models/segnet_iter_9500.solverstate
I0523 16:09:50.000406 21880 solver.cpp:337] Iteration 9500, Testing net (#0)
I0523 16:09:50.870062 21880 solver.cpp:404]     Test net output #0: accuracy = 0.985504
I0523 16:09:50.870086 21880 solver.cpp:404]     Test net output #1: loss = 0.0435145 (* 1 = 0.0435145 loss)
I0523 16:09:51.275923 21880 solver.cpp:228] Iteration 9500, loss = 0.029518
I0523 16:09:51.275949 21880 solver.cpp:244]     Train net output #0: accuracy = 0.98701
I0523 16:09:51.275957 21880 solver.cpp:244]     Train net output #1: loss = 0.029518 (* 1 = 0.029518 loss)
I0523 16:09:51.275964 21880 sgd_solver.cpp:106] Iteration 9500, lr = 1e-05
I0523 16:10:06.005467 21880 solver.cpp:228] Iteration 9520, loss = 0.033404
I0523 16:10:06.005496 21880 solver.cpp:244]     Train net output #0: accuracy = 0.987447
I0523 16:10:06.005503 21880 solver.cpp:244]     Train net output #1: loss = 0.033404 (* 1 = 0.033404 loss)
I0523 16:10:06.005509 21880 sgd_solver.cpp:106] Iteration 9520, lr = 1e-05
I0523 16:10:20.711869 21880 solver.cpp:228] Iteration 9540, loss = 0.0184392
I0523 16:10:20.711930 21880 solver.cpp:244]     Train net output #0: accuracy = 0.992344
I0523 16:10:20.711940 21880 solver.cpp:244]     Train net output #1: loss = 0.0184392 (* 1 = 0.0184392 loss)
I0523 16:10:20.711944 21880 sgd_solver.cpp:106] Iteration 9540, lr = 1e-05
I0523 16:10:35.417760 21880 solver.cpp:228] Iteration 9560, loss = 0.0213995
I0523 16:10:35.417788 21880 solver.cpp:244]     Train net output #0: accuracy = 0.991983
I0523 16:10:35.417794 21880 solver.cpp:244]     Train net output #1: loss = 0.0213995 (* 1 = 0.0213995 loss)
I0523 16:10:35.417801 21880 sgd_solver.cpp:106] Iteration 9560, lr = 1e-05
I0523 16:10:50.105125 21880 solver.cpp:228] Iteration 9580, loss = 0.0210866
I0523 16:10:50.105152 21880 solver.cpp:244]     Train net output #0: accuracy = 0.992552
I0523 16:10:50.105159 21880 solver.cpp:244]     Train net output #1: loss = 0.0210866 (* 1 = 0.0210866 loss)
I0523 16:10:50.105165 21880 sgd_solver.cpp:106] Iteration 9580, lr = 1e-05
I0523 16:11:04.386430 21880 solver.cpp:337] Iteration 9600, Testing net (#0)
I0523 16:11:05.256371 21880 solver.cpp:404]     Test net output #0: accuracy = 0.988393
I0523 16:11:05.256393 21880 solver.cpp:404]     Test net output #1: loss = 0.0342668 (* 1 = 0.0342668 loss)
I0523 16:11:05.662658 21880 solver.cpp:228] Iteration 9600, loss = 0.0228564
I0523 16:11:05.662685 21880 solver.cpp:244]     Train net output #0: accuracy = 0.990521
I0523 16:11:05.662693 21880 solver.cpp:244]     Train net output #1: loss = 0.0228564 (* 1 = 0.0228564 loss)
I0523 16:11:05.662699 21880 sgd_solver.cpp:106] Iteration 9600, lr = 1e-05
I0523 16:11:20.351627 21880 solver.cpp:228] Iteration 9620, loss = 0.0258949
I0523 16:11:20.351654 21880 solver.cpp:244]     Train net output #0: accuracy = 0.989063
I0523 16:11:20.351660 21880 solver.cpp:244]     Train net output #1: loss = 0.0258949 (* 1 = 0.0258949 loss)
I0523 16:11:20.351665 21880 sgd_solver.cpp:106] Iteration 9620, lr = 1e-05
I0523 16:11:35.056406 21880 solver.cpp:228] Iteration 9640, loss = 0.0204011
I0523 16:11:35.056514 21880 solver.cpp:244]     Train net output #0: accuracy = 0.991686
I0523 16:11:35.056531 21880 solver.cpp:244]     Train net output #1: loss = 0.0204011 (* 1 = 0.0204011 loss)
I0523 16:11:35.056536 21880 sgd_solver.cpp:106] Iteration 9640, lr = 1e-05
I0523 16:11:49.750740 21880 solver.cpp:228] Iteration 9660, loss = 0.0218424
I0523 16:11:49.750764 21880 solver.cpp:244]     Train net output #0: accuracy = 0.991499
I0523 16:11:49.750771 21880 solver.cpp:244]     Train net output #1: loss = 0.0218424 (* 1 = 0.0218424 loss)
I0523 16:11:49.750777 21880 sgd_solver.cpp:106] Iteration 9660, lr = 1e-05
I0523 16:12:04.433634 21880 solver.cpp:228] Iteration 9680, loss = 0.0329158
I0523 16:12:04.433660 21880 solver.cpp:244]     Train net output #0: accuracy = 0.988287
I0523 16:12:04.433666 21880 solver.cpp:244]     Train net output #1: loss = 0.0329158 (* 1 = 0.0329158 loss)
I0523 16:12:04.433672 21880 sgd_solver.cpp:106] Iteration 9680, lr = 1e-05
I0523 16:12:18.745045 21880 solver.cpp:337] Iteration 9700, Testing net (#0)
I0523 16:12:19.636142 21880 solver.cpp:404]     Test net output #0: accuracy = 0.98857
I0523 16:12:19.636176 21880 solver.cpp:404]     Test net output #1: loss = 0.035948 (* 1 = 0.035948 loss)
I0523 16:12:20.040983 21880 solver.cpp:228] Iteration 9700, loss = 0.0227639
I0523 16:12:20.041007 21880 solver.cpp:244]     Train net output #0: accuracy = 0.990663
I0523 16:12:20.041014 21880 solver.cpp:244]     Train net output #1: loss = 0.0227639 (* 1 = 0.0227639 loss)
I0523 16:12:20.041020 21880 sgd_solver.cpp:106] Iteration 9700, lr = 1e-05
I0523 16:12:34.723861 21880 solver.cpp:228] Iteration 9720, loss = 0.019716
I0523 16:12:34.723886 21880 solver.cpp:244]     Train net output #0: accuracy = 0.992232
I0523 16:12:34.723893 21880 solver.cpp:244]     Train net output #1: loss = 0.019716 (* 1 = 0.019716 loss)
I0523 16:12:34.723899 21880 sgd_solver.cpp:106] Iteration 9720, lr = 1e-05
I0523 16:12:49.416323 21880 solver.cpp:228] Iteration 9740, loss = 0.0241698
I0523 16:12:49.416369 21880 solver.cpp:244]     Train net output #0: accuracy = 0.990794
I0523 16:12:49.416378 21880 solver.cpp:244]     Train net output #1: loss = 0.0241698 (* 1 = 0.0241698 loss)
I0523 16:12:49.416383 21880 sgd_solver.cpp:106] Iteration 9740, lr = 1e-05
I0523 16:13:04.105195 21880 solver.cpp:228] Iteration 9760, loss = 0.0202021
I0523 16:13:04.105232 21880 solver.cpp:244]     Train net output #0: accuracy = 0.991047
I0523 16:13:04.105242 21880 solver.cpp:244]     Train net output #1: loss = 0.0202021 (* 1 = 0.0202021 loss)
I0523 16:13:04.105247 21880 sgd_solver.cpp:106] Iteration 9760, lr = 1e-05
I0523 16:13:18.789393 21880 solver.cpp:228] Iteration 9780, loss = 0.0268094
I0523 16:13:18.789419 21880 solver.cpp:244]     Train net output #0: accuracy = 0.991435
I0523 16:13:18.789427 21880 solver.cpp:244]     Train net output #1: loss = 0.0268094 (* 1 = 0.0268094 loss)
I0523 16:13:18.789432 21880 sgd_solver.cpp:106] Iteration 9780, lr = 1e-05
I0523 16:13:33.075845 21880 solver.cpp:337] Iteration 9800, Testing net (#0)
I0523 16:13:33.943199 21880 solver.cpp:404]     Test net output #0: accuracy = 0.985655
I0523 16:13:33.943223 21880 solver.cpp:404]     Test net output #1: loss = 0.0386772 (* 1 = 0.0386772 loss)
I0523 16:13:34.352408 21880 solver.cpp:228] Iteration 9800, loss = 0.0196169
I0523 16:13:34.352434 21880 solver.cpp:244]     Train net output #0: accuracy = 0.992682
I0523 16:13:34.352442 21880 solver.cpp:244]     Train net output #1: loss = 0.0196169 (* 1 = 0.0196169 loss)
I0523 16:13:34.352448 21880 sgd_solver.cpp:106] Iteration 9800, lr = 1e-05
I0523 16:13:49.041456 21880 solver.cpp:228] Iteration 9820, loss = 0.0230721
I0523 16:13:49.041482 21880 solver.cpp:244]     Train net output #0: accuracy = 0.990253
I0523 16:13:49.041491 21880 solver.cpp:244]     Train net output #1: loss = 0.0230721 (* 1 = 0.0230721 loss)
I0523 16:13:49.041497 21880 sgd_solver.cpp:106] Iteration 9820, lr = 1e-05
I0523 16:14:03.739928 21880 solver.cpp:228] Iteration 9840, loss = 0.019048
I0523 16:14:03.740037 21880 solver.cpp:244]     Train net output #0: accuracy = 0.993839
I0523 16:14:03.740056 21880 solver.cpp:244]     Train net output #1: loss = 0.019048 (* 1 = 0.019048 loss)
I0523 16:14:03.740062 21880 sgd_solver.cpp:106] Iteration 9840, lr = 1e-05
I0523 16:14:18.458026 21880 solver.cpp:228] Iteration 9860, loss = 0.0241225
I0523 16:14:18.458053 21880 solver.cpp:244]     Train net output #0: accuracy = 0.989938
I0523 16:14:18.458061 21880 solver.cpp:244]     Train net output #1: loss = 0.0241225 (* 1 = 0.0241225 loss)
I0523 16:14:18.458068 21880 sgd_solver.cpp:106] Iteration 9860, lr = 1e-05
I0523 16:14:33.162108 21880 solver.cpp:228] Iteration 9880, loss = 0.0215175
I0523 16:14:33.162137 21880 solver.cpp:244]     Train net output #0: accuracy = 0.992931
I0523 16:14:33.162145 21880 solver.cpp:244]     Train net output #1: loss = 0.0215175 (* 1 = 0.0215175 loss)
I0523 16:14:33.162152 21880 sgd_solver.cpp:106] Iteration 9880, lr = 1e-05
I0523 16:14:47.465817 21880 solver.cpp:337] Iteration 9900, Testing net (#0)
I0523 16:14:48.330421 21880 solver.cpp:404]     Test net output #0: accuracy = 0.985762
I0523 16:14:48.330446 21880 solver.cpp:404]     Test net output #1: loss = 0.0377498 (* 1 = 0.0377498 loss)
I0523 16:14:48.739439 21880 solver.cpp:228] Iteration 9900, loss = 0.0276536
I0523 16:14:48.739466 21880 solver.cpp:244]     Train net output #0: accuracy = 0.990549
I0523 16:14:48.739473 21880 solver.cpp:244]     Train net output #1: loss = 0.0276536 (* 1 = 0.0276536 loss)
I0523 16:14:48.739480 21880 sgd_solver.cpp:106] Iteration 9900, lr = 1e-05
I0523 16:15:03.428998 21880 solver.cpp:228] Iteration 9920, loss = 0.0264827
I0523 16:15:03.429028 21880 solver.cpp:244]     Train net output #0: accuracy = 0.989816
I0523 16:15:03.429035 21880 solver.cpp:244]     Train net output #1: loss = 0.0264827 (* 1 = 0.0264827 loss)
I0523 16:15:03.429041 21880 sgd_solver.cpp:106] Iteration 9920, lr = 1e-05
I0523 16:15:18.119786 21880 solver.cpp:228] Iteration 9940, loss = 0.025276
I0523 16:15:18.119854 21880 solver.cpp:244]     Train net output #0: accuracy = 0.989929
I0523 16:15:18.119863 21880 solver.cpp:244]     Train net output #1: loss = 0.025276 (* 1 = 0.025276 loss)
I0523 16:15:18.119870 21880 sgd_solver.cpp:106] Iteration 9940, lr = 1e-05
I0523 16:15:32.803689 21880 solver.cpp:228] Iteration 9960, loss = 0.0311818
I0523 16:15:32.803715 21880 solver.cpp:244]     Train net output #0: accuracy = 0.988004
I0523 16:15:32.803724 21880 solver.cpp:244]     Train net output #1: loss = 0.0311818 (* 1 = 0.0311818 loss)
I0523 16:15:32.803730 21880 sgd_solver.cpp:106] Iteration 9960, lr = 1e-05
I0523 16:15:47.512133 21880 solver.cpp:228] Iteration 9980, loss = 0.0236776
I0523 16:15:47.512161 21880 solver.cpp:244]     Train net output #0: accuracy = 0.991354
I0523 16:15:47.512168 21880 solver.cpp:244]     Train net output #1: loss = 0.0236776 (* 1 = 0.0236776 loss)
I0523 16:15:47.512176 21880 sgd_solver.cpp:106] Iteration 9980, lr = 1e-05
I0523 16:16:01.809984 21880 solver.cpp:454] Snapshotting to binary proto file data/models/segnet_iter_10000.caffemodel
I0523 16:16:01.888130 21880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file data/models/segnet_iter_10000.solverstate
I0523 16:16:02.232343 21880 solver.cpp:317] Iteration 10000, loss = 0.0178769
I0523 16:16:02.232369 21880 solver.cpp:337] Iteration 10000, Testing net (#0)
I0523 16:16:03.056638 21880 solver.cpp:404]     Test net output #0: accuracy = 0.988663
I0523 16:16:03.056679 21880 solver.cpp:404]     Test net output #1: loss = 0.0297636 (* 1 = 0.0297636 loss)
I0523 16:16:03.056682 21880 solver.cpp:322] Optimization Done.
I0523 16:16:03.056685 21880 caffe.cpp:222] Optimization Done.
